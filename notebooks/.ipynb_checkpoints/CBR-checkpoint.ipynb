{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Evaluating a COVID-19 oriented Information Retrieval Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this work, we are going to building and evaluating the performance of several custom Information Retrieval engines. For that, we will use the TREC-COVID19 corpus to recover relevant documents related to SARS-COV-2 and COVID-10 disease queries. \n",
    "\n",
    "Firstly, we carry out a state of the art to explore several ways to do vector space models. \n",
    "\n",
    "Then, we start with the process of the data  to preparing the dataset, queries, and relevance judgement data. \n",
    "\n",
    "After that,  we create the first Vector Space Model implementation. This implementation is the simple version of one VSM. Here, one TF-IDF model is used and the VSM provides a document ranking in descending order of relevance due to the cosine measure.\n",
    "\n",
    "Nextly, we implement the VSM based on word co-occurrences. This implementation takes into account the Mutual Information, the TF and IDF, of two word co-occurrences. And, at the same as Simple VSM, provides a document ranking.\n",
    "\n",
    "Afterwards, we combine the two previous VSM to create one ranking through a linearly combination of the two VSM. \n",
    " \n",
    "In all Vector Space Models one performance evaluation is done. \n",
    "\n",
    "And, at the end of this work, one conclusion is carry out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State of the art\n",
    "Before selecting a model to implement, we decided to review the recent literature concerning vector space information retrieval systems. First, we will describe the paradigm which we are dealing with, since information retrieval is such a broad task. Then, we will briefly summarize some interesting articles. Note that these articles are sometimes quite specific in its building and in its application. We aim to generalize and to explain how such systems would work in a vector space-based information retrieval system\n",
    "\n",
    "### 2.1 Supervised vs Unsupervised learning\n",
    "This distinction belongs more to the paradigm of Machine Learning and not all that much to Information retrieval, but some parallelism in the way of building models can be stablished between these two techniques.\n",
    "\n",
    "In supervised learning, we train a Machine Learning model using labelled data. The data is therefore explicitly characterized by that label and the model is trained into distinguish between labels using the attributes of the data. The most common tasks are the ones of classification and regression. In unsupervised learning, the data is not labelled and therefore implicitly characterized. Usually unsupervised learning aims towards forming clusters, exploring in that way the data.\n",
    "\n",
    "Our base model is a case of unsupervised learning training, since we only train our vector space model using a set of documents, but those are not a priori labelled as relevant or not according to any query. It is noteworthy that the test may be considered as supervised, since we have a series of relevance judgement and some interesting metrics can be obtained following the TREC guidelines. In the literature, we can find also examples of supervised information retrieval systems, such as \"cite\", that uses the relevance judgment or a relevant/irrelevant label to train the model using documents and queries too.\n",
    "\n",
    "### 2.2 Co-occurrence\n",
    "Our base model constructs a vector space in which each document d_i is transformed into a vector, whose component t_j represents the j-th word of the corpus and the value v_j of t_j represents the TF IDF metric for the term t_j in document d_i. An input query is transformed into a vector and the cosine similarity between the documents d_i of the corpus and the query q is studied, so that the more relevant queries are the one that maximizes the cosine similarity measure.\n",
    "\n",
    "While the model is straight-forward, precise and easy to understand, this model ignores the existing correlation between words. Many models develop a word co-occurrence to account for this correlation. Chen et al. (2020) proposes a vector space model for herbal prescription, an ambit in which we can find correlation between two herbs being recommended together. The proposed article first extracts the most relevant co-occurrences of the corpus, exploring exhaustively the search space of all possible co-occurrences and discarding the ones below a certain support (frequency) and confidence (mutual information) threshold. Then each document d_i is transformed into a vector where each component c_j represents the j co-occurrence of the set R of relevant co-occurencies. The value v_j of c_j can be computed as v_j = TF * IDF * MI, i.e. the product of the term frequency, inverse document frequency and mutual information of c_j.\n",
    "\n",
    "Although co-occurrence models are somehow deprecated comparing its performance to novel approaches, we decided to implement an adaption of the model of Chen et al. (2020) due to (1) it's theoretical simplicity, (2) it's explainability, giving us keys on how and when the model works and (3) the no-need to use specialized and complex libraries.\n",
    "\n",
    "\n",
    "### 2.3 Description logics\n",
    "Description logics is a formal knowledge widely used in IA, since it provides a platform to express ontologies (the Web Ontology Language is based on Description Logics). It is similar to L1, but it's knowledge representation power is slightly more limited.\n",
    "\n",
    "DL-VSM (Boukhari and Omri, 2020) uses a Description Logics inference and a Vector Space Model in parallel to extract the important concepts of a corpus. It also uses the MeSH thesaurus, that contains a set of biomedical concepts, characterized by a preference term (the term representing that concept) and a set of non-preference terms (alternative term to represent the concept). Each term is composed of a set of words. I.e. concept > term > word in an abstraction level.\n",
    "\n",
    "1) The Vector Space Model calculates the weight of each word in each document, using the BM25 metric instead of TFIDF. Next, the term weight in MeSH thesaurus is computed similarly and the similarity between each document and each MeSH term (a set of words describing the term) to weight each MeSH term and, finally, compute the weight of each concept for each document.\n",
    "\n",
    "2) Description logics is used to describe each document as a conjunction of relation represented_by(w_i), where w_i is a word of the document. Each concept of MeSH is represented as a conjunction of relations described_by(t_i) where t_i is a term that describes the concept. An inference is carried out to identify the concepts relevant of each document.\n",
    "\n",
    "The intersection between the concepts obtained with both approaches defines our model. Given an input query (in which a concept is extracted), the score of each document is the sum of the weight its concept words present in the documents calculated in (1).\n",
    "\n",
    "\n",
    "### 2.4 Case Based Reasoning\n",
    "Although not applied in the field of Information retrieval but in the one of costumer preference analysis, the VSM-CBR (Ke et al., 2020) supposes an interesting approach to this problem. The proposed algorithm analyses the different customer demand (which is in text format) and store it in a vector space, where each demand is represented by a vector. A k-means clustering algorithms clusters these demands, exploratory analyse each one of them and study future demands. Although clustering has been previously used, this articles highlight the exploratory analysis of those cluster\n",
    "\n",
    "The interesting philosophy of this work is the clustering part. Instead building a model with |D| documents, we can cluster them according to the stems frequency and build a model using |C| clusters, with |C|<|D|. Furthermore, an exploratory analysis can be performed to label each clusters with a set of concepts and use these concepts to retrieve information, instead of the words of the document, which have a higher cardinality. The computational charge will be diminished, but the downside is that documents in the same clusters will be scored with the same relevance.\n",
    "\n",
    "### 2.5 Neural Vector Space Models.\n",
    "Neural Information retrieval has arose a lot of enthusiasm concerning the efficiency and efficacy of Deep Learning approaches (Marchesin et al., 2020). In the following paragraph, we describe the intuition of two of those models\n",
    "\n",
    "1) The Deep Relevance Matching Model (Guo et al., 2020) is a supervised IR system that looks for exact matching (opposed to partial matching with stemming). It groups each pair of terms in the query and looks for matching in the documents. For each document, the matching values are input into the feedforward network and the output is the score of the aforementioned document given the query. Since it is a supervised model, the net should be trained with sample queries and documents.\n",
    "\n",
    "2) The Neural Vector Space Model (Gysel et al., 2020) is closer to the approach that we have to take in this work, since it creates a model without evaluating the results with the queries until the model is constructed. The general idea is to train an artificial neural network that optimizes a function that minimizes the distance between n-grams (co-occurrences of size n) and the documents. The optimization of the n-grams provides a closer and simplified representation of the documents, giving more weight to the words that are discriminative between each other. This give as a result a simplified vector space and, in order to perform the ranking, the queries are transformed to n-grams and the cosine similarity is studied between the n-gram (query) and each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Proccesing data \n",
    "Before starting to implement Vector Space Models, we need to prepare the dataset to obtain only relevant data. Also, we need to treat our queries to obtain only the name of the queries (query.text). And we need to recover relevant judgment data from one txt. So, first of all, we are going to process all of our data.\n",
    "\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\anaconda\\lib\\site-packages (from pandas) (1.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\anaconda\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\anaconda\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the dataset\n",
    "\n",
    "We download the data for this work from https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-07-16.tar.gz\n",
    "\n",
    "More information about this data can be found at\n",
    "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html\n",
    "\n",
    "We choose the metadata.csv to start to create our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cord_uid                                       sha  \\\n",
      "0       ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb   \n",
      "1       02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d   \n",
      "2       ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927   \n",
      "3       2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605   \n",
      "4       9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32   \n",
      "...          ...                                       ...   \n",
      "192504  z4ro6lmh  203f36475be74229101548475d68352b939f8b5b   \n",
      "192505  hi8k8wvb  9f1bc99798e8823e690697394dcb23533a45c60e   \n",
      "192506  ma3ndg41  ffba777376718ef2a0dd74a8eab90e2bfacd240f   \n",
      "192507  wh10285j  d521c5a2dcbd79a5be606fcf586b1e0448344172   \n",
      "192508  pnl9th2c  c047bf76813106d4fd586e49164e7feddfbe352f   \n",
      "\n",
      "                      source_x  \\\n",
      "0                          PMC   \n",
      "1                          PMC   \n",
      "2                          PMC   \n",
      "3                          PMC   \n",
      "4                          PMC   \n",
      "...                        ...   \n",
      "192504            Medline; PMC   \n",
      "192505  Elsevier; Medline; PMC   \n",
      "192506            Medline; PMC   \n",
      "192507            Medline; PMC   \n",
      "192508  Elsevier; Medline; PMC   \n",
      "\n",
      "                                                    title  \\\n",
      "0       Clinical features of culture-proven Mycoplasma...   \n",
      "1       Nitric oxide: a pro-inflammatory mediator in l...   \n",
      "2         Surfactant protein-D and pulmonary host defense   \n",
      "3                    Role of endothelin-1 in lung disease   \n",
      "4       Gene expression in epithelial cells in respons...   \n",
      "...                                                   ...   \n",
      "192504  Rapid radiological improvement of COVID-19 pne...   \n",
      "192505  SARS E protein in phospholipid bilayers: an an...   \n",
      "192506  Italian Society of Interventional Cardiology (...   \n",
      "192507  Nimble, Together: A Training Program's Respons...   \n",
      "192508  Vascular Life during the COVID-19 Pandemic Rem...   \n",
      "\n",
      "                                 doi       pmcid    pubmed_id      license  \\\n",
      "0              10.1186/1471-2334-1-6    PMC35282  1.14726e+07        no-cc   \n",
      "1                       10.1186/rr14    PMC59543   1.1668e+07        no-cc   \n",
      "2                       10.1186/rr19    PMC59549   1.1668e+07        no-cc   \n",
      "3                       10.1186/rr44    PMC59574  1.16869e+07        no-cc   \n",
      "4                       10.1186/rr61    PMC59580  1.16869e+07        no-cc   \n",
      "...                              ...         ...          ...          ...   \n",
      "192504    10.1007/s15010-020-01449-w  PMC7299451     32557206        no-cc   \n",
      "192505   10.1016/j.physb.2004.11.015  PMC7127356     32288217    els-covid   \n",
      "192506             10.1002/ccd.28888  PMC7228289     32223063        no-cc   \n",
      "192507  10.1097/sla.0000000000003994  PMC7224622     32355117  cc-by-nc-nd   \n",
      "192508    10.1016/j.ejvs.2020.04.040  PMC7214295     32446539    els-covid   \n",
      "\n",
      "                                                 abstract publish_time  \\\n",
      "0       OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
      "1       Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
      "2       Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
      "3       Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
      "4       Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
      "...                                                   ...          ...   \n",
      "192504                                                NaN   2020-06-15   \n",
      "192505  Abstract We report on an anomalous X-ray refle...   2005-02-28   \n",
      "192506  COVID‐19 pandemic raised the issue to guarante...   2020-04-11   \n",
      "192507                                                NaN   2020-04-29   \n",
      "192508                                                NaN   2020-05-12   \n",
      "\n",
      "                                                  authors  \\\n",
      "0                     Madani, Tariq A; Al-Ghamdi, Aisha A   \n",
      "1       Vliet, Albert van der; Eiserich, Jason P; Cros...   \n",
      "2                                         Crouch, Erika C   \n",
      "3       Fagan, Karen A; McMurtry, Ivan F; Rodman, David M   \n",
      "4       Domachowske, Joseph B; Bonville, Cynthia A; Ro...   \n",
      "...                                                   ...   \n",
      "192504  Comel, Andrea Claudio; Mosaner, Walter; Bragan...   \n",
      "192505  Khattari, Z.; Brotons, G.; Arbely, E.; Arkin, ...   \n",
      "192506  Tarantini, Giuseppe; Fraccaro, Chiara; Chieffo...   \n",
      "192507  Bryan, Darren S.; Benjamin, Andrew J.; Schneid...   \n",
      "192508  Reyes Valdivia, Andrés; Gandarias Zúñiga, Clau...   \n",
      "\n",
      "                            journal  mag_id who_covidence_id arxiv_id  \\\n",
      "0                    BMC Infect Dis     NaN              NaN      NaN   \n",
      "1                        Respir Res     NaN              NaN      NaN   \n",
      "2                        Respir Res     NaN              NaN      NaN   \n",
      "3                        Respir Res     NaN              NaN      NaN   \n",
      "4                        Respir Res     NaN              NaN      NaN   \n",
      "...                             ...     ...              ...      ...   \n",
      "192504                    Infection     NaN              NaN      NaN   \n",
      "192505  Physica B: Condensed Matter     NaN              NaN      NaN   \n",
      "192506   Catheter Cardiovasc Interv     NaN              NaN      NaN   \n",
      "192507                     Ann Surg     NaN              NaN      NaN   \n",
      "192508     Eur J Vasc Endovasc Surg     NaN              NaN      NaN   \n",
      "\n",
      "                                           pdf_json_files  \\\n",
      "0       document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
      "1       document_parses/pdf_json/6b0567729c2143a66d737...   \n",
      "2       document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
      "3       document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
      "4       document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
      "...                                                   ...   \n",
      "192504  document_parses/pdf_json/203f36475be7422910154...   \n",
      "192505  document_parses/pdf_json/9f1bc99798e8823e69069...   \n",
      "192506  document_parses/pdf_json/ffba777376718ef2a0dd7...   \n",
      "192507  document_parses/pdf_json/d521c5a2dcbd79a5be606...   \n",
      "192508  document_parses/pdf_json/c047bf76813106d4fd586...   \n",
      "\n",
      "                                      pmc_json_files  \\\n",
      "0         document_parses/pmc_json/PMC35282.xml.json   \n",
      "1         document_parses/pmc_json/PMC59543.xml.json   \n",
      "2         document_parses/pmc_json/PMC59549.xml.json   \n",
      "3         document_parses/pmc_json/PMC59574.xml.json   \n",
      "4         document_parses/pmc_json/PMC59580.xml.json   \n",
      "...                                              ...   \n",
      "192504  document_parses/pmc_json/PMC7299451.xml.json   \n",
      "192505  document_parses/pmc_json/PMC7127356.xml.json   \n",
      "192506  document_parses/pmc_json/PMC7228289.xml.json   \n",
      "192507  document_parses/pmc_json/PMC7224622.xml.json   \n",
      "192508                                           NaN   \n",
      "\n",
      "                                                      url        s2_id  \n",
      "0       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...          NaN  \n",
      "1       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "2       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "3       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "4       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "...                                                   ...          ...  \n",
      "192504  https://doi.org/10.1007/s15010-020-01449-w; ht...  219729576.0  \n",
      "192505  https://www.ncbi.nlm.nih.gov/pubmed/32288217/;...  122247693.0  \n",
      "192506  https://www.ncbi.nlm.nih.gov/pubmed/32223063/;...  214715941.0  \n",
      "192507  https://www.ncbi.nlm.nih.gov/pubmed/32355117/;...  218468770.0  \n",
      "192508  https://www.sciencedirect.com/science/article/...  218582792.0  \n",
      "\n",
      "[192509 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dt = pd.read_csv(\"../data/metadata.csv\")\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided just to work with the papers of the PDF_JSON corpus. Therefore, the first step is to delete from the data frame the elements that are not in that folder. The number of examples is reduced from 192509 to 79755. Still, there are more documents in the pdf_json than in the data frame (over 84000), because many documents (PDF) in the corpus have the same cord_uid. Technically, the papers mapped into the same cord_uid are the same one, but with differences in the publication (if one article has been published by Elsevier and Springer, it will be mapped twice with the same cord_uid). Our take on the problem will be to consider just one of the documents associated with one cord_uid, instead of the full_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cord_uid                                       sha  \\\n",
      "0      ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb   \n",
      "1      02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d   \n",
      "2      ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927   \n",
      "3      2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605   \n",
      "4      9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32   \n",
      "...         ...                                       ...   \n",
      "79750  z4ro6lmh  203f36475be74229101548475d68352b939f8b5b   \n",
      "79751  hi8k8wvb  9f1bc99798e8823e690697394dcb23533a45c60e   \n",
      "79752  ma3ndg41  ffba777376718ef2a0dd74a8eab90e2bfacd240f   \n",
      "79753  wh10285j  d521c5a2dcbd79a5be606fcf586b1e0448344172   \n",
      "79754  pnl9th2c  c047bf76813106d4fd586e49164e7feddfbe352f   \n",
      "\n",
      "                     source_x  \\\n",
      "0                         PMC   \n",
      "1                         PMC   \n",
      "2                         PMC   \n",
      "3                         PMC   \n",
      "4                         PMC   \n",
      "...                       ...   \n",
      "79750            Medline; PMC   \n",
      "79751  Elsevier; Medline; PMC   \n",
      "79752            Medline; PMC   \n",
      "79753            Medline; PMC   \n",
      "79754  Elsevier; Medline; PMC   \n",
      "\n",
      "                                                   title  \\\n",
      "0      Clinical features of culture-proven Mycoplasma...   \n",
      "1      Nitric oxide: a pro-inflammatory mediator in l...   \n",
      "2        Surfactant protein-D and pulmonary host defense   \n",
      "3                   Role of endothelin-1 in lung disease   \n",
      "4      Gene expression in epithelial cells in respons...   \n",
      "...                                                  ...   \n",
      "79750  Rapid radiological improvement of COVID-19 pne...   \n",
      "79751  SARS E protein in phospholipid bilayers: an an...   \n",
      "79752  Italian Society of Interventional Cardiology (...   \n",
      "79753  Nimble, Together: A Training Program's Respons...   \n",
      "79754  Vascular Life during the COVID-19 Pandemic Rem...   \n",
      "\n",
      "                                doi       pmcid    pubmed_id      license  \\\n",
      "0             10.1186/1471-2334-1-6    PMC35282  1.14726e+07        no-cc   \n",
      "1                      10.1186/rr14    PMC59543   1.1668e+07        no-cc   \n",
      "2                      10.1186/rr19    PMC59549   1.1668e+07        no-cc   \n",
      "3                      10.1186/rr44    PMC59574  1.16869e+07        no-cc   \n",
      "4                      10.1186/rr61    PMC59580  1.16869e+07        no-cc   \n",
      "...                             ...         ...          ...          ...   \n",
      "79750    10.1007/s15010-020-01449-w  PMC7299451     32557206        no-cc   \n",
      "79751   10.1016/j.physb.2004.11.015  PMC7127356     32288217    els-covid   \n",
      "79752             10.1002/ccd.28888  PMC7228289     32223063        no-cc   \n",
      "79753  10.1097/sla.0000000000003994  PMC7224622     32355117  cc-by-nc-nd   \n",
      "79754    10.1016/j.ejvs.2020.04.040  PMC7214295     32446539    els-covid   \n",
      "\n",
      "                                                abstract publish_time  \\\n",
      "0      OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
      "1      Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
      "2      Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
      "3      Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
      "4      Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
      "...                                                  ...          ...   \n",
      "79750                                                NaN   2020-06-15   \n",
      "79751  Abstract We report on an anomalous X-ray refle...   2005-02-28   \n",
      "79752  COVID‐19 pandemic raised the issue to guarante...   2020-04-11   \n",
      "79753                                                NaN   2020-04-29   \n",
      "79754                                                NaN   2020-05-12   \n",
      "\n",
      "                                                 authors  \\\n",
      "0                    Madani, Tariq A; Al-Ghamdi, Aisha A   \n",
      "1      Vliet, Albert van der; Eiserich, Jason P; Cros...   \n",
      "2                                        Crouch, Erika C   \n",
      "3      Fagan, Karen A; McMurtry, Ivan F; Rodman, David M   \n",
      "4      Domachowske, Joseph B; Bonville, Cynthia A; Ro...   \n",
      "...                                                  ...   \n",
      "79750  Comel, Andrea Claudio; Mosaner, Walter; Bragan...   \n",
      "79751  Khattari, Z.; Brotons, G.; Arbely, E.; Arkin, ...   \n",
      "79752  Tarantini, Giuseppe; Fraccaro, Chiara; Chieffo...   \n",
      "79753  Bryan, Darren S.; Benjamin, Andrew J.; Schneid...   \n",
      "79754  Reyes Valdivia, Andrés; Gandarias Zúñiga, Clau...   \n",
      "\n",
      "                           journal  mag_id who_covidence_id arxiv_id  \\\n",
      "0                   BMC Infect Dis     NaN              NaN      NaN   \n",
      "1                       Respir Res     NaN              NaN      NaN   \n",
      "2                       Respir Res     NaN              NaN      NaN   \n",
      "3                       Respir Res     NaN              NaN      NaN   \n",
      "4                       Respir Res     NaN              NaN      NaN   \n",
      "...                            ...     ...              ...      ...   \n",
      "79750                    Infection     NaN              NaN      NaN   \n",
      "79751  Physica B: Condensed Matter     NaN              NaN      NaN   \n",
      "79752   Catheter Cardiovasc Interv     NaN              NaN      NaN   \n",
      "79753                     Ann Surg     NaN              NaN      NaN   \n",
      "79754     Eur J Vasc Endovasc Surg     NaN              NaN      NaN   \n",
      "\n",
      "                                          pdf_json_files  \\\n",
      "0      document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
      "1      document_parses/pdf_json/6b0567729c2143a66d737...   \n",
      "2      document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
      "3      document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
      "4      document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
      "...                                                  ...   \n",
      "79750  document_parses/pdf_json/203f36475be7422910154...   \n",
      "79751  document_parses/pdf_json/9f1bc99798e8823e69069...   \n",
      "79752  document_parses/pdf_json/ffba777376718ef2a0dd7...   \n",
      "79753  document_parses/pdf_json/d521c5a2dcbd79a5be606...   \n",
      "79754  document_parses/pdf_json/c047bf76813106d4fd586...   \n",
      "\n",
      "                                     pmc_json_files  \\\n",
      "0        document_parses/pmc_json/PMC35282.xml.json   \n",
      "1        document_parses/pmc_json/PMC59543.xml.json   \n",
      "2        document_parses/pmc_json/PMC59549.xml.json   \n",
      "3        document_parses/pmc_json/PMC59574.xml.json   \n",
      "4        document_parses/pmc_json/PMC59580.xml.json   \n",
      "...                                             ...   \n",
      "79750  document_parses/pmc_json/PMC7299451.xml.json   \n",
      "79751  document_parses/pmc_json/PMC7127356.xml.json   \n",
      "79752  document_parses/pmc_json/PMC7228289.xml.json   \n",
      "79753  document_parses/pmc_json/PMC7224622.xml.json   \n",
      "79754                                           NaN   \n",
      "\n",
      "                                                     url        s2_id  \n",
      "0      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...          NaN  \n",
      "1      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "2      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "3      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "4      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "...                                                  ...          ...  \n",
      "79750  https://doi.org/10.1007/s15010-020-01449-w; ht...  219729576.0  \n",
      "79751  https://www.ncbi.nlm.nih.gov/pubmed/32288217/;...  122247693.0  \n",
      "79752  https://www.ncbi.nlm.nih.gov/pubmed/32223063/;...  214715941.0  \n",
      "79753  https://www.ncbi.nlm.nih.gov/pubmed/32355117/;...  218468770.0  \n",
      "79754  https://www.sciencedirect.com/science/article/...  218582792.0  \n",
      "\n",
      "[79755 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "dt = dt[dt.pdf_json_files.notnull()]\n",
    "dt = dt.reset_index(drop = True)\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we drop the columns that will not add information to our information retrieval system and that do not help to map each example of the data frame with a document in the pdf_json corpus.\n",
    "\n",
    "In our opinion, these columns are doi, source_x, pmcid, pubmed_id, license, mag_id, who_covidence_id, arxiv_id, pmc_json_files, url, and s2_id. \n",
    "\n",
    "We think only need the columns cord_uid, sha, title, abstract, publish_time, authors, journal, and pdf_json_files. But, it is possible that we do not need all the columns for all the VSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cord_uid                                       sha  \\\n",
      "0      ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb   \n",
      "1      02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d   \n",
      "2      ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927   \n",
      "3      2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605   \n",
      "4      9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32   \n",
      "...         ...                                       ...   \n",
      "79750  z4ro6lmh  203f36475be74229101548475d68352b939f8b5b   \n",
      "79751  hi8k8wvb  9f1bc99798e8823e690697394dcb23533a45c60e   \n",
      "79752  ma3ndg41  ffba777376718ef2a0dd74a8eab90e2bfacd240f   \n",
      "79753  wh10285j  d521c5a2dcbd79a5be606fcf586b1e0448344172   \n",
      "79754  pnl9th2c  c047bf76813106d4fd586e49164e7feddfbe352f   \n",
      "\n",
      "                                                   title  \\\n",
      "0      Clinical features of culture-proven Mycoplasma...   \n",
      "1      Nitric oxide: a pro-inflammatory mediator in l...   \n",
      "2        Surfactant protein-D and pulmonary host defense   \n",
      "3                   Role of endothelin-1 in lung disease   \n",
      "4      Gene expression in epithelial cells in respons...   \n",
      "...                                                  ...   \n",
      "79750  Rapid radiological improvement of COVID-19 pne...   \n",
      "79751  SARS E protein in phospholipid bilayers: an an...   \n",
      "79752  Italian Society of Interventional Cardiology (...   \n",
      "79753  Nimble, Together: A Training Program's Respons...   \n",
      "79754  Vascular Life during the COVID-19 Pandemic Rem...   \n",
      "\n",
      "                                                abstract publish_time  \\\n",
      "0      OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
      "1      Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
      "2      Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
      "3      Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
      "4      Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
      "...                                                  ...          ...   \n",
      "79750                                                NaN   2020-06-15   \n",
      "79751  Abstract We report on an anomalous X-ray refle...   2005-02-28   \n",
      "79752  COVID‐19 pandemic raised the issue to guarante...   2020-04-11   \n",
      "79753                                                NaN   2020-04-29   \n",
      "79754                                                NaN   2020-05-12   \n",
      "\n",
      "                                                 authors  \\\n",
      "0                    Madani, Tariq A; Al-Ghamdi, Aisha A   \n",
      "1      Vliet, Albert van der; Eiserich, Jason P; Cros...   \n",
      "2                                        Crouch, Erika C   \n",
      "3      Fagan, Karen A; McMurtry, Ivan F; Rodman, David M   \n",
      "4      Domachowske, Joseph B; Bonville, Cynthia A; Ro...   \n",
      "...                                                  ...   \n",
      "79750  Comel, Andrea Claudio; Mosaner, Walter; Bragan...   \n",
      "79751  Khattari, Z.; Brotons, G.; Arbely, E.; Arkin, ...   \n",
      "79752  Tarantini, Giuseppe; Fraccaro, Chiara; Chieffo...   \n",
      "79753  Bryan, Darren S.; Benjamin, Andrew J.; Schneid...   \n",
      "79754  Reyes Valdivia, Andrés; Gandarias Zúñiga, Clau...   \n",
      "\n",
      "                           journal  \\\n",
      "0                   BMC Infect Dis   \n",
      "1                       Respir Res   \n",
      "2                       Respir Res   \n",
      "3                       Respir Res   \n",
      "4                       Respir Res   \n",
      "...                            ...   \n",
      "79750                    Infection   \n",
      "79751  Physica B: Condensed Matter   \n",
      "79752   Catheter Cardiovasc Interv   \n",
      "79753                     Ann Surg   \n",
      "79754     Eur J Vasc Endovasc Surg   \n",
      "\n",
      "                                          pdf_json_files  \n",
      "0      document_parses/pdf_json/d1aafb70c066a2068b027...  \n",
      "1      document_parses/pdf_json/6b0567729c2143a66d737...  \n",
      "2      document_parses/pdf_json/06ced00a5fc04215949aa...  \n",
      "3      document_parses/pdf_json/348055649b6b8cf2b9a37...  \n",
      "4      document_parses/pdf_json/5f48792a5fa08bed9f560...  \n",
      "...                                                  ...  \n",
      "79750  document_parses/pdf_json/203f36475be7422910154...  \n",
      "79751  document_parses/pdf_json/9f1bc99798e8823e69069...  \n",
      "79752  document_parses/pdf_json/ffba777376718ef2a0dd7...  \n",
      "79753  document_parses/pdf_json/d521c5a2dcbd79a5be606...  \n",
      "79754  document_parses/pdf_json/c047bf76813106d4fd586...  \n",
      "\n",
      "[79755 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "columns_to_delete = [\"doi\", \"source_x\", \"pmcid\", \"pubmed_id\", \"license\", \"mag_id\", \"who_covidence_id\", \"arxiv_id\", \"pmc_json_files\", \"url\", \"s2_id\"]\n",
    "# dt_original = dt\n",
    "dt = dt.drop(columns_to_delete, axis = 1)\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79755\n"
     ]
    }
   ],
   "source": [
    "print(dt.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cord_uid                                                   ug7v899j\n",
      "sha                        d1aafb70c066a2068b02786f8929fd9c900897fb\n",
      "title             Clinical features of culture-proven Mycoplasma...\n",
      "abstract          OBJECTIVE: This retrospective chart review des...\n",
      "publish_time                                             2001-07-04\n",
      "authors                         Madani, Tariq A; Al-Ghamdi, Aisha A\n",
      "journal                                              BMC Infect Dis\n",
      "pdf_json_files    document_parses/pdf_json/d1aafb70c066a2068b027...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Document 1 \n",
    "print(dt.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia\n",
      "OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.\n"
     ]
    }
   ],
   "source": [
    "# Document 1\n",
    "print(dt.iloc[0].title)\n",
    "print(dt.iloc[0].abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the test_queries file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the test queries, we use the script that appears at https://towardsdatascience.com/download-and-parse-trec-covid-data-8f9840686c37\n",
    "\n",
    "The file test_queries.txt contains 50 queries (topics). Each query has three parts: query, question, and narrative. We will only need the query part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['query'])\n"
     ]
    }
   ],
   "source": [
    "topics = {}\n",
    "root = ET.parse(\"../queries/test_queries.xml\").getroot()\n",
    "for topic in root.findall(\"topic\"):\n",
    "    topic_number = int(topic.attrib[\"number\"])\n",
    "    topics[topic_number] = {}\n",
    "    for query in topic.findall(\"query\"):\n",
    "        topics[topic_number][\"query\"] = query.text  # We only need the query part\n",
    "        \n",
    "print(topics[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'query': 'coronavirus origin'}, 2: {'query': 'coronavirus response to weather changes'}, 3: {'query': 'coronavirus immunity'}, 4: {'query': 'how do people die from the coronavirus'}, 5: {'query': 'animal models of COVID-19'}, 6: {'query': 'coronavirus test rapid testing'}, 7: {'query': 'serological tests for coronavirus'}, 8: {'query': 'coronavirus under reporting'}, 9: {'query': 'coronavirus in Canada'}, 10: {'query': 'coronavirus social distancing impact'}, 11: {'query': 'coronavirus hospital rationing'}, 12: {'query': 'coronavirus quarantine'}, 13: {'query': 'how does coronavirus spread'}, 14: {'query': 'coronavirus super spreaders'}, 15: {'query': 'coronavirus outside body'}, 16: {'query': 'how long does coronavirus survive on surfaces'}, 17: {'query': 'coronavirus clinical trials'}, 18: {'query': 'masks prevent coronavirus'}, 19: {'query': 'what alcohol sanitizer kills coronavirus'}, 20: {'query': 'coronavirus and ACE inhibitors'}, 21: {'query': 'coronavirus mortality'}, 22: {'query': 'coronavirus heart impacts'}, 23: {'query': 'coronavirus hypertension'}, 24: {'query': 'coronavirus diabetes'}, 25: {'query': 'coronavirus biomarkers'}, 26: {'query': 'coronavirus early symptoms'}, 27: {'query': 'coronavirus asymptomatic'}, 28: {'query': 'coronavirus hydroxychloroquine'}, 29: {'query': 'coronavirus drug repurposing'}, 30: {'query': 'coronavirus remdesivir'}, 31: {'query': 'difference between coronavirus and flu'}, 32: {'query': 'coronavirus subtypes'}, 33: {'query': 'coronavirus vaccine candidates'}, 34: {'query': 'coronavirus recovery'}, 35: {'query': 'coronavirus public datasets'}, 36: {'query': 'SARS-CoV-2 spike structure'}, 37: {'query': 'SARS-CoV-2 phylogenetic analysis'}, 38: {'query': 'COVID inflammatory response'}, 39: {'query': 'COVID-19 cytokine storm'}, 40: {'query': 'coronavirus mutations'}, 41: {'query': 'COVID-19 in African-Americans'}, 42: {'query': 'Vitamin D and COVID-19'}, 43: {'query': 'violence during pandemic'}, 44: {'query': 'impact of masks on coronavirus transmission'}, 45: {'query': 'coronavirus mental health impact'}, 46: {'query': 'dexamethasone coronavirus'}, 47: {'query': 'COVID-19 outcomes in children'}, 48: {'query': 'school reopening coronavirus'}, 49: {'query': 'post-infection COVID-19 immunity'}, 50: {'query': 'mRNA vaccine coronavirus'}}\n"
     ]
    }
   ],
   "source": [
    "# We show all queries. Topics is a python dictionary \n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 coronavirus origin\n",
      "2 coronavirus response to weather changes\n",
      "3 coronavirus immunity\n",
      "4 how do people die from the coronavirus\n",
      "5 animal models of COVID-19\n",
      "6 coronavirus test rapid testing\n",
      "7 serological tests for coronavirus\n",
      "8 coronavirus under reporting\n",
      "9 coronavirus in Canada\n",
      "10 coronavirus social distancing impact\n",
      "11 coronavirus hospital rationing\n",
      "12 coronavirus quarantine\n",
      "13 how does coronavirus spread\n",
      "14 coronavirus super spreaders\n",
      "15 coronavirus outside body\n",
      "16 how long does coronavirus survive on surfaces\n",
      "17 coronavirus clinical trials\n",
      "18 masks prevent coronavirus\n",
      "19 what alcohol sanitizer kills coronavirus\n",
      "20 coronavirus and ACE inhibitors\n",
      "21 coronavirus mortality\n",
      "22 coronavirus heart impacts\n",
      "23 coronavirus hypertension\n",
      "24 coronavirus diabetes\n",
      "25 coronavirus biomarkers\n",
      "26 coronavirus early symptoms\n",
      "27 coronavirus asymptomatic\n",
      "28 coronavirus hydroxychloroquine\n",
      "29 coronavirus drug repurposing\n",
      "30 coronavirus remdesivir\n",
      "31 difference between coronavirus and flu\n",
      "32 coronavirus subtypes\n",
      "33 coronavirus vaccine candidates\n",
      "34 coronavirus recovery\n",
      "35 coronavirus public datasets\n",
      "36 SARS-CoV-2 spike structure\n",
      "37 SARS-CoV-2 phylogenetic analysis\n",
      "38 COVID inflammatory response\n",
      "39 COVID-19 cytokine storm\n",
      "40 coronavirus mutations\n",
      "41 COVID-19 in African-Americans\n",
      "42 Vitamin D and COVID-19\n",
      "43 violence during pandemic\n",
      "44 impact of masks on coronavirus transmission\n",
      "45 coronavirus mental health impact\n",
      "46 dexamethasone coronavirus\n",
      "47 COVID-19 outcomes in children\n",
      "48 school reopening coronavirus\n",
      "49 post-infection COVID-19 immunity\n",
      "50 mRNA vaccine coronavirus\n"
     ]
    }
   ],
   "source": [
    "# We show only queries names\n",
    "for key in topics: \n",
    "    value = topics[key]\n",
    "    print(key, value[\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the relevance_judgements file\n",
    "Here, we implement the script to read the relevance judgment. This, is the information needed to evaluate our system. For that, the round_id is not needed and it is therefore omitted. Also, relevancy (0, 1 or 2) is binarized (1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       topic_id  cord_uid relevancy\n",
      "0             1  005b2j4b         1\n",
      "1             1  00fmeepz         1\n",
      "2             1  010vptx3         1\n",
      "3             1  0194oljo         1\n",
      "4             1  021q9884         1\n",
      "...         ...       ...       ...\n",
      "69313        50  zvop8bxh         1\n",
      "69314        50  zwf26o63         1\n",
      "69315        50  zwsvlnwe         0\n",
      "69316        50  zxr01yln         1\n",
      "69317        50  zz8wvos9         1\n",
      "\n",
      "[69318 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "relevance_data = pd.read_csv(\"../queries/relevance_judgements.txt\", sep=\" \", header=None)\n",
    "relevance_data.columns = [\"topic_id\", \"round_id\", \"cord_uid\", \"relevancy\"]\n",
    "relevance_data = relevance_data.drop(\"round_id\" ,axis = 1)\n",
    "relevance_data['relevancy'] = relevance_data['relevancy'].replace([2],'1')\n",
    "print(relevance_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "With all the metadata (and optionally json_pdf), test topics, and relevance judgment, we are prepared to build and validate the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A simple VSM implementation\n",
    "We have adapted the simple vector space model implementation for our code. \n",
    "\n",
    "Here, we use one TF-IDF model and the VSM provides a document ranking in descending order of relevance due to the cosine measure.\n",
    "\n",
    "On the TF-IDF model, each pair-document is created using the weight:\n",
    "$$w_{t,d}=log(1+tf_{t,d})xlog_{10}(N/df_{t});$$\n",
    "with: \n",
    "\n",
    "- t = term or word\n",
    "    \n",
    "- d = document\n",
    "    \n",
    "- $tf_{t,d}$ = document frequency of t. It is the number of times that t occurs in d \n",
    "- $df_{t}$ = inverse document frequency.\n",
    "\n",
    "\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we define all the python functions that we will use to create the VSM to ranking the documents with the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first install the NLTK toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from nltk) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now install the gensim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in d:\\anaconda\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\anaconda\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in d:\\anaconda\\lib\\site-packages (from gensim) (1.18.2)\n",
      "Requirement already satisfied: six>=1.5.0 in d:\\anaconda\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: Cython==0.29.14 in d:\\anaconda\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\anaconda\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to download the NLTK data bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\enriq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the required software is now installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now import the functions provided by NLTK to perform tokenizing considering punctuation signs.\n",
    "from nltk.tokenize import wordpunct_tokenize, regexp_tokenize\n",
    "# Next, we import required functions to filter-out stopwords for the English language.\n",
    "from nltk.corpus import stopwords\n",
    "# Now we import the function that implements the Porter's stemming algorithm.\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is aimed at preprocessing each document in the collection. We write a function that receives one document and returns a list containing all STEMS in the document whose associated token is longer than 2 characters and is NOT an (English) stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(doc): # Each doc is each dt row. We will only use title and abstract: dt.iloc[i].title and dt.iloc[i].abstract\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    if type(doc.title) != str and type(doc.abstract) != str: # For empty documents without title and abstract\n",
    "        final = [\"\"]\n",
    "    else:\n",
    "        if type(doc.title) == str: \n",
    "            tokens = wordpunct_tokenize(doc.title)\n",
    "        if type(doc.abstract) == str:\n",
    "            tokens.extend(wordpunct_tokenize(doc.abstract))\n",
    "        # clean saves the words (in lower case) that are not included in stopset\n",
    "        clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2 and \"%\" not in token]\n",
    "        final = [stemmer.stem(word) for word in clean]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clinic', 'featur', 'cultur', 'proven', 'mycoplasma', 'pneumonia', 'infect', 'king', 'abdulaziz', 'univers', 'hospit', 'jeddah', 'saudi', 'arabia', 'object', 'retrospect', 'chart', 'review', 'describ', 'epidemiolog', 'clinic', 'featur', 'patient', 'cultur', 'proven', 'mycoplasma', 'pneumonia', 'infect', 'king', 'abdulaziz', 'univers', 'hospit', 'jeddah', 'saudi', 'arabia', 'method', 'patient', 'posit', 'pneumonia', 'cultur', 'respiratori', 'specimen', 'januari', '1997', 'decemb', '1998', 'identifi', 'microbiolog', 'record', 'chart', 'patient', 'review', 'result', 'patient', 'identifi', 'requir', 'admiss', 'infect', 'commun', 'acquir', 'infect', 'affect', 'age', 'group', 'common', 'infant', 'pre', 'school', 'children', 'occur', 'year', 'round', 'common', 'fall', 'spring', 'three', 'quarter', 'patient', 'comorbid', 'twenti', 'four', 'isol', 'associ', 'pneumonia', 'upper', 'respiratori', 'tract', 'infect', 'bronchiol', 'cough', 'fever', 'malais', 'common', 'symptom', 'crepit', 'wheez', 'common', 'sign', 'patient', 'pneumonia', 'crepit', 'bronchial', 'breath', 'immunocompromis', 'patient', 'like', 'non', 'immunocompromis', 'patient', 'present', 'pneumonia', 'versu', 'patient', 'pneumonia', 'unev', 'recoveri', 'recov', 'follow', 'complic', 'die', 'pneumonia', 'infect', 'die', 'due', 'underli', 'comorbid', 'patient', 'die', 'pneumonia', 'pneumonia', 'comorbid', 'conclus', 'result', 'similar', 'publish', 'data', 'except', 'find', 'infect', 'common', 'infant', 'preschool', 'children', 'mortal', 'rate', 'pneumonia', 'patient', 'comorbid', 'high']\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_document(dt.iloc[0])) # Print STEMS for the document 1\n",
    "print(type(dt.iloc[0].title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cord_uid                                                   n06og3cw\n",
      "sha                        8d35867e078939b7f20187322e41011cec8b8cb3\n",
      "title                                                           NaN\n",
      "abstract                                                        NaN\n",
      "publish_time                                             2020-05-13\n",
      "authors           De Coninck, David; d'Haenens, Leen; Matthijs, ...\n",
      "journal                                               Public Health\n",
      "pdf_json_files    document_parses/pdf_json/8d35867e078939b7f2018...\n",
      "Name: 29264, dtype: object\n",
      "nan\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(dt.iloc[29264])\n",
    "print(dt.iloc[29264].title)\n",
    "print(type(dt.iloc[29264].title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_document(dt.iloc[29264]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we need a function to preprocess each document, we need a function to preprocess each query to returns a list containing all STEMS in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(q):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = wordpunct_tokenize(q)\n",
    "    # clean saves the words (in lower case) that are not included in stopset\n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2] \n",
    "    final = [stemmer.stem(word) for word in clean]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the function to preprocess documents and queries, we need to implement a function to create a dictionary containing the mappings \n",
    "$$WORD\\_ID \\ ->\\ WORD$$\n",
    "This dictionary is required to create vector-based word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Dictionary: Different words (STEMS) in the collection (CORPUS, all documents) with their IDs\n",
    "def create_dictionary(docs):\n",
    "    # List all pre-processing documents\n",
    "    pdocs = [preprocess_document(docs.iloc[i]) for i in range(docs.shape[0])]\n",
    "    # Build the dictionary with corpora (gensim)\n",
    "    dictionary = corpora.Dictionary(pdocs)\n",
    "    # Save in a file\n",
    "    dictionary.save('simple_vsm.dict')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have built the dictionary containing the vocabulary that we will use for indexing. \n",
    "\n",
    "Next, we will write a function that creates the bag of words-based representation for each document in the collection.\n",
    "\n",
    "One bag of words model is a vector representation that contains for each vector the frequency of the words associate with its word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2bows(allData, dictionary):\n",
    "    docs = [preprocess_document(allData.iloc[i]) for i in range(allData.shape[0])]\n",
    "    # We obtain the set of frequencies for each term\n",
    "    vectors = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    corpora.MmCorpus.serialize('simple_vsm_docs.mm', vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the funcion docs2bows we have basically TF-weighted vectors. \n",
    "\n",
    "We now want to convert these vectors into their TF-IDF weighted counterparts. We need, however, to import the models module from Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "##############################\n",
    "###### COPIAR LO DE ABAJO#####\n",
    "##############################\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster-VSM\n",
    "Inspiring ourselves in the article by Ke et al. (2020) we decided to implement a VSM model in which the results are documents are grouped in clusters (using the k-means algorithm) and then, the tfidf model is built using the centroids of these clusters instead of the document. This relaxes the computational charge of doing query, while adding computational charge to the model building. Also, we lost some precision in the document scoring, since all the documents in the same query will get the same score. However, the bigger the number of clusters, the better the precision in scoring, but it will take more time to launch the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def create_TF_IDF_model_cluster(data, filename = 'simple_vsm_docs_CBR.mm'):\n",
    "    number_of_clusters = 200\n",
    "    dictionary = create_dictionary(data) # Create the dictionary\n",
    "    vectors = docs2bows(data, dictionary) # We will not really use this function for this case because we will use gensim models.\n",
    "    \n",
    "    # Extra step of cluster-VSM. We perform k-means and use the centroids as the vectors of our vector space model\n",
    "    matrix = []\n",
    "    for vector in vectors :\n",
    "        matrix_row = [0]*len(dictionary)\n",
    "        for i in vector :\n",
    "            matrix_row[i[0]] = i[1]\n",
    "        matrix.append(matrix_row)\n",
    "    matrix = np.array(matrix)\n",
    "    kmeans = KMeans(n_clusters=number_of_clusters, random_state=0).fit(matrix)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    cluster_centers_vectors = []\n",
    "    for row in cluster_centers :\n",
    "        doc_vector = []\n",
    "        for j,element in enumerate(row) :\n",
    "            if element > 0 :\n",
    "                doc_vector.append((j,element))\n",
    "        cluster_centers_vectors.append(doc_vector)\n",
    "    cluster_asignments = [[]]*number_of_clusters\n",
    "    for i,cluster in enumerate(kmeans.labels_) :\n",
    "        cluster_asignments[cluster].append(i)\n",
    "            \n",
    "    \n",
    "    corpora.MmCorpus.serialize(filename, cluster_centers_vectors)\n",
    "    loaded_allData = corpora.MmCorpus(filename)\n",
    "    print(loaded_allData)\n",
    "    tfidf = models.TfidfModel(loaded_allData)\n",
    "    print(\"tfidf\", tfidf)\n",
    "    return tfidf, dictionary, cluster_asignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, one would want to build the model using all the data. However, the k-means implementation is not optimized for a 80000x10000 matrix and it would work. Again, we are going to consider for each query it's document of its relevance judgment. This is not what one would normally do, but for purposes of exemplification and show the functioning, we consider it a small price to pay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now create the TF-IDF model.\n",
    "# ** THIS IS NOT NECESSARY. IT IS ONLY FOR TESTING. THIS TAKE PLENTY OF TIME.\n",
    "#tfidfm,dictionary,clusters_asignments = create_TF_IDF_model(dt)\n",
    "#print(tfidfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally create a function that given the dt and the topics provides a document ranking, according to the cosine measure, sorted in descending order of relevance (the better documents at the beginning). The function returns the ranking for all the documents but only shows the ten first positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from gensim import similarities\n",
    " \n",
    "    \n",
    "def launch_query_cluster(allData, q, number,tfidf, dictionary, cluster_asignments, filename = 'simple_vsm_docs_CBR.mm', f=\"null\"):\n",
    "    #tfidf, dictionary, cluster_asignments = create_TF_IDF_model(allData) \n",
    "    loaded_allData = corpora.MmCorpus(filename)\n",
    "    index = similarities.MatrixSimilarity(loaded_allData, num_features=len(dictionary))\n",
    "    pq = preprocess_query(q)\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = tfidf[vq]\n",
    "    sim = index[qtfidf]\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "    ranking_real = []\n",
    "    \n",
    "    # We do a ranking with the documents of each cluster. Documents in the same cluster will receive the same score\n",
    "    for i in ranking :\n",
    "        cluster = i[0]\n",
    "        ranking_real = ranking_real + [(e,i[1]) for e in cluster_asignments[cluster]]\n",
    "        \n",
    "        \n",
    "    print(\"QUERY:\",q)\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0,10) :\n",
    "        print(\"[ Score = \"+str(ranking[i][1])+\" ] \"+allData.iloc[ranking[i][0]].title)\n",
    "        if f!=\"null\":\n",
    "            f.write(str(number)+\" Q0 \"+str(allData.iloc[ranking[i][0]].cord_uid)+\" \"+str(i+1)+\" \"+str(ranking[i][1])+\" mySystem \\n\")\n",
    "    \"\"\"\n",
    "    pos = 1\n",
    "    for doc, score in ranking_real:\n",
    "        if ( pos <=10 ): # First ten positions\n",
    "            print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + allData.iloc[doc].title); \n",
    "            if f!=\"null\":\n",
    "                f.write(str(number)+\" Q0 \"+str(allData.iloc[doc].cord_uid)+\" \"+str(pos)+\" \"+str(round(score,3))+\" mySystem \\n\")\n",
    "        else: \n",
    "            if f!=\"null\":\n",
    "                f.write(str(number)+\" Q0 \"+str(allData.iloc[doc].cord_uid)+\" \"+str(pos)+\" \"+str(round(score,3))+\" mySystem \\n\")\n",
    "            else:\n",
    "                break\n",
    "        pos += 1\n",
    "    return ranking_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can launch any query we see fit to our newly created Information Retrieval engine.\n",
    "\n",
    "To do a little test, we choose the query 1: \"coronavirus origin\".\n",
    "\n",
    "We know that this program takes plenty of time if we use all the dt (pdf_json) dataset, to avoid that, we have decided to choose only the documents that appear in relevance_data (relevance_judgements.txt). With this strategy we can use the VSM in an acceptable time, and, also, we can evaluate our ranking with the relevance_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "QUERY NUMBER  1\n",
      "MmCorpus(200 documents, 8416 features, 48616 non-zero entries)\n",
      "tfidf TfidfModel(num_docs=200, num_nnz=48616)\n",
      "QUERY: coronavirus origin\n",
      "[ Score = 0.508 ] NSs Encoded by Groundnut Bud Necrosis Virus Is a Bifunctional Enzyme\n",
      "[ Score = 0.508 ] Comparative Efficacy of Hemagglutinin, Nucleoprotein, and Matrix 2 Protein Gene-Based Vaccination against H5N1 Influenza in Mouse and Ferret\n",
      "[ Score = 0.508 ] Elevation of Intact and Proteolytic Fragments of Acute Phase Proteins Constitutes the Earliest Systemic Antiviral Response in HIV-1 Infection\n",
      "[ Score = 0.508 ] Large-scale evolutionary surveillance of the 2009 H1N1 influenza A virus using resequencing arrays\n",
      "[ Score = 0.508 ] Angiotensin-converting enzyme 2 autoantibodies: further evidence for a role of the renin-angiotensin system in inflammation\n",
      "[ Score = 0.508 ] Diagnostic value of triggering receptor expressed on myeloid cells-1 and C-reactive protein for patients with lung infiltrates: an observational study\n",
      "[ Score = 0.508 ] PhEVER: a database for the global exploration of virus–host evolutionary relationships\n",
      "[ Score = 0.508 ] Tylosema esculentum (Marama) Tuber and Bean Extracts Are Strong Antiviral Agents against Rotavirus Infection\n",
      "[ Score = 0.508 ] Antibody-Mediated “Universal” Osteoclast Targeting Platform using Calcitonin as a Model Drug\n",
      "[ Score = 0.508 ] Making sense of perceptions of risk of diseases and vaccinations: a qualitative study combining models of health beliefs, decision-making and risk perception\n",
      "0.8467621803283691seconds\n",
      "##############\n",
      "QUERY NUMBER  2\n",
      "MmCorpus(200 documents, 8480 features, 43084 non-zero entries)\n",
      "tfidf TfidfModel(num_docs=200, num_nnz=43084)\n",
      "QUERY: coronavirus response to weather changes\n",
      "[ Score = 0.352 ] Comparisons of substitution, insertion and deletion probes for resequencing and mutational analysis using oligonucleotide microarrays\n",
      "[ Score = 0.352 ] ElaD, a Deubiquitinating Protease Expressed by E. coli\n",
      "[ Score = 0.352 ] Disease ecology and the global emergence of zoonotic pathogens\n",
      "[ Score = 0.352 ] A large and accurate collection of peptidase cleavages in the MEROPS database\n",
      "[ Score = 0.352 ] Outdoor environments and human pathogens in air\n",
      "[ Score = 0.352 ] Let the sun shine in: effects of ultraviolet radiation on invasive pneumococcal disease risk in Philadelphia, Pennsylvania\n",
      "[ Score = 0.352 ] Using Satellite Images of Environmental Changes to Predict Infectious Disease Outbreaks\n",
      "[ Score = 0.352 ] Predicting Drug-Target Interaction Networks Based on Functional Groups and Biological Features\n",
      "[ Score = 0.352 ] Large-scale evolutionary surveillance of the 2009 H1N1 influenza A virus using resequencing arrays\n",
      "[ Score = 0.352 ] Human monoclonal IgG selection of Plasmodium falciparum for the expression of placental malaria-specific variant surface antigens\n",
      "0.6951615810394287seconds\n",
      "##############\n",
      "QUERY NUMBER  3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-d30d39f5a6fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"QUERY NUMBER \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtopi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mtfidfm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclusters_asignments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_TF_IDF_model_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_rel_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mrank_cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlaunch_query_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_rel_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"query\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidfm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclusters_asignments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-ac3cc6d7fea8>\u001b[0m in \u001b[0;36mcreate_TF_IDF_model_cluster\u001b[1;34m(data, filename)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_TF_IDF_model_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'simple_vsm_docs_CBR.mm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnumber_of_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Create the dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocs2bows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We will not really use this function for this case because we will use gensim models.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-37441f8162ce>\u001b[0m in \u001b[0;36mcreate_dictionary\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# List all pre-processing documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Build the dictionary with corpora (gensim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-37441f8162ce>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# List all pre-processing documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Build the dictionary with corpora (gensim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-51c0d976a2a2>\u001b[0m in \u001b[0;36mpreprocess_document\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Each doc is each dt row. We will only use title and abstract: dt.iloc[i].title and dt.iloc[i].abstract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mstopset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstract\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# For empty documents without title and abstract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     23\u001b[0m         return [\n\u001b[0;32m     24\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         ]\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No such file or directory: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "all_rankings_cluster = []\n",
    "for topi in range(len(topics)):\n",
    "    # We select all cord_uid in relevance_data for the query number topi\n",
    "    cords_relevance_data = set()\n",
    "    aux = 0\n",
    "    for i in range(topi+1):\n",
    "        aux += relevance_data.loc[relevance_data.topic_id==(i)].shape[0]\n",
    "    for i in range(relevance_data.loc[relevance_data.topic_id==(topi+1)].shape[0]):\n",
    "        cords_relevance_data.add(relevance_data.loc[relevance_data.topic_id==(topi+1)].cord_uid[aux+i])\n",
    "    # We select all cord_uid that are in both (relevance_data and dt) for the query number topi\n",
    "    dt_rel_data = pd.DataFrame()\n",
    "    aux = 0\n",
    "    for i in range(dt.cord_uid.shape[0]):\n",
    "        if dt.cord_uid[i] in cords_relevance_data:\n",
    "            dt_rel_data[aux] = dt.iloc[i]\n",
    "            aux +=1\n",
    "    dt_rel_data = dt_rel_data.T\n",
    "    # Now we rank for the query number topi\n",
    "    print(\"##############\")\n",
    "    print(\"QUERY NUMBER \", (topi+1))\n",
    "    value = topics[(topi+1)]\n",
    "    tfidfm,dictionary,clusters_asignments = create_TF_IDF_model_cluster(dt_rel_data)\n",
    "    t0 = time.time()\n",
    "    rank_cluster = launch_query_cluster(dt_rel_data, value[\"query\"], topi+1,tfidfm, dictionary, clusters_asignments)\n",
    "    t1 = time.time()\n",
    "    all_rankings_cluster.append(rank_cluster)\n",
    "    print(str(t1-t0)+\"seconds\")\n",
    "\n",
    "'''\n",
    "for k in range(1,51) :\n",
    "    # We select all cord_uid in relevance_data for the query k\n",
    "    cords_relevance_data = set()\n",
    "    for i in range(relevance_data.loc[relevance_data.topic_id==k].shape[0]):\n",
    "        cords_relevance_data.add(relevance_data.loc[relevance_data.topic_id==k].cord_uid[i])\n",
    "\n",
    "    # We select all cord_uid that are in both (relevance_data and dt) for the query 1\n",
    "    dt_rel_data = pd.DataFrame()\n",
    "    aux = 0\n",
    "\n",
    "    for i in range(dt.cord_uid.shape[0]):\n",
    "        #print(dt.cord_uid[i])\n",
    "        if dt.cord_uid[i] in cords_relevance_data:\n",
    "            #print(dt.cord_uid[i])\n",
    "            dt_rel_data[aux] = dt.iloc[i]\n",
    "            aux +=1\n",
    "    dt_rel_data = dt_rel_data.T\n",
    "    \n",
    "    value = topics[k]\n",
    "    tfidfm,dictionary,clusters_asignments = create_TF_IDF_model(dt_rel_data)\n",
    "    t0 = time.time()\n",
    "    rank = launch_query(dt_rel_data, value[\"query\"], 1,tfidfm, dictionary, clusters_asignments)\n",
    "    t1 = time.time()\n",
    "    print(str(t1-t0)+\"seconds\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
