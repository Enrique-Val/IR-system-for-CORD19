{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Evaluating a COVID-19 oriented Information Retrieval Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Proccesing data \n",
    "Firstly, we are going to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cord_uid                                       sha  \\\n",
      "0       ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb   \n",
      "1       02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d   \n",
      "2       ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927   \n",
      "3       2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605   \n",
      "4       9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32   \n",
      "...          ...                                       ...   \n",
      "192504  z4ro6lmh  203f36475be74229101548475d68352b939f8b5b   \n",
      "192505  hi8k8wvb  9f1bc99798e8823e690697394dcb23533a45c60e   \n",
      "192506  ma3ndg41  ffba777376718ef2a0dd74a8eab90e2bfacd240f   \n",
      "192507  wh10285j  d521c5a2dcbd79a5be606fcf586b1e0448344172   \n",
      "192508  pnl9th2c  c047bf76813106d4fd586e49164e7feddfbe352f   \n",
      "\n",
      "                      source_x  \\\n",
      "0                          PMC   \n",
      "1                          PMC   \n",
      "2                          PMC   \n",
      "3                          PMC   \n",
      "4                          PMC   \n",
      "...                        ...   \n",
      "192504            Medline; PMC   \n",
      "192505  Elsevier; Medline; PMC   \n",
      "192506            Medline; PMC   \n",
      "192507            Medline; PMC   \n",
      "192508  Elsevier; Medline; PMC   \n",
      "\n",
      "                                                    title  \\\n",
      "0       Clinical features of culture-proven Mycoplasma...   \n",
      "1       Nitric oxide: a pro-inflammatory mediator in l...   \n",
      "2         Surfactant protein-D and pulmonary host defense   \n",
      "3                    Role of endothelin-1 in lung disease   \n",
      "4       Gene expression in epithelial cells in respons...   \n",
      "...                                                   ...   \n",
      "192504  Rapid radiological improvement of COVID-19 pne...   \n",
      "192505  SARS E protein in phospholipid bilayers: an an...   \n",
      "192506  Italian Society of Interventional Cardiology (...   \n",
      "192507  Nimble, Together: A Training Program's Respons...   \n",
      "192508  Vascular Life during the COVID-19 Pandemic Rem...   \n",
      "\n",
      "                                 doi       pmcid    pubmed_id      license  \\\n",
      "0              10.1186/1471-2334-1-6    PMC35282  1.14726e+07        no-cc   \n",
      "1                       10.1186/rr14    PMC59543   1.1668e+07        no-cc   \n",
      "2                       10.1186/rr19    PMC59549   1.1668e+07        no-cc   \n",
      "3                       10.1186/rr44    PMC59574  1.16869e+07        no-cc   \n",
      "4                       10.1186/rr61    PMC59580  1.16869e+07        no-cc   \n",
      "...                              ...         ...          ...          ...   \n",
      "192504    10.1007/s15010-020-01449-w  PMC7299451     32557206        no-cc   \n",
      "192505   10.1016/j.physb.2004.11.015  PMC7127356     32288217    els-covid   \n",
      "192506             10.1002/ccd.28888  PMC7228289     32223063        no-cc   \n",
      "192507  10.1097/sla.0000000000003994  PMC7224622     32355117  cc-by-nc-nd   \n",
      "192508    10.1016/j.ejvs.2020.04.040  PMC7214295     32446539    els-covid   \n",
      "\n",
      "                                                 abstract publish_time  \\\n",
      "0       OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
      "1       Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
      "2       Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
      "3       Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
      "4       Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
      "...                                                   ...          ...   \n",
      "192504                                                NaN   2020-06-15   \n",
      "192505  Abstract We report on an anomalous X-ray refle...   2005-02-28   \n",
      "192506  COVID‐19 pandemic raised the issue to guarante...   2020-04-11   \n",
      "192507                                                NaN   2020-04-29   \n",
      "192508                                                NaN   2020-05-12   \n",
      "\n",
      "                                                  authors  \\\n",
      "0                     Madani, Tariq A; Al-Ghamdi, Aisha A   \n",
      "1       Vliet, Albert van der; Eiserich, Jason P; Cros...   \n",
      "2                                         Crouch, Erika C   \n",
      "3       Fagan, Karen A; McMurtry, Ivan F; Rodman, David M   \n",
      "4       Domachowske, Joseph B; Bonville, Cynthia A; Ro...   \n",
      "...                                                   ...   \n",
      "192504  Comel, Andrea Claudio; Mosaner, Walter; Bragan...   \n",
      "192505  Khattari, Z.; Brotons, G.; Arbely, E.; Arkin, ...   \n",
      "192506  Tarantini, Giuseppe; Fraccaro, Chiara; Chieffo...   \n",
      "192507  Bryan, Darren S.; Benjamin, Andrew J.; Schneid...   \n",
      "192508  Reyes Valdivia, Andrés; Gandarias Zúñiga, Clau...   \n",
      "\n",
      "                            journal  mag_id who_covidence_id arxiv_id  \\\n",
      "0                    BMC Infect Dis     NaN              NaN      NaN   \n",
      "1                        Respir Res     NaN              NaN      NaN   \n",
      "2                        Respir Res     NaN              NaN      NaN   \n",
      "3                        Respir Res     NaN              NaN      NaN   \n",
      "4                        Respir Res     NaN              NaN      NaN   \n",
      "...                             ...     ...              ...      ...   \n",
      "192504                    Infection     NaN              NaN      NaN   \n",
      "192505  Physica B: Condensed Matter     NaN              NaN      NaN   \n",
      "192506   Catheter Cardiovasc Interv     NaN              NaN      NaN   \n",
      "192507                     Ann Surg     NaN              NaN      NaN   \n",
      "192508     Eur J Vasc Endovasc Surg     NaN              NaN      NaN   \n",
      "\n",
      "                                           pdf_json_files  \\\n",
      "0       document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
      "1       document_parses/pdf_json/6b0567729c2143a66d737...   \n",
      "2       document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
      "3       document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
      "4       document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
      "...                                                   ...   \n",
      "192504  document_parses/pdf_json/203f36475be7422910154...   \n",
      "192505  document_parses/pdf_json/9f1bc99798e8823e69069...   \n",
      "192506  document_parses/pdf_json/ffba777376718ef2a0dd7...   \n",
      "192507  document_parses/pdf_json/d521c5a2dcbd79a5be606...   \n",
      "192508  document_parses/pdf_json/c047bf76813106d4fd586...   \n",
      "\n",
      "                                      pmc_json_files  \\\n",
      "0         document_parses/pmc_json/PMC35282.xml.json   \n",
      "1         document_parses/pmc_json/PMC59543.xml.json   \n",
      "2         document_parses/pmc_json/PMC59549.xml.json   \n",
      "3         document_parses/pmc_json/PMC59574.xml.json   \n",
      "4         document_parses/pmc_json/PMC59580.xml.json   \n",
      "...                                              ...   \n",
      "192504  document_parses/pmc_json/PMC7299451.xml.json   \n",
      "192505  document_parses/pmc_json/PMC7127356.xml.json   \n",
      "192506  document_parses/pmc_json/PMC7228289.xml.json   \n",
      "192507  document_parses/pmc_json/PMC7224622.xml.json   \n",
      "192508                                           NaN   \n",
      "\n",
      "                                                      url        s2_id  \n",
      "0       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...          NaN  \n",
      "1       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "2       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "3       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "4       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "...                                                   ...          ...  \n",
      "192504  https://doi.org/10.1007/s15010-020-01449-w; ht...  219729576.0  \n",
      "192505  https://www.ncbi.nlm.nih.gov/pubmed/32288217/;...  122247693.0  \n",
      "192506  https://www.ncbi.nlm.nih.gov/pubmed/32223063/;...  214715941.0  \n",
      "192507  https://www.ncbi.nlm.nih.gov/pubmed/32355117/;...  218468770.0  \n",
      "192508  https://www.sciencedirect.com/science/article/...  218582792.0  \n",
      "\n",
      "[192509 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandrus\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dt = pd.read_csv(\"../data/metadata.csv\")\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the dataset\n",
    "We decided just to work with the papers of the PDF_JSON corpus. Therefore, the first step is to delete from the dataframe the elements that are not in that folder. The number of examples is reduced from 192509 to 79755. Still, there are more documents in the pdf_json than in the dataframe (over 84000) because many pdf in the corpus have the same cord_uid. Technically, the papers mapped into the the same cord_uid are the same one, but with differences in ghe publication (if one article has been published by Elsevier and Springer, it will be mapped twice with the same cord_uid). Our take on the problem will be to consider just on of the documents associated to one cord_uid, instead of the full_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cord_uid                                       sha  \\\n",
      "0      ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb   \n",
      "1      02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d   \n",
      "2      ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927   \n",
      "3      2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605   \n",
      "4      9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32   \n",
      "...         ...                                       ...   \n",
      "79750  z4ro6lmh  203f36475be74229101548475d68352b939f8b5b   \n",
      "79751  hi8k8wvb  9f1bc99798e8823e690697394dcb23533a45c60e   \n",
      "79752  ma3ndg41  ffba777376718ef2a0dd74a8eab90e2bfacd240f   \n",
      "79753  wh10285j  d521c5a2dcbd79a5be606fcf586b1e0448344172   \n",
      "79754  pnl9th2c  c047bf76813106d4fd586e49164e7feddfbe352f   \n",
      "\n",
      "                     source_x  \\\n",
      "0                         PMC   \n",
      "1                         PMC   \n",
      "2                         PMC   \n",
      "3                         PMC   \n",
      "4                         PMC   \n",
      "...                       ...   \n",
      "79750            Medline; PMC   \n",
      "79751  Elsevier; Medline; PMC   \n",
      "79752            Medline; PMC   \n",
      "79753            Medline; PMC   \n",
      "79754  Elsevier; Medline; PMC   \n",
      "\n",
      "                                                   title  \\\n",
      "0      Clinical features of culture-proven Mycoplasma...   \n",
      "1      Nitric oxide: a pro-inflammatory mediator in l...   \n",
      "2        Surfactant protein-D and pulmonary host defense   \n",
      "3                   Role of endothelin-1 in lung disease   \n",
      "4      Gene expression in epithelial cells in respons...   \n",
      "...                                                  ...   \n",
      "79750  Rapid radiological improvement of COVID-19 pne...   \n",
      "79751  SARS E protein in phospholipid bilayers: an an...   \n",
      "79752  Italian Society of Interventional Cardiology (...   \n",
      "79753  Nimble, Together: A Training Program's Respons...   \n",
      "79754  Vascular Life during the COVID-19 Pandemic Rem...   \n",
      "\n",
      "                                doi       pmcid    pubmed_id      license  \\\n",
      "0             10.1186/1471-2334-1-6    PMC35282  1.14726e+07        no-cc   \n",
      "1                      10.1186/rr14    PMC59543   1.1668e+07        no-cc   \n",
      "2                      10.1186/rr19    PMC59549   1.1668e+07        no-cc   \n",
      "3                      10.1186/rr44    PMC59574  1.16869e+07        no-cc   \n",
      "4                      10.1186/rr61    PMC59580  1.16869e+07        no-cc   \n",
      "...                             ...         ...          ...          ...   \n",
      "79750    10.1007/s15010-020-01449-w  PMC7299451     32557206        no-cc   \n",
      "79751   10.1016/j.physb.2004.11.015  PMC7127356     32288217    els-covid   \n",
      "79752             10.1002/ccd.28888  PMC7228289     32223063        no-cc   \n",
      "79753  10.1097/sla.0000000000003994  PMC7224622     32355117  cc-by-nc-nd   \n",
      "79754    10.1016/j.ejvs.2020.04.040  PMC7214295     32446539    els-covid   \n",
      "\n",
      "                                                abstract publish_time  \\\n",
      "0      OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
      "1      Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
      "2      Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
      "3      Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
      "4      Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
      "...                                                  ...          ...   \n",
      "79750                                                NaN   2020-06-15   \n",
      "79751  Abstract We report on an anomalous X-ray refle...   2005-02-28   \n",
      "79752  COVID‐19 pandemic raised the issue to guarante...   2020-04-11   \n",
      "79753                                                NaN   2020-04-29   \n",
      "79754                                                NaN   2020-05-12   \n",
      "\n",
      "                                                 authors  \\\n",
      "0                    Madani, Tariq A; Al-Ghamdi, Aisha A   \n",
      "1      Vliet, Albert van der; Eiserich, Jason P; Cros...   \n",
      "2                                        Crouch, Erika C   \n",
      "3      Fagan, Karen A; McMurtry, Ivan F; Rodman, David M   \n",
      "4      Domachowske, Joseph B; Bonville, Cynthia A; Ro...   \n",
      "...                                                  ...   \n",
      "79750  Comel, Andrea Claudio; Mosaner, Walter; Bragan...   \n",
      "79751  Khattari, Z.; Brotons, G.; Arbely, E.; Arkin, ...   \n",
      "79752  Tarantini, Giuseppe; Fraccaro, Chiara; Chieffo...   \n",
      "79753  Bryan, Darren S.; Benjamin, Andrew J.; Schneid...   \n",
      "79754  Reyes Valdivia, Andrés; Gandarias Zúñiga, Clau...   \n",
      "\n",
      "                           journal  mag_id who_covidence_id arxiv_id  \\\n",
      "0                   BMC Infect Dis     NaN              NaN      NaN   \n",
      "1                       Respir Res     NaN              NaN      NaN   \n",
      "2                       Respir Res     NaN              NaN      NaN   \n",
      "3                       Respir Res     NaN              NaN      NaN   \n",
      "4                       Respir Res     NaN              NaN      NaN   \n",
      "...                            ...     ...              ...      ...   \n",
      "79750                    Infection     NaN              NaN      NaN   \n",
      "79751  Physica B: Condensed Matter     NaN              NaN      NaN   \n",
      "79752   Catheter Cardiovasc Interv     NaN              NaN      NaN   \n",
      "79753                     Ann Surg     NaN              NaN      NaN   \n",
      "79754     Eur J Vasc Endovasc Surg     NaN              NaN      NaN   \n",
      "\n",
      "                                          pdf_json_files  \\\n",
      "0      document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
      "1      document_parses/pdf_json/6b0567729c2143a66d737...   \n",
      "2      document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
      "3      document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
      "4      document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
      "...                                                  ...   \n",
      "79750  document_parses/pdf_json/203f36475be7422910154...   \n",
      "79751  document_parses/pdf_json/9f1bc99798e8823e69069...   \n",
      "79752  document_parses/pdf_json/ffba777376718ef2a0dd7...   \n",
      "79753  document_parses/pdf_json/d521c5a2dcbd79a5be606...   \n",
      "79754  document_parses/pdf_json/c047bf76813106d4fd586...   \n",
      "\n",
      "                                     pmc_json_files  \\\n",
      "0        document_parses/pmc_json/PMC35282.xml.json   \n",
      "1        document_parses/pmc_json/PMC59543.xml.json   \n",
      "2        document_parses/pmc_json/PMC59549.xml.json   \n",
      "3        document_parses/pmc_json/PMC59574.xml.json   \n",
      "4        document_parses/pmc_json/PMC59580.xml.json   \n",
      "...                                             ...   \n",
      "79750  document_parses/pmc_json/PMC7299451.xml.json   \n",
      "79751  document_parses/pmc_json/PMC7127356.xml.json   \n",
      "79752  document_parses/pmc_json/PMC7228289.xml.json   \n",
      "79753  document_parses/pmc_json/PMC7224622.xml.json   \n",
      "79754                                           NaN   \n",
      "\n",
      "                                                     url        s2_id  \n",
      "0      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...          NaN  \n",
      "1      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "2      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "3      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "4      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...          NaN  \n",
      "...                                                  ...          ...  \n",
      "79750  https://doi.org/10.1007/s15010-020-01449-w; ht...  219729576.0  \n",
      "79751  https://www.ncbi.nlm.nih.gov/pubmed/32288217/;...  122247693.0  \n",
      "79752  https://www.ncbi.nlm.nih.gov/pubmed/32223063/;...  214715941.0  \n",
      "79753  https://www.ncbi.nlm.nih.gov/pubmed/32355117/;...  218468770.0  \n",
      "79754  https://www.sciencedirect.com/science/article/...  218582792.0  \n",
      "\n",
      "[79755 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "dt = dt[dt.pdf_json_files.notnull()]\n",
    "dt = dt.reset_index(drop = True)\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we drop the columns that will not add information to our information retrieval system (such as the licenses or the doi) and that do not help to map each example of the dataframe with a document in the pdf_json corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cord_uid                                       sha  \\\n",
      "0      ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb   \n",
      "1      02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d   \n",
      "2      ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927   \n",
      "3      2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605   \n",
      "4      9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32   \n",
      "...         ...                                       ...   \n",
      "79750  z4ro6lmh  203f36475be74229101548475d68352b939f8b5b   \n",
      "79751  hi8k8wvb  9f1bc99798e8823e690697394dcb23533a45c60e   \n",
      "79752  ma3ndg41  ffba777376718ef2a0dd74a8eab90e2bfacd240f   \n",
      "79753  wh10285j  d521c5a2dcbd79a5be606fcf586b1e0448344172   \n",
      "79754  pnl9th2c  c047bf76813106d4fd586e49164e7feddfbe352f   \n",
      "\n",
      "                                                   title  \\\n",
      "0      Clinical features of culture-proven Mycoplasma...   \n",
      "1      Nitric oxide: a pro-inflammatory mediator in l...   \n",
      "2        Surfactant protein-D and pulmonary host defense   \n",
      "3                   Role of endothelin-1 in lung disease   \n",
      "4      Gene expression in epithelial cells in respons...   \n",
      "...                                                  ...   \n",
      "79750  Rapid radiological improvement of COVID-19 pne...   \n",
      "79751  SARS E protein in phospholipid bilayers: an an...   \n",
      "79752  Italian Society of Interventional Cardiology (...   \n",
      "79753  Nimble, Together: A Training Program's Respons...   \n",
      "79754  Vascular Life during the COVID-19 Pandemic Rem...   \n",
      "\n",
      "                                                abstract publish_time  \\\n",
      "0      OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
      "1      Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
      "2      Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
      "3      Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
      "4      Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
      "...                                                  ...          ...   \n",
      "79750                                                NaN   2020-06-15   \n",
      "79751  Abstract We report on an anomalous X-ray refle...   2005-02-28   \n",
      "79752  COVID‐19 pandemic raised the issue to guarante...   2020-04-11   \n",
      "79753                                                NaN   2020-04-29   \n",
      "79754                                                NaN   2020-05-12   \n",
      "\n",
      "                                                 authors  \\\n",
      "0                    Madani, Tariq A; Al-Ghamdi, Aisha A   \n",
      "1      Vliet, Albert van der; Eiserich, Jason P; Cros...   \n",
      "2                                        Crouch, Erika C   \n",
      "3      Fagan, Karen A; McMurtry, Ivan F; Rodman, David M   \n",
      "4      Domachowske, Joseph B; Bonville, Cynthia A; Ro...   \n",
      "...                                                  ...   \n",
      "79750  Comel, Andrea Claudio; Mosaner, Walter; Bragan...   \n",
      "79751  Khattari, Z.; Brotons, G.; Arbely, E.; Arkin, ...   \n",
      "79752  Tarantini, Giuseppe; Fraccaro, Chiara; Chieffo...   \n",
      "79753  Bryan, Darren S.; Benjamin, Andrew J.; Schneid...   \n",
      "79754  Reyes Valdivia, Andrés; Gandarias Zúñiga, Clau...   \n",
      "\n",
      "                           journal  \\\n",
      "0                   BMC Infect Dis   \n",
      "1                       Respir Res   \n",
      "2                       Respir Res   \n",
      "3                       Respir Res   \n",
      "4                       Respir Res   \n",
      "...                            ...   \n",
      "79750                    Infection   \n",
      "79751  Physica B: Condensed Matter   \n",
      "79752   Catheter Cardiovasc Interv   \n",
      "79753                     Ann Surg   \n",
      "79754     Eur J Vasc Endovasc Surg   \n",
      "\n",
      "                                          pdf_json_files  \n",
      "0      document_parses/pdf_json/d1aafb70c066a2068b027...  \n",
      "1      document_parses/pdf_json/6b0567729c2143a66d737...  \n",
      "2      document_parses/pdf_json/06ced00a5fc04215949aa...  \n",
      "3      document_parses/pdf_json/348055649b6b8cf2b9a37...  \n",
      "4      document_parses/pdf_json/5f48792a5fa08bed9f560...  \n",
      "...                                                  ...  \n",
      "79750  document_parses/pdf_json/203f36475be7422910154...  \n",
      "79751  document_parses/pdf_json/9f1bc99798e8823e69069...  \n",
      "79752  document_parses/pdf_json/ffba777376718ef2a0dd7...  \n",
      "79753  document_parses/pdf_json/d521c5a2dcbd79a5be606...  \n",
      "79754  document_parses/pdf_json/c047bf76813106d4fd586...  \n",
      "\n",
      "[79755 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "columns_to_delete = [\"doi\", \"source_x\", \"pmcid\", \"pubmed_id\", \"license\", \"mag_id\", \"who_covidence_id\", \"arxiv_id\", \"pmc_json_files\", \"url\", \"s2_id\"]\n",
    "# dt_original = dt\n",
    "dt = dt.drop(columns_to_delete, axis = 1)\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79755\n"
     ]
    }
   ],
   "source": [
    "print(dt.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cord_uid                                                   ug7v899j\n",
      "sha                        d1aafb70c066a2068b02786f8929fd9c900897fb\n",
      "title             Clinical features of culture-proven Mycoplasma...\n",
      "abstract          OBJECTIVE: This retrospective chart review des...\n",
      "publish_time                                             2001-07-04\n",
      "authors                         Madani, Tariq A; Al-Ghamdi, Aisha A\n",
      "journal                                              BMC Infect Dis\n",
      "pdf_json_files    document_parses/pdf_json/d1aafb70c066a2068b027...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Document 1 \n",
    "print(dt.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia\n",
      "OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.\n"
     ]
    }
   ],
   "source": [
    "# Document 1\n",
    "print(dt.iloc[0].title)\n",
    "print(dt.iloc[0].abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the pdf\n",
    "The articles mapped with each row of the metadata (cord_uid) are stored separately and parsed in .json. The following script converts an article mapped with each cord_uid into a python dictionary, storing the set of dictionaries on pdf_json_list.\n",
    "\n",
    "##### Es probable que esto sea opcional, porque los metadatos ya nos dan el título, autores y abstract (las tres cosas más importantes). Trabajar usando todo el cuerpo de los artículos puede mejorar los resultados, pero el tiempo de cómputo aumentaría mucho. Además, la información sobre el título, autores y abstract es mejor en los metadatos, dado que los metadatos son proporcionados directamente de las revistas y, en contraste, la info. sobre titulo, autores y abstract del pdf_json corpus son obtenidos automáticamente mediante un parseo de pdf a json y puede haber errores.\n",
    "##### Poca broma, a mi se me desborda la memoria. Mi RAM la tenía al 94% de ocupación con el siguiente script (obviamente no pude terminarlo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npdf_json_list = []\\nt0 = time.time()\\nfor row in dt.index :\\n    json_path = (dt.loc[row][\\'pdf_json_files\\'].split(\\'; \\'))[0]\\n    json_file = open(\"../data/\"+json_path) \\n    full_text_dict = json.load(json_file)\\n    pdf_json_list.append(full_text_dict)\\nt1 = time.time()\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pdf_json_list = []\n",
    "t0 = time.time()\n",
    "for row in dt.index :\n",
    "    json_path = (dt.loc[row]['pdf_json_files'].split('; '))[0]\n",
    "    json_file = open(\"../data/\"+json_path) \n",
    "    full_text_dict = json.load(json_file)\n",
    "    pdf_json_list.append(full_text_dict)\n",
    "t1 = time.time()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to read the test queries (https://towardsdatascience.com/download-and-parse-trec-covid-data-8f9840686c37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['query'])\n"
     ]
    }
   ],
   "source": [
    "topics = {}\n",
    "root = ET.parse(\"../queries/test_queries.xml\").getroot()\n",
    "for topic in root.findall(\"topic\"):\n",
    "    topic_number = int(topic.attrib[\"number\"])\n",
    "    topics[topic_number] = {}\n",
    "    for query in topic.findall(\"query\"):\n",
    "        topics[topic_number][\"query\"] = query.text  # We only need the query: \n",
    "    #for question in topic.findall(\"question\"):\n",
    "     #   topics[topic_number][\"question\"] = question.text        \n",
    "    #for narrative in topic.findall(\"narrative\"):\n",
    "     #   topics[topic_number][\"narrative\"] = narrative.text\n",
    "print(topics[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'query': 'coronavirus origin'}, 2: {'query': 'coronavirus response to weather changes'}, 3: {'query': 'coronavirus immunity'}, 4: {'query': 'how do people die from the coronavirus'}, 5: {'query': 'animal models of COVID-19'}, 6: {'query': 'coronavirus test rapid testing'}, 7: {'query': 'serological tests for coronavirus'}, 8: {'query': 'coronavirus under reporting'}, 9: {'query': 'coronavirus in Canada'}, 10: {'query': 'coronavirus social distancing impact'}, 11: {'query': 'coronavirus hospital rationing'}, 12: {'query': 'coronavirus quarantine'}, 13: {'query': 'how does coronavirus spread'}, 14: {'query': 'coronavirus super spreaders'}, 15: {'query': 'coronavirus outside body'}, 16: {'query': 'how long does coronavirus survive on surfaces'}, 17: {'query': 'coronavirus clinical trials'}, 18: {'query': 'masks prevent coronavirus'}, 19: {'query': 'what alcohol sanitizer kills coronavirus'}, 20: {'query': 'coronavirus and ACE inhibitors'}, 21: {'query': 'coronavirus mortality'}, 22: {'query': 'coronavirus heart impacts'}, 23: {'query': 'coronavirus hypertension'}, 24: {'query': 'coronavirus diabetes'}, 25: {'query': 'coronavirus biomarkers'}, 26: {'query': 'coronavirus early symptoms'}, 27: {'query': 'coronavirus asymptomatic'}, 28: {'query': 'coronavirus hydroxychloroquine'}, 29: {'query': 'coronavirus drug repurposing'}, 30: {'query': 'coronavirus remdesivir'}, 31: {'query': 'difference between coronavirus and flu'}, 32: {'query': 'coronavirus subtypes'}, 33: {'query': 'coronavirus vaccine candidates'}, 34: {'query': 'coronavirus recovery'}, 35: {'query': 'coronavirus public datasets'}, 36: {'query': 'SARS-CoV-2 spike structure'}, 37: {'query': 'SARS-CoV-2 phylogenetic analysis'}, 38: {'query': 'COVID inflammatory response'}, 39: {'query': 'COVID-19 cytokine storm'}, 40: {'query': 'coronavirus mutations'}, 41: {'query': 'COVID-19 in African-Americans'}, 42: {'query': 'Vitamin D and COVID-19'}, 43: {'query': 'violence during pandemic'}, 44: {'query': 'impact of masks on coronavirus transmission'}, 45: {'query': 'coronavirus mental health impact'}, 46: {'query': 'dexamethasone coronavirus'}, 47: {'query': 'COVID-19 outcomes in children'}, 48: {'query': 'school reopening coronavirus'}, 49: {'query': 'post-infection COVID-19 immunity'}, 50: {'query': 'mRNA vaccine coronavirus'}}\n"
     ]
    }
   ],
   "source": [
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 coronavirus origin\n",
      "2 coronavirus response to weather changes\n",
      "3 coronavirus immunity\n",
      "4 how do people die from the coronavirus\n",
      "5 animal models of COVID-19\n",
      "6 coronavirus test rapid testing\n",
      "7 serological tests for coronavirus\n",
      "8 coronavirus under reporting\n",
      "9 coronavirus in Canada\n",
      "10 coronavirus social distancing impact\n",
      "11 coronavirus hospital rationing\n",
      "12 coronavirus quarantine\n",
      "13 how does coronavirus spread\n",
      "14 coronavirus super spreaders\n",
      "15 coronavirus outside body\n",
      "16 how long does coronavirus survive on surfaces\n",
      "17 coronavirus clinical trials\n",
      "18 masks prevent coronavirus\n",
      "19 what alcohol sanitizer kills coronavirus\n",
      "20 coronavirus and ACE inhibitors\n",
      "21 coronavirus mortality\n",
      "22 coronavirus heart impacts\n",
      "23 coronavirus hypertension\n",
      "24 coronavirus diabetes\n",
      "25 coronavirus biomarkers\n",
      "26 coronavirus early symptoms\n",
      "27 coronavirus asymptomatic\n",
      "28 coronavirus hydroxychloroquine\n",
      "29 coronavirus drug repurposing\n",
      "30 coronavirus remdesivir\n",
      "31 difference between coronavirus and flu\n",
      "32 coronavirus subtypes\n",
      "33 coronavirus vaccine candidates\n",
      "34 coronavirus recovery\n",
      "35 coronavirus public datasets\n",
      "36 SARS-CoV-2 spike structure\n",
      "37 SARS-CoV-2 phylogenetic analysis\n",
      "38 COVID inflammatory response\n",
      "39 COVID-19 cytokine storm\n",
      "40 coronavirus mutations\n",
      "41 COVID-19 in African-Americans\n",
      "42 Vitamin D and COVID-19\n",
      "43 violence during pandemic\n",
      "44 impact of masks on coronavirus transmission\n",
      "45 coronavirus mental health impact\n",
      "46 dexamethasone coronavirus\n",
      "47 COVID-19 outcomes in children\n",
      "48 school reopening coronavirus\n",
      "49 post-infection COVID-19 immunity\n",
      "50 mRNA vaccine coronavirus\n"
     ]
    }
   ],
   "source": [
    "for key in topics: \n",
    "    value = topics[key]\n",
    "    print(key, value[\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to read the relevance judgement (the information needed to evaluate our system). The round id is not needed and it is therefore omitted. Also, the relevance is binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       topic_id  cord_uid relevancy\n",
      "0             1  005b2j4b         1\n",
      "1             1  00fmeepz         1\n",
      "2             1  010vptx3         1\n",
      "3             1  0194oljo         1\n",
      "4             1  021q9884         1\n",
      "...         ...       ...       ...\n",
      "69313        50  zvop8bxh         1\n",
      "69314        50  zwf26o63         1\n",
      "69315        50  zwsvlnwe         0\n",
      "69316        50  zxr01yln         1\n",
      "69317        50  zz8wvos9         1\n",
      "\n",
      "[69318 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "relevance_data = pd.read_csv(\"../queries/relevance_judgements.txt\", sep=\" \", header=None)\n",
    "relevance_data.columns = [\"topic_id\", \"round_id\", \"cord_uid\", \"relevancy\"]\n",
    "relevance_data = relevance_data.drop(\"round_id\" ,axis = 1)\n",
    "relevance_data['relevancy'] = relevance_data['relevancy'].replace([2],'1')\n",
    "print(relevance_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the metadata (and optionally json_pdf), test topics and relevance judgement we are prepared to build and validate the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A simple VSM implementation\n",
    "We have adapted the simple vector space model implementation for our code. \n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install and import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first install the NLTK toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: click in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to download the NLTK data bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now install the gensim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from gensim) (4.0.1)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the required software is now installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now import the functions provided by NLTK to perform tokenizing considering punctuation signs.\n",
    "from nltk.tokenize import wordpunct_tokenize, regexp_tokenize\n",
    "# Next, we import required functions to filter-out stopwords for the English language.\n",
    "from nltk.corpus import stopwords\n",
    "# Now we import the function that implements the Porter's stemming algorithm.\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is aimed at preprocessing each document in the collection. We write a function that receives the variable dt, and returns a list containing all STEMS in the collection whose associated token is longer than 2 characters and is NOT an (English) stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(doc): # Each doc is each df row. We will only use title and abstract: dt.iloc[i].title and dt.iloc[i].abstract\n",
    "    #print(i)\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    if type(doc.title) != str and type(doc.abstract) != str: # For empty documents without title and abstract\n",
    "        final = [\"\"]\n",
    "    else:\n",
    "        if type(doc.title) == str: \n",
    "            tokens = wordpunct_tokenize(doc.title)\n",
    "        if type(doc.abstract) == str:\n",
    "            tokens.extend(wordpunct_tokenize(doc.abstract))\n",
    "            # clean guarda las palabras (en minuscula) que no están incluidas en stopset\n",
    "        clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2 and \"%\" not in token]\n",
    "        final = [stemmer.stem(word) for word in clean]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clinic', 'featur', 'cultur', 'proven', 'mycoplasma', 'pneumonia', 'infect', 'king', 'abdulaziz', 'univers', 'hospit', 'jeddah', 'saudi', 'arabia', 'object', 'retrospect', 'chart', 'review', 'describ', 'epidemiolog', 'clinic', 'featur', 'patient', 'cultur', 'proven', 'mycoplasma', 'pneumonia', 'infect', 'king', 'abdulaziz', 'univers', 'hospit', 'jeddah', 'saudi', 'arabia', 'method', 'patient', 'posit', 'pneumonia', 'cultur', 'respiratori', 'specimen', 'januari', '1997', 'decemb', '1998', 'identifi', 'microbiolog', 'record', 'chart', 'patient', 'review', 'result', 'patient', 'identifi', 'requir', 'admiss', 'infect', 'commun', 'acquir', 'infect', 'affect', 'age', 'group', 'common', 'infant', 'pre', 'school', 'children', 'occur', 'year', 'round', 'common', 'fall', 'spring', 'three', 'quarter', 'patient', 'comorbid', 'twenti', 'four', 'isol', 'associ', 'pneumonia', 'upper', 'respiratori', 'tract', 'infect', 'bronchiol', 'cough', 'fever', 'malais', 'common', 'symptom', 'crepit', 'wheez', 'common', 'sign', 'patient', 'pneumonia', 'crepit', 'bronchial', 'breath', 'immunocompromis', 'patient', 'like', 'non', 'immunocompromis', 'patient', 'present', 'pneumonia', 'versu', 'patient', 'pneumonia', 'unev', 'recoveri', 'recov', 'follow', 'complic', 'die', 'pneumonia', 'infect', 'die', 'due', 'underli', 'comorbid', 'patient', 'die', 'pneumonia', 'pneumonia', 'comorbid', 'conclus', 'result', 'similar', 'publish', 'data', 'except', 'find', 'infect', 'common', 'infant', 'preschool', 'children', 'mortal', 'rate', 'pneumonia', 'patient', 'comorbid', 'high']\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_document(dt.iloc[0])) # Print STEMS for the document 1\n",
    "print(type(dt.iloc[0].title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cord_uid                                                   n06og3cw\n",
      "sha                        8d35867e078939b7f20187322e41011cec8b8cb3\n",
      "title                                                           NaN\n",
      "abstract                                                        NaN\n",
      "publish_time                                             2020-05-13\n",
      "authors           De Coninck, David; d'Haenens, Leen; Matthijs, ...\n",
      "journal                                               Public Health\n",
      "pdf_json_files    document_parses/pdf_json/8d35867e078939b7f2018...\n",
      "Name: 29264, dtype: object\n",
      "nan\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(dt.iloc[29264])\n",
    "print(dt.iloc[29264].title)\n",
    "print(type(dt.iloc[29264].title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_document(dt.iloc[29264]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(q):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = wordpunct_tokenize(q)\n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    "    final = [stemmer.stem(word) for word in clean]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all documents in the collection have been preprocessed, we need to create a dictionary containing the mappings WORD_ID -> WORD. This dictionary is required to create the vector-based word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Different words in the collection\n",
    "def create_dictionary(docs):\n",
    "    #print(preprocess_document(docs.iloc[79754]))\n",
    "    # List all pre-processing documents\n",
    "    pdocs = [preprocess_document(docs.iloc[i]) for i in range(docs.shape[0])]\n",
    "    #print(\"pdocs: \", pdocs)\n",
    "    # Build the dictionary\n",
    "    dictionary = corpora.Dictionary(pdocs)\n",
    "    # Save in a file\n",
    "    dictionary.save('simple_vsm.dict')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us call the create_dictionary function feeding it with the complete dt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(123368 unique tokens: ['1997', '1998', 'abdulaziz', 'acquir', 'admiss']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = create_dictionary(dt)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have built the dictionary containing the vocabulary that we will use for indexing. Now we write a function that create the bag of words-based representation for each document in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2bows(allData, dictionary):\n",
    "    docs = [preprocess_document(allData.iloc[i]) for i in range(allData.shape[0])]\n",
    "    # We obtain the set of frequencies for each term\n",
    "    vectors = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    corpora.MmCorpus.serialize('simple_vsm_docs.mm', vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now generate the BOWs for the complete dt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-f306250a220e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocs2bows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-708310655e01>\u001b[0m in \u001b[0;36mdocs2bows\u001b[1;34m(allData, dictionary)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdocs2bows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# We obtain the set of frequencies for each term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'simple_vsm_docs.mm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-708310655e01>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdocs2bows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# We obtain the set of frequencies for each term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'simple_vsm_docs.mm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-4287d362c0b2>\u001b[0m in \u001b[0;36mpreprocess_document\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Each doc is each df row. We will only use title and abstract: dt.iloc[i].title and dt.iloc[i].abstract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mstopset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstract\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# For empty documents without title and abstract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     21\u001b[0m         return [\n\u001b[0;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         ]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bows = docs2bows(dt, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are pairs (word identifier, frequency). Let us now convert them into something a bit more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in bows:\n",
    "    tvec = [(dictionary[id], freq) for (id, freq) in v]\n",
    "    print(tvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are basically TF-weighted vectors. We now want to convert these vectors into their TF-IDF weighted counterparts. We need, however, to import the models module from Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "def create_TF_IDF_model(allData):\n",
    "    dictionary = create_dictionary(allData)\n",
    "    docs2bows(allData, dictionary)\n",
    "    loaded_allData = corpora.MmCorpus('simple_vsm_docs.mm')\n",
    "    tfidf = models.TfidfModel(loaded_allData)\n",
    "    print(\"tfidf\", tfidf)\n",
    "    return tfidf, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create the TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfm = create_TF_IDF_model(dt)\n",
    "print(tfidfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, a complex object is returned that contains the TF-IDF model and the associated dictionary. Let us now take a closer look of such a TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidfm[0].__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally create a function that given the dt and the topics provides a document ranking sorted in descending order of relevance (according to the cosine measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from gensim import similarities\n",
    " \n",
    "    \n",
    "def launch_query(allData, q, number, f=\"null\", filename='simple_vsm_docs.mm'):\n",
    "    tfidf, dictionary = create_TF_IDF_model(allData)\n",
    "    loaded_allData = corpora.MmCorpus(filename)\n",
    "    index = similarities.MatrixSimilarity(loaded_allData, num_features=len(dictionary))\n",
    "    pq = preprocess_query(q)\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = tfidf[vq]\n",
    "    sim = index[qtfidf]\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    print(\"QUERY:\",q)\n",
    "    for i in range(0,10) :\n",
    "        print(\"[ Score = \"+str(ranking[i][1])+\" ] \"+allData.iloc[ranking[i][0]].title)\n",
    "        if f!=\"null\":\n",
    "            f.write(str(number)+\" Q0 \"+str(allData.iloc[ranking[i][0]].cord_uid)+\" \"+str(i+1)+\" \"+str(ranking[i][1])+\" mySystem \\n\")\n",
    "    \"\"\"\n",
    "    pos = 1\n",
    "    for doc, score in ranking:\n",
    "        if ( pos <=10 ): # First ten positions\n",
    "            print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + allData.iloc[doc].title); \n",
    "            f.write(str(number)+\" Q0 \"+str(allData.iloc[doc].cord_uid)+\" \"+str(pos)+\" \"+str(round(score,3))+\" mySystem \\n\")\n",
    "        else: \n",
    "            f.write(str(number)+\" Q0 \"+str(allData.iloc[doc].cord_uid)+\" \"+str(pos)+\" \"+str(round(score,3))+\" mySystem \\n\")\n",
    "            #break\n",
    "        pos += 1\"\"\"\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can launch any query we see fit to our newly created Information Retrieval engine.\n",
    "\n",
    "We choose the query 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key in topics: \n",
    "    #value = topics[key]\n",
    "    #launch_query(dt, value[\"query\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vydgwa8o', 'wfh9pzb2', '7utfozx5', '1e28zj1d', 'tvbnv5gz', 'o9oxchq6', 'tb3yyq44', 'etmhbrfe', 'gjdlp10q', 'cwfujgya', 'erozilox', 'qopcs6jy', 'gy8d8285', 'xlqzprqn', 'wt9j5mvd', 'b6dsdiux', 'tj4pn38j', 'ljcr0xtm', 'sxbmd0df', 'w53u5ive', 'rgmurgbf', '4n6v5kfv', 'kjx0z1hc', 'ab757i3f', 't8m9i4vv', '7odpslba', '5l10cbp2', 'e0b8gnh8', 'ljqrxjvv', 'eju7wnb9', 'bfjdvfuq', '52lcpf0x', 'nw4wap1d', '1s6dlcer', 'oewvpv66', '1585stal', '85acs4lk', '6yfj4co0', 'aki28lhp', '5p8gkbi7', '7x3nq9cp', 'kx5hihnr', 'cdthfl5f', 'shvsnkxd', 'fby616ap', 'yvh2fzxt', 'jsyao6qu', '65b267ic', 'flc25wlz', 'o349msnm', 'bzldjzp3', 'z7a3g6e8', '2uwnamao', '98syj71y', 'm2c5bvuj', 'utn21ce1', '2kbi9drl', 'esq33kx4', 'jntrjg8u', 'jbtrdvhe', 't0cw7l2a', '8a1cia8s', 'zi0lc3lp', 'b2znv6pa', 'gytg4iku', 'jjraqr85', 'z0ni2jsr', 'ao7bkcv5', '08efpohc', '8pkrg0mx', 'l5q5wc06', '4ilpph77', 'mgdfwbfm', '1ag9jkk6', 'moz22sur', '59492sjb', 'yeih3tfo', 'chbegdex', '1sq2uvur', 's0893qap', 'ibn71fcd', 'jqgyln7b', '1ldynibm', 'gzcbwys1', 'gsaw1snc', '9sofxhj7', 'rd0bwuqc', 'kzk4i2j2', 'uo6h1h2e', 'lzlqpbzz', '0qaoam29', 'lm9urlat', 'w8v0mwph', '262bcl7h', 'ndpuezpo', '405jvqyv', '6xcctafe', '9mxvbf17', '3g17e7sg', 'p60xy2ki', 'p0ovi8bl', 'sqrn6kjy', '85cdy3f9', 'ayhsbfd1', 'k9lcpjyo', 'dbowa5bt', '6jakyj24', 'dir0jtyb', 'zz24hrva', 'igdkz3ht', 'e1ykafdd', 'x6s34z1s', 'rko3e5va', 'ienet82k', 'zxvim4t8', 'x98j7mii', 'zq2h0avw', 'eecwjrdc', 'b8ofli35', 'sxvgue36', 'cndb031c', '66xk0qqq', '9slpoyz7', 'r0gr0bhl', 'vj64ykwf', 'sdiwnhs0', 'we4ptygr', 'dkreswvk', 'tyhtdawb', 'z2kgcjtp', '1nrkfkv5', 'etqj2mq3', 'niw61l9r', '2te9myos', 'dc30gkfe', '3we40x62', 'vlpdirjs', 'ksva9eah', 'mowlquh4', '1p7fz9zr', 'xtqvtleb', '88yf7igp', 'gt7meidh', 'k4l45ene', 'bzc7luwj', 'ngietu0n', 'ua6aeg5a', 't6zf5k99', 'kiq6xb6k', '5een8p3r', '2f8c66zi', 'l2n0xvmd', 'fw4pmaoc', 'k5y1qc82', 'qs7dsxn5', 'u7tz6xzz', 'jh9e85c0', 'msvex3ae', '57x3j1np', 'rmqu2ngg', 'iua8c4hy', 'c3ikb81g', 'tfkhptdg', '1mjaycee', 'aoyyk5fl', 'zb434ve3', 'lavcsqov', 'e2p46wa8', 'r8el8nqm', '6rpt47gm', '13ir7swr', 'u7zxlgxz', 's4ef9qob', '4nmpwmzz', 'd8n9711b', 'efk2jj50', 'q0ngoa61', 'fzr8fkzb', 'tojgojjf', 'r6wwpfcy', '06o2tbon', '9rlgx2s5', '5i2dxg8z', 'od97az43', 'hicqy5sj', 'otal8v81', 'y3uzb4dx', 't8q99tlq', 'ldxo05z4', 'mo5myrz4', 'pu9l36j9', 'n2o7iiew', 'vrqe7kxm', '1pnc889f', 'te1bf690', '021q9884', '79tozwzq', 'b21szgvy', 'b9r4cyp7', 'pidy7erw', 'gs4r9j7o', 'dle938bt', 'dopa8hxy', 'nqca8mio', 'of9wlhga', 'mjh422vm', '2upbpr8b', 'mzf7j8xd', 'm6akijzn', 'thg4fqi2', 'yh1d2g75', '8ejc5543', 'hncf2qe8', 'y708ej9l', '8xsuntfd', '3uif1ve9', 'o4d7038f', '0e1w86tg', '3xq5jwof', 'bgdcm1i1', 'job4854r', 'dmc8wjyl', 'mdry8qzv', 'l0nrgl9o', 'gmi1ewc2', 'wuegn0jg', 'iudq5jdu', 'efhnptt9', '4ywt0yqn', 'yw7w65xw', 'ghi0ekm5', '0194oljo', 'fcmzdcuh', 'eec649x4', 'jxbk30gh', 'n4qh8vg4', 'u7zze0dk', 'wlmu65vm', 'pfv7q4v6', 'f6muhf3d', '8bw843ek', 'niytf3wo', 'cobv635z', 'ter67nri', 'iohvj16d', 'j56v40u3', 'fihedvms', 'rjthvtul', '96ee2c1s', '9rhydy8u', 'qg2hoxfw', 'cns5rnj8', 'kjkfsy7w', 'wesrv44a', 'aaq4ks3n', '8gncbgot', 'zjr6csla', 'v6dfel3l', '7463vcy9', 'tswojeuv', '61snvddh', 'kkga96h9', '1hwcp09t', 'e6jt8yhs', 'qp4efhwq', 'vembiw2k', 'u8zlc3n2', '368pz09b', 'yg4rs07n', 'vvui9das', 'n8ji4714', 'mkwpuav6', 'bojfc3q0', 'mqduxqdl', 'nw7ymn3c', '4zg3ms5b', 'a4jn6gpk', 'mrvk9r4w', 'xgsnvndf', 'zcy4hpqp', 'apc0lm5e', '7og47tqv', '7grrlb8l', 'hzuo6pwx', 'g7qioapl', '2hb28brw', 'unvabosp', '75773gwg', '0v5wo0ty', 'hj830rxs', 'k05cdqlw', 'vnnnevrl', 'tj4deqfw', 'z73bozz7', 'yc5an9rd', 'xxblr8qd', '3bozjfv7', 'r0peje13', 'aejbfk3l', 'jtsxpjhc', '5f2e6063', 'plxz5mkg', 'nvofyg16', '7zme0kxy', 'olmein3q', 'rdh4xi2u', 'yf5g53a9', 'nspl014y', 'wh3btcd4', 'gy78e87y', 'g0gh28hu', 'hfkzu18p', 'nk9hhco3', 'sjyrr2bn', '41rsi8ce', '8x0duuh6', '9jdfp9y3', '3wuh6k6g', '4ze0mfxp', 'h8ahn8fw', 'icwvm7jp', 'mwj2qby4', 'k9yus2sv', 'ew2kjq5s', 'auu02ibd', '629fwmgk', 'hl967ekh', 'ulygq434', 'e7hefkd3', 'uywz5nqw', '8iyjzd28', 'kjovtgua', '480d3yfv', '9uo0hsst', '4b6jtbor', '9rg9xe57', 'h3ovjqcn', 'gvq24eyp', 'f3ds1rq6', 'abqrh2aw', 'hlw3t2rb', 'vh72tfri', 'rx68247n', '9igk3ke1', '60xeqgrf', 'fvig79k3', 'uqahztur', 'aeogp8c7', 'chln5r8w', 'fma2ejqd', 'b23xm6sg', 'n2abv3ub', '25va2cvt', 'uj96zdld', '1ub01j2x', 'n7ylgqfu', 'bqoob23i', 'dnxhtbxn', 'hwjr891o', '9spfx07v', 'truei5y2', 'a66sszp2', '8gkh7se3', 'jujrazsr', 'jty4ebdf', 't34myl7l', 'dnla56uh', 'neqndgpc', 'vvhhpchm', '971d0sir', 'n5fobgxb', 'sf1rgw61', 'mtngelbr', 'wbd0s4fo', 'b9srpqjp', 'ris4ijq9', '69fmtury', 'wvx6q999', 'c7j2eoz0', '5op2m7pu', '8klkojpo', '36g4zdqa', 'wtvjjc7p', '2kkw9nwa', 'pl48ev5o', '6bv54unc', '8wzd4j82', '475nei28', '0lyxvex0', '4ulei2uh', 'cjfm44ef', 'zd7smm8r', 'b4gfbenh', '9fngwsh3', '1eytelxn', 'su8czd4x', 'nv4l2ctd', 'kqsg9wol', 'ne5r4d4b', 'ltar1aa5', '50oy9qqy', 'z9dolxky', 'wm0nk0k3', 'rpeqkfxc', '5h7qyn1g', 'bw9lbzvt', 'ur4eua83', 'liuo74xn', '82x7gq5z', 'luxgc7st', 'nvzfpntu', 'ugc7ce21', 'b5h5cv3b', 'aw0kkvvl', 't3t20wji', '5p29uhak', '2054zqw9', 'ol2exczc', '49360l2a', 'jwmrgy5d', '9gi2yoqn', 'dei0xd1b', 'rjltw0qn', 't2msfh6b', 'dgbf77hn', '8a6y1qsf', '296ilxyg', 'ezi2mret', '0hnh4n9e', '8ywd6j2b', 'bim76jna', 'zqf351sv', 'm6eal7lu', 'jmwn8pdn', '2uqf6qed', '3g8pwbcq', 'egzztatj', 'nxfr24sp', 'v99vlnox', 'mo4luyx6', 'my16g91c', 'q8im1agz', 'b5287qd4', '9q5zckir', 'f6i10v6g', 'eav5gr3y', 'n8fwy791', 'u13oge9m', 'ailr08y5', '5yk1j4ms', 'gcwrill0', '127c5bve', '9dlr6kqh', 'u3hz6sz1', 'neba2o7n', '1s2zsqoq', 'u1k48t29', 'p7ifetgw', 'kv1od8i4', 'stnjl8f8', 'pn9nwl64', '047xpt2c', 'd80ispem', '6q90bayx', '05vx82oo', '3e2soc6w', '3saa7d76', 'uusi3nph', 'x3sb1o4u', '105q161g', 'lk67yfrp', '3sacf4wu', '3h1o0oz3', 'xpbcoipf', '6q8afty7', 'xqqn1t4e', 'xuj4yymz', 'sr0umk33', 'vcfljekt', 'x4cxp6be', '3r8jbhhq', '0t2a5500', 'k4l759k2', 'bqodr0h0', 'ffmxvb23', '9w3ap3gz', 'e328fesl', 'btiy6jhr', 't1iagum7', '4j6cbnk2', 'fe0w2spw', 'p6a3j6mr', 'a56u5e2o', '0xhho1sh', 'xtibubra', '6y1gwszn', 'arhqix9h', 'nip1ax1x', 'm4y8tf6u', 'es7q6c90', 'vp1r00m9', 'qyf59ghf', 'uj0i2anr', 'nro623aa', '3uvuo4sf', '5217c3jn', 'd3rrnjz2', 'r55fe25x', 'zav5gksq', 'apqizr54', '5opiip58', '6r08hj06', 'yk60ol95', 's21obnqy', '5fg87lvu', 'xwi9pdd2', 'kx7ybq42', 'suhqgmlo', 'bawgldfi', 'qed8hayx', 'ds5mauer', 'cs8gfjp4', '1mchg5yg', 'u6ewlh16', 'hgg33kwz', '00fmeepz', 'kn2z7lho', 'lbcxl6w9', '915srotp', '2mwahvq0', 'ilxd0ih9', '6dq8xx7c', 'ioq50ntq', 'zp4uy1v7', 'bgh729s5', '0ne21in2', 'sof70xa1', '7hw23xae', '12dcftwt', 'pyeb86on', 'v9ftmdlv', 'kbux2l4o', '8d3dym7g', 'ba6v30jg', 'xl8gfkjs', '1bpc8g6n', 'x7rqfsgs', 'rtw8m00s', 'svu06v9c', 'z443e8lk', '75dwpw6l', 'qpwvn2fi', 'n9l86dln', 'quee5dy5', 'y2zcwcic', '9xueqdri', 'i758v1vb', 'mb1y5ort', 'is20odaq', 'sfm7y2lo', 'wuo5g9x1', '1rhy8td0', '2w0zr9c0', 'cmaw7jcn', 'qw05apnf', '7e8zlt3t', 'fs5rx07q', 'b2d7t7ge', 'j9csyc3j', '4jag7lzb', 'e2zyplbi', 'gdxvbspx', '2t3evpxf', 'xvnv17zy', 't22il8to', 'l2wzr3w1', 'thpl8giu', 'zpiaka80', '44zduv27', 'ccxj4s6j', 'kgifmjvb', 'uqls3p01', '3goj00ej', '95fc828i', '3lqqk9da', 'qpcvxwti', '0oiq44gl', 'n4v4n8d1', 'ri7v2ka3', 'kqqantwg', 'vexo81k5', '0iq9s94n', 'fkqohnyz', 'dyhd8p8z', '97vub46z', 'hkwu5o6d', 'dtv7to3l', 'f35cqkpf', 'mofqbemr', 'ycrrsr5c', 'feq8djas', 'oufi7w1d', 'fybo3ltn', 'jg607kt6', '9f5i6crg', 't7rxmzvi', '9dwpnvxf', '3dm9duoz', '0xruezf2', '68g8noys', 'yh1lioyz', 'fvj3cpie', 'vy1mu04l', 'm9fikesi', 'jb6o4v6p', 'phlxsez8', 'dlpgq0rm', 'rbw1vam9', 'skkng2le', 'm5u3cnvh', 'ck301hcu', '084o1dmp', 'pjeddlgs', 'jep80bed', '5ekdfers', '2arx86a6', 'yoy3hj3j', 'o877uul1', 'eisfz30c', 'g4bsul8u', 'z4l3pk23', 'lf941vhd', 'rmia5w3x', 'tvv73r7h', '3en3yey1', 'dx3jywv7', 'jqexwnq2', '3fiz0tqy', 'ycvu5l5f', 'l2zvxwsp', 'h7so7vcl', 'j2ybesoy', 't8j4q00m', 'uwj62cuv', 'kyrkx2ii', 'bayj4dtg', 'pgtvx6wb', 'iweu0pm8', 'xy1khp7w', '0qpfoh5t', '1spgrrmw', 'mmy45mhm', 'xxcx7hg5', 'e220ut9u', 'v5brk5us', '4v5lo6dd', 'yxjhfg7m', 'ys51rshu', 'ptizke03', 'rcljauve', 'xa6kwguo', 'e56ry1x4', 't20huil6', 'bea9x5j1', 'gnk3m0b8', 'llv3cvdr', '6zfmjq9p', 'on70zzn0', 'wr7vrild', '6wbrv9o1', '8ow952d8', 'y5e2gs15', '4txctk7k', 'ja2s5cxy', '3pqtmhob', 'e8qubwha', 'rehr84c7', 'eahhf7yh', 'rr7r7ef0', 'u4mvk89w', 'uc7ubqbj', '9mrtic2k', '4almssg6', 'pka6ipav', 'deajwhx0', 'x64cwmqf', 'lslh9yg3', 'irkjiqll', '2ip9nqwv', 'q3yx3x1l', 'nm2ubi13', 'gu2mt6zp', 'pl2d1yc0', 'tuas2tgd', '13jupb26', 'vnafx1ng', 'lxmhs0ow', 'e46whqk9', 'yzp9wjuk', '51nsm10x', 'pwtiobv4', 'vww8ypyz', 'fozglfc8', '3uyuwzyr', 'snp8o9fw', 'z2g1mx1p', '4p1ah2h3', '8kqhvxlz', 'fctcwpyc', 'lw12h047', 'bupb4ooh', 'n2t35wgn', '6ez0u7iq', 'ew2paw3x', 'rxrlbw60', '4fb291hq', 'tfspedf1', 'zel9a3u6', '8pwe4ugb', '8dhoxvvs', '3114rz8f', 'vmtb5swj', '99doz9m6', 'fgxbyb7l', '22ioujwl', '950x4b9a', 'nzwe7hpg', '3zmq7nd5', 'k2sz4tn2', 'lasv4e6a', 's7uop4h8', 'rtybezro', 'uexahhdr', '08ds967z', '2dw318eg', 'bnuda70x', 'st5idleq', 's0wmll4q', 'wnvtkiew', 'xbpb9nfi', 'afek8kgr', 'mf75uwuv', 'xutbbxpr', 'z6fd4yua', '1qkwsh6a', 'ax2b0ynt', 'sphk023v', '6foz003n', 'r5cgvo7m', 'c4u0gxp5', 'lbv69du4', 'sygnmiun', 'c6yrtwq9', 'fofy6whl', '8z0ti0dx', 'wn9l4wtc', '4cy3er3y', 'k0mhfs1h', 'kc1xl0sg', 't7gpi2vo', 'l5ogbl5p', 'uvavdk6n', '3y4ulpkh', 'mrg2pomd', 'lfndq85x', 'ejg393zs', 'ec8lpgl3', 'pyfa7vye', 'o15bf7eh', 'walpfa6r', 'zsm7v3am', 'r9scxa76', 'b66bb2ri', 'bmwgq7py', 'jc00ulx5', 'rflu6axg', 'qkask7md', 'n8e2n30b', '6v7oru2l', 'q97pgwqf', 'zbjqjese', 'szg12wfa', 'a3groawt', 'jgwvjkbj', 'v6yrr39c', 'dz1ikex1', 'zu46bdpu', 'rte05edo', 'uph136sn', 'xibub2in', 'el1vyyj9', 'hk5w5h58', 'ewdrl9ym', 'yon280y8', 'sy8sloqo', 'r2ivqg5j', 'vq3axp9l', 'eeqzmm8k', 'egvf8hoq', '6dbt99h0', 'd2z556s0', '0ti403i4', '0evw0fc5', 'gbzty2ia', 'k2qj6o7j', 'ax6v6ham', 'ga5vpakb', 'pidar1gz', '7i52vltp', '89fol3pq', '1de98sxz', 'de0xr8wd', 'hq4jb2wy', 'fs07zdu6', '37v59fs8', 'uffxyas1', 'dqour5jr', '9siu7wgs', '533qw3h2', 'tuzmu7p5', 'b1iyr42n', 'i8a5dxne', 'c5be70t6', '2fb4z0dr', 'o2hspsxj', 'zndtddty', 'afx977mr', '3jgedokv', 'ut61qxyc', 'yg2d3ohx', 'aw15xmia', 'bsypo08l', '1ls84u6w', 'v92f17b6', 'iql8h6td', '9a475qhj', 's9gje0pz', 'msohf5oa', '1ihz8z6q', 'vz98i5u3', 'gnw8adj5', 'qkl2yiqa', 'ivgwn32n', 'ov3r7izq', 'sbxqwfmy', 'e5nxkvo7', 'oxs048lr', '99kce33b', 'jsm6o5pq', 'j00m2ctc', 't7cdmy3e', 'udr1z9wf', 'ia3rct46', 'ectly1v4', '7nla9aji', 'utsr0zv7', 'j99cgsjt', 'cqeulecp', 'kvfau8j0', 'sbswy7el', 'sdtxi8xw', '8xz0ddci', 'rtguh0ow', '37dadupn', 'ifoe8x28', 'dqxfcwyu', 'ibnudp1x', '2tu707ng', 'txk3j2b7', 'bh198xgl', 'h0oqnue8', 'djuomhww', 'zaxjj9q7', '6wsx0jby', 'f4c9aorn', 'mrnid1i2', 'q7e8xa4u', 'qi9323yl', '602a2xdn', '24viekl7', '4sfgha4z', 'u65mey2z', 'eecyduzq', 'u2w2bb2p', 'qb0jtsdk', 'jjdtuofy', 'npcu4wq1', '2sjsxf96', 'eqewn6j4', 'xvfl7ycj', 'ggw5l1qm', '3x2psny9', '3xwbq8ov', '6c68pmem', '5p16rrpc', 'le0ogx1s', 'hxvib7wf', '7im0hci3', 'b5329o75', 'japmze1b', 'ip16lh2a', 'w8ewm8ve', 'roy4420g', '54t8xg3a', 's3q6l9hv', 'n4d3lmam', '4qenzjiu', '3hmsknon', 'e4hmo4yc', 'c0jjut0x', '5hth74jb', 'd11cj86k', 'xa8b1nuo', 'a5udnv5f', 'mn0l7nar', 'xubsodnk', 'yn8nzezq', 'jd6fvaop', '0nh58odf', 'xceg0iz9', '5f42du0b', 'fzq71ghi', 'ki7bn67o', 'jkejiuf2', '0l33i6s4', '3ll2tlzr', '9118j6pd', 'mvwkflnq', 'o47v5vgw', 'sl1zw5rz', 'yzcq3380', 'dhrtpo56', 'fiievwy7', 'lhh2b4r0', '4ismfcjt', '4j8b8z4t', 'kx0n2oy5', '4hvv4sep', '3oxnox9j', 'zdv0ilti', '4gpq5syb', '8gtnbm1c', 'sw4wtxdk', '7dlg582w', 'rmmp3gms', 'r16y83dm', 'j9525ohh', 'vs6ev73e', 'jj86scxd', '5aazi299', '4dtk1kyh', 'rq9hmjsx', 'qaz23cps', '82709xmy', 'mw0wb8a8', 'puqcbf8t', '1abp6oom', 'bfnav9sn', 'gpdzxp63', 'jkm496ip', 'cniyembt', 'y5n7zsct', '9ujofcsm', 'um2khqhn', 'h8abjsxr', 'bze2qz13', 'k3f7ohzg', 'dr7cz1jo', 'vx7ebtbp', '1aal6njl', 'zk44e4qy', 'tg8kfiot', '5w194etz', '8arwlhf0', 'safr9z37', 'a59qzsre', '3ta5lhnw', '93bvy38i', 'gyj5213f', 'nh29zqh2', 'k2ixwz9w', 'jg20ouix', 'h3czhqtt', 'uzdydmmh', '35k1tt3b', 'y4uo7o8g', 'deee71uw', '8c87geuy', 'p60pwfib', 'tvhybr82', '99s7lzdh', 'pphi42lj', 'kkknzw2i', 'jgr1beit', 'i6um5iu2', 'wwdn6uo2', 'h8cemq2n', 'wg86ws3b', 'v8i97mbx', 'jcu3pasy', 'u19e9j2w', '1p941spn', 'n3d130t8', 'kriro0t4', 'dv9m19yk', 'raelqb6j', '8m06zdho', 'tcrxm7jy', '18xs6375', '55loucvc', 'm5h19hy6', 'cvp10nx0', 't4ns3syl', 'i2ppoe55', 'ra28885e', 'xwjtjopl', 'oeqf7uk9', 'g9wmlvnq', 'sis4fjh5', 'hyjzofps', 'gfwqog3x', '6fxeqgtu', 'zoqatt6t', '5ezqv94a', 'bsnibsuf', 'qqsefagq', 'qi7l4beq', 'uaoyounl', '5oisrm5s', 'juz9jnfk', 'mudxyy68', 'qla6edp4', 'p68tyvdw', 'ssa59gng', 'cdejx8b9', 'erntrh3p', 'xppdm547', 'misprnq8', 'ro1qo4k6', 'qmp2tqtb', '02f0opkr', '6hyrcq7y', '8rxjju0t', 'v38tjof3', 'zy8qjaai', 's9mqv4lo', 'zhxrlopp', '8ruux4sw', 'b047c2sp', '6mef38eo', 'y29kct7v', '43gik8e3', 'qeehgxa1', 'k83zt387', '0pbjttv4', '7q45jz7z', 'wco27nop', '8pyjwe9x', 'oaujwxhq', '4ay3hsi7', '7vrm081c', 'jct2t2br', 'nfdgx0ye', 'c1nuvprk', '958u08vb', 'p6h2a0d5', 'welsrjao', 'yzffm05r', 'imvbkt69', 'sfv9tu3t', '5trqi9tp', 'tu1vevx9', 'e6h1qvdk', 'ksh63dpl', 'sphwclzs', 'qftsizis', 'wzcaugst', 'cdjhmk5z', 'f0ny4ur5', 'yqc43a5t', 'athjtu2j', 'bjhdy8mg', '3qqzthx8', 'ei9w19i7', '9blg1oke', 'w2uqaz8p', 'zi98dq1v', 'chrj5h72', 'jgx9msdq', 'h2uc7ria', '8fmykb4c', 'w0pbk3kv', '6k5ac3f2', '8l1vfsbc', 'nlzdto1h', 'q8dq3alv', 'p0y2rpqa', 'mrsya6wz', 'q0yqnlyl', '5pv11lfo', 'cq0jkb1d', 'sn7rswab', '33fs6exl', 'xly61tfw', '8ybfiz8f', 'cks6aij2', '8hiurkho', 'ys6s9rps', 'dkbujz83', 'bn7hsuv9', 'jljjqs6m', '50xzptr1', 'n0n4n6ut', 'onciplo1', 'g8rsiiy7', 'gv1k7u7j', '2wshgzjk', 'awitk3se', '8fuj072z', 'b05qpb7d', 'k2juhyex', 'nz02frdm', '7mldvo3j', 'pbuevs5v', 'ctvnmjsl', 'tsb6gul8', 'wim5q9a5', '37l48ch4', 'vpih1wvs', 'eyf2dogw', 'sn1a7ikq', 'ore3r4m3', '901m6zw0', 'ip38x8bu', 'ohyrqs9v', 'nvavj9gk', 'nm6nu8pj', 'yqeifpoy', '1esupl4q', 'w3catjj3', '1a8uevk8', 'he853mwa', 'o1uol5u3', 'a1chcrk8', 'j7fx64k1', '5uwzo304', 'rvg5elrc', 'jmrg4oeb', 't8azymo7', 'l41pnvfd', 'ndt8f812', 'v4mbry22', 'xo3lk95l', '594qzvsk', '41378qru', 'kem4x4wr', 'i14gkdw0', 'ns628u21', 'mjfbvl5n', 'eezzpg42', 'j530ia4u', 'nz8qvuw6', '4ko557n1', '52kqp9yw', 'xg0pkjrh', '010vptx3', '2e4gz2bo', 'rcwck1y3', 'ks3fn9pq', 'hsowmxan', 'w8cvq0m5', '1bapn9w0', 'e41vn10l', 'vpodtbjk', 'y691hark', 'ft4rbcxf', '1c47w4q5', 'nn15iyqd', 'p57tsbyw', '2tyt8255', 'srrd9ip4', '2qto9vsb', 'c22mytwr', 'ky33ju30', 'amp19v88', '21htepa1', '73a7uvyz', '5xeergll', 'pjgngs6u', 'lxakf79k', 'n6jgr1kx', '69jikv6s', '30duqivi', 'vndydx85', 'h2h4bnd5', 'b2ju3poi', 'djclli8n', 'szsb1oan', 'zph6r4il', '84hxim2n', 'd2knbzhl', 'l5oaxugi', 'iyv86437', '3zy5dgxz', 'xcsl2evz', '9aegg5sd', 'la6xtpjf', 'izltn367', 'mweeepam', 'ork04nx8', 't8s4s0wo', '2w3bx6p8', 'xdzbwa6z', 's12mcitm', 'xsrqjk3m', 'ol0bj3hs', 'hrbeh0me', 'jsz2lg6l', 'zknmfgsh', '330hioeg', '4mtnqfqw', '73xil5op', 'j43v6a8b', 'i6pv90s9', 'vt8w7z01', 'nc05llre', 'kbvy4g7i', 'yb54i1ne', 'zixnvehs', 'xuczplaf', 'uz91cd6h', 'kh06egig', '9atjk9si', 'giabjjnz', 'zv0ysi8m', '89sg0cpk', 'y9eoyo45', '4ihv80au', 'pkxc2219', 'qqbfpe4b', 'tohbzenc', 'rbnfh89u', 'klzen04m', 'd13bq4ij', '6cbnpqjj', '84ib5ol5', 'jm4bu1bf', 'lnys6iuu', 'zf98jzdn', 'yacnxh6i', 'puuqv6zk', 'ik1y1fln', 'tku1dr32', 'lfo6otkc', 'qele28zk', 'e4sl5zaj', 'gcwtn3ei', 'vcgsx07g', '1ybj2p1n', 'kvyvwhja', 'oxs4o9xe', 'mbb4oj3i', 'hu9oxea1', 'sob33vqy', '6yh82e1w', '6lngz7y1', '0b4o0ccp', 'efarquxn', 'b8j2hn9w', 'hhpnewdt', '479wkesk', 'zzpw375i', 'wvicjqdj', 'cbc98t7x', 'qkadqohe', 'dp2xzul1', 'pauzgepm', '3otc2ac1', 'q5inrxcm', '3b5jzndg', 'kromgrnu', 'hdkmzdq9', 'blqzi69t', '9il7coyk', 'u7egknq6', 'sh7lrdou', 'xgnv3eiw', 'yy7abob9', '78w5fafs', 'v22k72tw', 'q8dl0gcc', '8xm0kacj', 'cyp9fbw0', 'd6eh49hu', 'b2f3a8un', '6tzx945b', 'piwowtxa', '6bzfptbs', 'j5teavse', 'ynze6yaz', 'iicbaevw', 'm3nqa321', '7aj3fx5u', '7zbvgvk8', '3k1ks3wg', 'j4b2534p', '8eg8zqnw', 'ne0qwv56', 'gttuxtw6', 'ogqsi3ns', 'ewdfqktw', 'caturc6q', '0chuwvg6', 'qubmr10h', 'j53i5xfs', '10ecm4wi', 'qhelk6u6', 'o004ggon', 'm40kwgcg', 'mbm5bufs', 'w0lgteil', 'tsje2x90', 'wjm0aox6', 'iqr419fp', 'jdpxu0ef', 'u7u75sl0', 'h13v8i14', 'wb19vqv3', 'j6y806qu', '50jgbtnr', 'dblrxlt1', 'f5yjz0zp', 'siz32uvy', 'ze8bdzkb', 'fcd07krs', 'dckuhrlf', 'hp5x637c', '04ftw7k9', 'ii6ih2r0', 'm0cnkpb6', 'z11exbyu', 'tug2t7dm', 'x9az3twa', 'z122v1uz', '5hio4lgc', '0be4wta5', 'vahts6pu', 'iy0h2pyf', 'a5xknb8f', 'lt09z61w', 'frkk6w0a', '2s7ki6g7', 'vqbreyna', 'ik15f074', 'jfjnqgr1', 'mvdq6fw0', 'aqdlfl37', 'fyi48tnn', 'ei89dddm', 'q856rx6b', '7ij5el3p', 'pwvcwlh8', '417tzufc', '8aj97x4n', '8l411r1w', '44l5q07k', 'svxz6pm6', 'j318qn5p', 'thke7tdu', '8f5m0gej', 'r71g2e9y', '2054tkb7', 'k86pf2yf', 'uhq8tp4u', '0604jed8', '3okdfxzq', 'cns12o35', '1dogron2', 'v2syspu6', 'yaspd6l7', 'qmyb365g', 'code9hzz', 'o3b5zm5l', '8t35z4gl', 'g5fqxdqh', 'ghcbttba', 'p9tx2oer', '30j0auua', 'bwfpf02e', 'cnxybwfb', '7yhko91n', 'ig0rnbqb', '3pvstiz6', '1bvsn9e8', '6wu024ng', '05qglt1f', '2gokv7id', 'tvoxbi3q', 't17wgjfv', 'iy4knx7j', 'fas2ar3k', 'r6a6ro7w', 'zjmiw6f3', 'cqin4o8l', 'qnntyqud', 'gbkgzvc8', 'cetdqgff', 'p0kv1pht', '42wv7zl6', '2ftw85xw', 'm142t5nj', '33pxylb3', 'evutu0lx', 'zawf4jzm', 'f2zpapwe', '391max5m', '2b25p7t7', 'f3g3ek5u', 'vof63qat', 'fzd4o4xd', 'owhx6k4t', '26pcjts9', 'fb0nwixw', 'n11fcwmw', 'uadfehr6', 'zpaqd5vd', '1xf2sxtv', '3840bxvn', '0m5mc320', '75cotmn2', 'i5hkucrn', '9c09xlma', 'jm18lj5t', 'cigv3t0h', 'v6ci69n0', '6ml0puh8', '2lk8vq03', '60p2a1i2', 'tnfgbl05', 'w7hezm6e', '2uxptj0y', '32vs98ya', '7cb88t8n', 'e1urdt9w', 'c3geck81', 'agchvvx9', 'nnftb092', 'fofr17c8', 'qyg8hn56', 'v4c1lgpd', '0paafp5j', 'v3631w0b', '0y34yxlb', 'ztcyvsoi', 'm0waducs', 'l0z79udh', 'v861kk0i', 'idawavsd', 'jlo2zol8', 'tnyuk2gn', '3jireyep', 'aivub6mi', '65nroic5', '5lm5ttai', '22fc1qly', 'ypgrw78s', 'eioy4rhn', 'qxkxjxsz', 'q2gwajdj', 'et6wjhpf', '4nt20c06', 'mhytn634', 'c834itam', 'j8n06hzx', '9xs8aukq', 'rg5xsoe2', 'v08cs51n', '0khg28ex', 'lj8t52yl', 'tv13furv', '1g2mup0k', '5jwmvjm6', 'xummm9xu', 'p7aam9kw', 'ex7rta8f', 'hib30ct6', 'p4x32v3u', 'gz2k3bvo', '16ciqu9w', 'eyitkr3s', '76laky91', '662pfa61', 'rwsfw1ei', '2m6ks6nd', 'x9bxnrtn', '679qfp2s', 'dcll52nd', 'f03san07', '0cvoeiy0', 'oq16de67', '11edrkav', 'l6xgrxp5', 'o1r7l4v1', 'qutdsod1', '8ccl9aui', 'nw0jbs1s', 'k21qt6ee', 'n5xyqqp5', '23yi8so0', '7v5aln90', 'i2qi4wm0', '005b2j4b', 'vmhrt6bk', 'ymhcouo5', '16rgt4ca', '74k12s1z', '9i024s08', 'fqs40ivc', '51w1fe7k', 'jfd5gd2p', 'ar6psvlo', 'row2mn17', 'gr9l1uja', '97gawzw4', 'hrdfyd8h', 'ipl6189w', 'up929iw0', 'iwvrpbao', 'lmqm1bio', '1i6n6tv6', 'le0ftbps', 'ot3wi5tc', 'b9vkja80', 'kw7mon2o', '2l4xxu3v', 'h3imwhss', 'epcel2ez', '9jb3w0zu', '2y452utz', 'ich6h3w6', 'klj710h2', 'v0qy3yqa', 'gq3965se', 'jf4pdfuy', 'c0r2nbzo', 'yq05djrc', '21fhsooy', 'b4kzgubs', 'rvr86c6c', '9h4pq7up', 'l9vtsj3e', 'f8xrgguq', '24yavi1w', 'e07vt846', 'hmvo5b0q', 'xvhg4m16', 'no3etlvi', 'p15bdmo2', '7kw9lws0', 'olior5vk', 'mtv80pjo', 'qq48448d', 'pg6893i5', '8nv1a76u', 'gwh0tsvv', 'tomsdx3z', 'edc509xr', 'lbpve38s', 'fwdpzv85', '2mrtq4ya', 'k0f9fser', 'rerkx6js', 'beguhous', 'oi0zsdtd', '9ec6hygx', 'aeyf0yu1', '2ytec133', 'myu0e9vf', '2u4d235j', '6ifjzt93', 'dwf5vydu', 'kqbeinez', 'g3odc7da', 'py6qu4tl', 'sh8julcb', 'gy0kfhy6', '2zaxn6tq', '96zsd27n', '1x66nxgx', 'q9m3kjlu', '3qzlo90e', 'a845az43', 'mtfrbn87', 'd6by9p41', 'ufmcfic2', 'cn89d1mx', 'ulvep69h', 'l48iq9yj', 'orqlbw9m', 'zsx7wfyj', 'cp6he5mc', 'r48tp5py', 'fxixiggj', '97edhhkt', '9pl7mta8', '8g70j0qw', 'btwrjr87', '5283jsnu', 'wl121lg4', 'ejg6hpuy', 'sj84atb0', '4mqxa2nw', 'w7ej6jfg', '9ae26r2z', 'n3sb0pze', 'x5cqpnnw', 'lpbb4rga', '06ya15z8', 'fnj57b0l', 'ox7xetlq', 'qb2a4s2p', 'hfgmmuy6', 'aj2ixlo7', 'wmfcey6f', 'bgialj4d', '8d1076ge', 'tt0lj7rp', 'vahud6o5', 'urkuofmu', '9edp04ud', 'jahm572k', '87d7gzgb', '73ylxhb7', 'mexfkial', 'ca380xbl', '6o8dt2bg', 'puj74k7y', 'e9ix35x3', 'wrmfxkoa', 'aoz3j9rl', '5sfojk7y', 'ijsn8d7b', 'jwxt4ygt', 'ccdj7137', '5d7zien3', 'z14rf85c', '1s25r3o0'}\n"
     ]
    }
   ],
   "source": [
    "# We select all cord_uid in relevance_data for the query 1\n",
    "cords_relevance_data = set()\n",
    "for i in range(relevance_data.loc[relevance_data.topic_id==1].shape[0]):\n",
    "    cords_relevance_data.add(relevance_data.loc[relevance_data.topic_id==1].cord_uid[i])\n",
    "print(cords_relevance_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cord_uid                                                sha  \\\n",
      "0     sw4wtxdk           4faf1ac964c605b384dda60bc37df300766401b9   \n",
      "1     6wu024ng           f2ab1be1bbd80c0f102714fdc90597af2739442c   \n",
      "2     sbxqwfmy           c9b0389a55de2f9cbfe37049d1072e0984613923   \n",
      "3     1rhy8td0           8a6809df45d5f80a822d68d3c305f7640e10234a   \n",
      "4     t7rxmzvi           1a162c4dd45cad2c49168b2d6f2c350e47a3db09   \n",
      "...        ...                                                ...   \n",
      "1215  py6qu4tl  dca8ced82157924ed86c698a7dd482be81b4b266; ff9f...   \n",
      "1216  4qenzjiu           fdf021cfe745daed338cce7eaa5e548581477ff4   \n",
      "1217  fozglfc8           422bcfff056d118337d3941c0afcb8b888142182   \n",
      "1218  gy0kfhy6           877f7dfd596cabc12cf7228ffb19cd6b663cea93   \n",
      "1219  rcwck1y3           7db1d6af433a96af8d1dac0fdd078ea9d1980e9c   \n",
      "\n",
      "                                                  title  \\\n",
      "0     NSs Encoded by Groundnut Bud Necrosis Virus Is...   \n",
      "1     Comparative Efficacy of Hemagglutinin, Nucleop...   \n",
      "2     Elevation of Intact and Proteolytic Fragments ...   \n",
      "3     Large-scale evolutionary surveillance of the 2...   \n",
      "4     Angiotensin-converting enzyme 2 autoantibodies...   \n",
      "...                                                 ...   \n",
      "1215  Teicoplanin potently blocks the cell entry of ...   \n",
      "1216  A Diallel of the Mouse Collaborative Cross Fou...   \n",
      "1217               Cytokine Storm Induced by SARS-CoV-2   \n",
      "1218  Management and Treatment of COVID-19: The Chin...   \n",
      "1219  Moroccan Medicinal plants as inhibitors agains...   \n",
      "\n",
      "                                               abstract publish_time  \\\n",
      "0     Groundnut bud necrosis virus (GBNV), a member ...   2010-03-18   \n",
      "1     Efforts to develop a broadly protective vaccin...   2010-03-23   \n",
      "2     The earliest immune responses activated in acu...   2010-05-06   \n",
      "3     In April 2009, a new influenza A (H1N1 2009) v...   2010-02-25   \n",
      "4     Traditionally viewed as important in the regul...   2010-06-28   \n",
      "...                                                 ...          ...   \n",
      "1215  Since December 2019, the outbreak of a new cor...   2020-02-13   \n",
      "1216  Reproductive success in the eight founder stra...   2019-03-15   \n",
      "1217  A novel coronavirus disease 2019 (COVID-19) tr...   2020-06-10   \n",
      "1218  Abstract With over 1,800,000 cases and 110,000...   2020-04-17   \n",
      "1219  The new Corona-virus, recently called the seve...   2020-05-06   \n",
      "\n",
      "                                                authors              journal  \\\n",
      "0     Lokesh, Bhushan; Rashmi, Panigrahi R.; Amruta,...             PLoS One   \n",
      "1     Rao, Srinivas S.; Kong, Wing-Pui; Wei, Chih-Je...             PLoS One   \n",
      "2     Kramer, Holger B.; Lavender, Kerry J.; Qin, Li...          PLoS Pathog   \n",
      "3     Lee, Charlie Wah Heng; Koh, Chee Wee; Chan, Ya...    Nucleic Acids Res   \n",
      "4                                      Chappell, Mark C   Arthritis Res Ther   \n",
      "...                                                 ...                  ...   \n",
      "1215  Zhang, Junsong; Ma, Xiancai; Yu, Fei; Liu, Jun...              bioRxiv   \n",
      "1216  Shorter, John R.; Maurizio, Paul L.; Bell, Tim...        G3 (Bethesda)   \n",
      "1217  Song, Peipei; Li, Wei; Xie, Jianqin; Hou, Yanl...       Clin Chim Acta   \n",
      "1218  Peng, Fujun; Tu, Lei; Yang, Yongshi; Hu, Peng;...        Can J Cardiol   \n",
      "1219  Aanouz, I.; Belhassan, A.; El-Khatabi, K.; Lak...  J Biomol Struct Dyn   \n",
      "\n",
      "                                         pdf_json_files  \n",
      "0     document_parses/pdf_json/4faf1ac964c605b384dda...  \n",
      "1     document_parses/pdf_json/f2ab1be1bbd80c0f10271...  \n",
      "2     document_parses/pdf_json/c9b0389a55de2f9cbfe37...  \n",
      "3     document_parses/pdf_json/8a6809df45d5f80a822d6...  \n",
      "4     document_parses/pdf_json/1a162c4dd45cad2c49168...  \n",
      "...                                                 ...  \n",
      "1215  document_parses/pdf_json/dca8ced82157924ed86c6...  \n",
      "1216  document_parses/pdf_json/fdf021cfe745daed338cc...  \n",
      "1217  document_parses/pdf_json/422bcfff056d118337d39...  \n",
      "1218  document_parses/pdf_json/877f7dfd596cabc12cf72...  \n",
      "1219  document_parses/pdf_json/7db1d6af433a96af8d1da...  \n",
      "\n",
      "[1220 rows x 8 columns]\n",
      "1647\n"
     ]
    }
   ],
   "source": [
    "# We select all cord_uid that are in both (relevance_data and dt) for the query 1\n",
    "dt_rel_data = pd.DataFrame()\n",
    "aux = 0\n",
    "\n",
    "for i in range(dt.cord_uid.shape[0]):\n",
    "    #print(dt.cord_uid[i])\n",
    "    if dt.cord_uid[i] in cords_relevance_data:\n",
    "        #print(dt.cord_uid[i])\n",
    "        dt_rel_data[aux] = dt.iloc[i]\n",
    "        aux +=1\n",
    "dt_rel_data = dt_rel_data.T\n",
    "print(dt_rel_data)\n",
    "print(len(cords_relevance_data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf TfidfModel(num_docs=1220, num_nnz=96453)\n",
      "QUERY: coronavirus origin\n",
      "[ Score = 0.58731437 ] Origin and Evolution of the 2019 Novel Coronavirus\n",
      "[ Score = 0.55520093 ] Characteristics of Metazoan DNA Replication Origins\n",
      "[ Score = 0.53614223 ] Bat origin of a new human coronavirus: there and back again\n",
      "[ Score = 0.41098344 ] Commentary: Origin and evolution of pathogenic coronaviruses\n",
      "[ Score = 0.41098344 ] Strategies to trace back the origin of COVID-19\n",
      "[ Score = 0.37517485 ] A glimpse into the origins of genetic diversity in SARS-CoV-2\n",
      "[ Score = 0.36360946 ] A phylogenomic data-driven exploration of viral origins and evolution\n",
      "[ Score = 0.34331477 ] Experimental infection of a US spike-insertion deletion porcine epidemic diarrhea virus in conventional nursing piglets and cross-protection to the original US PEDV infection\n",
      "[ Score = 0.33538014 ] Zoonotic origins of human coronavirus 2019 (HCoV-19 / SARS-CoV-2): why is this work important?\n",
      "[ Score = 0.33410308 ] Tracking the origin of early COVID-19 cases in Canada\n"
     ]
    }
   ],
   "source": [
    "# Now we rank for the query 1\n",
    "value = topics[1]\n",
    "rank = launch_query(dt_rel_data, value[\"query\"], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance evaluation -- All Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to ranking for all the queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "QUERY NUMBER  1\n",
      "tfidf TfidfModel(num_docs=1220, num_nnz=96453)\n",
      "QUERY: coronavirus origin\n",
      "[ Score = 0.58731437 ] Origin and Evolution of the 2019 Novel Coronavirus\n",
      "[ Score = 0.55520093 ] Characteristics of Metazoan DNA Replication Origins\n",
      "[ Score = 0.53614223 ] Bat origin of a new human coronavirus: there and back again\n",
      "[ Score = 0.41098344 ] Commentary: Origin and evolution of pathogenic coronaviruses\n",
      "[ Score = 0.41098344 ] Strategies to trace back the origin of COVID-19\n",
      "[ Score = 0.37517485 ] A glimpse into the origins of genetic diversity in SARS-CoV-2\n",
      "[ Score = 0.36360946 ] A phylogenomic data-driven exploration of viral origins and evolution\n",
      "[ Score = 0.34331477 ] Experimental infection of a US spike-insertion deletion porcine epidemic diarrhea virus in conventional nursing piglets and cross-protection to the original US PEDV infection\n",
      "[ Score = 0.33538014 ] Zoonotic origins of human coronavirus 2019 (HCoV-19 / SARS-CoV-2): why is this work important?\n",
      "[ Score = 0.33410308 ] Tracking the origin of early COVID-19 cases in Canada\n",
      "##############\n",
      "QUERY NUMBER  2\n",
      "tfidf TfidfModel(num_docs=987, num_nnz=79596)\n",
      "QUERY: coronavirus response to weather changes\n",
      "[ Score = 0.40458322 ] Advice-giving in newspaper weather commentaries\n",
      "[ Score = 0.35992858 ] Weather-Dependent Risk for Legionnaires’ Disease, United States\n",
      "[ Score = 0.34849778 ] Weather Conditions and COVID-19 Transmission: Estimates and Projections\n",
      "[ Score = 0.34285784 ] Regulation of Stress Responses and Translational Control by Coronavirus\n",
      "[ Score = 0.32625523 ] The Weather Impacts the Outbreak of COVID-19 in Mainland China\n",
      "[ Score = 0.3185863 ] Coronavirus, Climate Change, and a Bit of Hope\n",
      "[ Score = 0.31378576 ] Effect of weather on COVID-19 spread in the US: A prediction model for India in 2020\n",
      "[ Score = 0.3087086 ] Vulnerable Populations: Weathering the Pandemic Storm\n",
      "[ Score = 0.2942183 ] China’s response to a novel coronavirus stands in stark contrast to the 2002 SARS outbreak response\n",
      "[ Score = 0.2942168 ] Comparison of culturable antibiotic-resistant bacteria in polluted and non-polluted air in Beijing, China\n",
      "##############\n",
      "QUERY NUMBER  3\n",
      "tfidf TfidfModel(num_docs=1313, num_nnz=103542)\n",
      "QUERY: coronavirus immunity\n",
      "[ Score = 0.57634246 ] Coronavirus Immunity: From T Cells to B Cells\n",
      "[ Score = 0.5316733 ] A phase trial of the oral Lactobacillus casei vaccine polarizes Th2 cell immunity against transmissible gastroenteritis coronavirus infection\n",
      "[ Score = 0.52998435 ] Defining trained immunity and its role in health and disease\n",
      "[ Score = 0.50714993 ] SARS‐CoV‐2 infection‐induced immune responses: friends or foes?\n",
      "[ Score = 0.5059397 ] SARS CORONAVIRUS AND INNATE IMMUNITY\n",
      "[ Score = 0.47894883 ] Molecular mechanisms of primary and secondary mucosal immunity using avian infectious bronchitis virus as a model system\n",
      "[ Score = 0.47621366 ] COVID-19: Immunology and treatment options\n",
      "[ Score = 0.4554844 ] Reflection on lower rates of COVID-19 in children: does childhood immunizations offer unexpected protection?\n",
      "[ Score = 0.45298868 ] Children coronavirus dilemma\n",
      "[ Score = 0.44985706 ] Cross-immunity between respiratory coronaviruses may limit COVID-19 fatalities\n",
      "##############\n",
      "QUERY NUMBER  4\n",
      "tfidf TfidfModel(num_docs=1383, num_nnz=110149)\n",
      "QUERY: how do people die from the coronavirus\n",
      "[ Score = 0.47851217 ] To All Doctors: What You Can Do to Help as a Bunch of Older People Are About to Get Sick and Die\n",
      "[ Score = 0.44592467 ] Die Auswirkungen der SARS-CoV-2-Pandemie auf die Sporttraumatologie\n",
      "[ Score = 0.42390466 ] The Covid-Shock Doctrine: Under the Tutorship of CoV-2, the Voice(s) From Poland\n",
      "[ Score = 0.29693434 ] Novel coronavirus: From discovery to clinical diagnostics\n",
      "[ Score = 0.29495844 ] What is the people posting about symptoms related to Coronavirus in Bogota, Colombia?\n",
      "[ Score = 0.28728256 ] COVID-19 und die Niere\n",
      "[ Score = 0.280062 ] COVID 19 in INDIA: Strategies to combat from combination threat of life and livelihood\n",
      "[ Score = 0.27267474 ] A dual-centre observational review of hospital based palliative care in patients dying with COVID-19\n",
      "[ Score = 0.26749063 ] Coronavirus Optimization Algorithm: A bioinspired metaheuristic based on the COVID-19 propagation model\n",
      "[ Score = 0.26562992 ] Uncharakteristisches Fieber (UF), afebrile Allgemeinreaktion (AFAR), Luftwegekatarrhe, Tonsillitis\n",
      "##############\n",
      "QUERY NUMBER  5\n",
      "tfidf TfidfModel(num_docs=1341, num_nnz=108617)\n",
      "QUERY: animal models of COVID-19\n",
      "[ Score = 0.70257807 ] The Importance of Animal Models in the Development of Vaccines\n",
      "[ Score = 0.66789097 ] Commentary on two reports on animal models of COVID‐19\n",
      "[ Score = 0.6250793 ] Current global vaccine and drug efforts against COVID-19: Pros and cons of bypassing animal trials\n",
      "[ Score = 0.60150754 ] Animal Models for the Study of Neuroimmunological Disease\n",
      "[ Score = 0.5956446 ] Is there an ideal animal model for SARS?\n",
      "[ Score = 0.5609352 ] Chapter 38 Animal Models of Human Viral Diseases\n",
      "[ Score = 0.55326945 ] Animal models: an important tool in mycology\n",
      "[ Score = 0.50912917 ] Experimental models of demyelination and remyelination\n",
      "[ Score = 0.5083907 ] Chapter 33 Animal Models of Human Viral Diseases\n",
      "[ Score = 0.5074081 ] Animal Models of Aspiration Pneumonia\n",
      "##############\n",
      "QUERY NUMBER  6\n",
      "tfidf TfidfModel(num_docs=1227, num_nnz=99747)\n",
      "QUERY: coronavirus test rapid testing\n",
      "[ Score = 0.6118541 ] ‘Test, re-test, re-test’: using inaccurate tests to greatly increase the accuracy of COVID-19 testing\n",
      "[ Score = 0.55604434 ] Clinical Testing For Covid-19\n",
      "[ Score = 0.555044 ] Rapid point-of-care testing for SARS-CoV-2 in a community screening setting shows low sensitivity\n",
      "[ Score = 0.54046774 ] The impact of rapid molecular diagnostic testing for respiratory viruses on outcomes for emergency department patients\n",
      "[ Score = 0.53189594 ] \"No test is better than a bad test\": Impact of diagnostic uncertainty in mass testing on the spread of Covid-19\n",
      "[ Score = 0.5316166 ] Ultra-sensitive nanozyme-based chemiluminescence paper test for rapid diagnosis of SARS-CoV-2 infection\n",
      "[ Score = 0.5312647 ] Current and emerging diagnostic tests available for the novel COVID-19 global pandemic\n",
      "[ Score = 0.52186173 ] Variation of positiveness to enhance testing of specimens during an epidemic\n",
      "[ Score = 0.51521 ] Development of a rapid test kit for SARS-CoV-2: an example of product design\n",
      "[ Score = 0.51293486 ] Rapid Antigen Tests for Diagnosis of Pandemic (Swine) Influenza A/H1N1\n",
      "##############\n",
      "QUERY NUMBER  7\n",
      "tfidf TfidfModel(num_docs=988, num_nnz=79898)\n",
      "QUERY: serological tests for coronavirus\n",
      "[ Score = 0.4889766 ] Investigation of COVID-19 cases and use of serological tests\n",
      "[ Score = 0.47824678 ] Testing for SARS-CoV-2 (COVID-19): a systematic review and clinical guide to molecular and serological in-vitro diagnostic assays\n",
      "[ Score = 0.45270437 ] Seroconversion rate and diagnostic accuracy of serological tests for COVID-19\n",
      "[ Score = 0.44186494 ] Sequential serological surveys in the early stages of the coronavirus disease epidemic: limitations and perspectives\n",
      "[ Score = 0.43392798 ] Modeling serological testing to inform relaxation of social distancing for COVID-19 control\n",
      "[ Score = 0.42554072 ] Evaluation of serological tests for SARS-CoV-2: Implications for serology testing in a low-prevalence setting\n",
      "[ Score = 0.42483047 ] An Evolving Approach to the Laboratory Assessment of COVID‐19\n",
      "[ Score = 0.42223245 ] Estimating seroprevalence with imperfect serological tests: exploiting cutoff-free approaches\n",
      "[ Score = 0.4207378 ] Comparison of SARS-CoV-2 serological tests with different antigen targets\n",
      "[ Score = 0.41730535 ] Immunological assays for SARS-CoV-2: an analysis of available commercial tests to measure antigen and antibodies\n",
      "##############\n",
      "QUERY NUMBER  8\n",
      "tfidf TfidfModel(num_docs=1488, num_nnz=123312)\n",
      "QUERY: coronavirus under reporting\n",
      "[ Score = 0.5557481 ] Coronavirus infections reported by ProMED, February 2000–January 2020\n",
      "[ Score = 0.46483216 ] Prevalence of coronavirus\n",
      "[ Score = 0.39705214 ] Bacterial and fungal co-infection in individuals with coronavirus: A rapid review to support COVID-19 antimicrobial prescribing\n",
      "[ Score = 0.38640136 ] Update: Severe Respiratory Illness Associated with a Novel Coronavirus — Worldwide, 2012–2013\n",
      "[ Score = 0.38346696 ] Genetic evolution analysis of 2019 novel coronavirus and coronavirus from other species\n",
      "[ Score = 0.37953386 ] Fighting the Coronavirus Outbreak\n",
      "[ Score = 0.37349296 ] CT Manifestations of Novel Coronavirus Pneumonia: A Case Report\n",
      "[ Score = 0.3489275 ] Asymptomatic novel coronavirus pneumonia patient outside Wuhan: The value of CT images in the course of the disease\n",
      "[ Score = 0.3464343 ] The Second Worldwide Wave of Interest in Coronavirus since the COVID-19 Outbreaks in South Korea, Italy and Iran: A Google Trends Study\n",
      "[ Score = 0.34526867 ] Cats, coronaviruses and coronavirus antibody tests\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "QUERY NUMBER  9\n",
      "tfidf TfidfModel(num_docs=1235, num_nnz=98401)\n",
      "QUERY: coronavirus in Canada\n",
      "[ Score = 0.5790585 ] Genome Organization of Canada Goose Coronavirus, A Novel Species Identified in a Mass Die-off of Canada Geese\n",
      "[ Score = 0.45560414 ] Climate Change and Health in Canada\n",
      "[ Score = 0.41809103 ] Hospital-Based HTA and Know4Go at MEDICI in London, Ontario, Canada\n",
      "[ Score = 0.4078517 ] Novel Coronavirus, Old Partisanship: COVID-19 Attitudes and Behaviours in the United States and Canada\n",
      "[ Score = 0.40750474 ] Canada 2010: what should global health expect?\n",
      "[ Score = 0.40750474 ] The Impact of COVID-19 on Diabetes Research in Canada\n",
      "[ Score = 0.37199923 ] COVID-19 in Canada and the use of Personal Protective Equipment\n",
      "[ Score = 0.36303407 ] The United States and Canada as a coupled epidemiological system: An example from hepatitis A\n",
      "[ Score = 0.32216078 ] Effect of COVID-19 on the mental health care of older people in Canada\n",
      "[ Score = 0.3080446 ] Human Bocavirus Infection, Canada\n",
      "##############\n",
      "QUERY NUMBER  10\n",
      "tfidf TfidfModel(num_docs=871, num_nnz=71367)\n",
      "QUERY: coronavirus social distancing impact\n",
      "[ Score = 0.43067294 ] Impact of social distancing during COVID-19 pandemic on crime in Los Angeles and Indianapolis\n",
      "[ Score = 0.41857493 ] Coronavirus disease (COVID-19) – impact on vaccine preventable diseases\n",
      "[ Score = 0.4088481 ] The Impact of Social Distancing on the Transmission of Influenza Virus, South Korea, 2020\n",
      "[ Score = 0.39954972 ] Modelling the potential impact of social distancing on the COVID-19 epidemic in South Africa\n",
      "[ Score = 0.39640793 ] The effect of uncontrolled travelers and social distancing on the spread of novel coronavirus disease (COVID-19) in Colombia\n",
      "[ Score = 0.39427662 ] COVID-19 and the impact of social determinants of health\n",
      "[ Score = 0.39197576 ] Genetic evolution analysis of 2019 novel coronavirus and coronavirus from other species\n",
      "[ Score = 0.3879554 ] Coronavirus y atención primaria\n",
      "[ Score = 0.3879554 ] Un nuevo coronavirus emerge\n",
      "[ Score = 0.38567808 ] When Blackness Does Not Fade After a Pandemic: An Appeal to Acknowledge the Unequal Burden of Social Isolation\n",
      "##############\n",
      "QUERY NUMBER  11\n",
      "tfidf TfidfModel(num_docs=1334, num_nnz=109189)\n",
      "QUERY: coronavirus hospital rationing\n",
      "[ Score = 0.470519 ] Natural disaster and rationing of care\n",
      "[ Score = 0.38417718 ] Equality or utility? Ethics and law of rationing ventilators\n",
      "[ Score = 0.38417718 ] Rational use of face masks in the COVID-19 pandemic\n",
      "[ Score = 0.33270717 ] Rationing of Medical Supplies Including Ventilators in Patients With Kidney Disease\n",
      "[ Score = 0.31367934 ] Rationing Medical Resources fairly during the Covid -19 Crisis: Is this possible in India(or America)?\n",
      "[ Score = 0.30094495 ] Vaccine Rationing and the Urgency of Social Justice in the Covid‐19 Response\n",
      "[ Score = 0.29758236 ] ISUOG Consensus Statement on rationalization of gynecological ultrasound services in context of SARS‐CoV‐2\n",
      "[ Score = 0.27165428 ] ISUOG Consensus Statement on rationalization of early‐pregnancy care and provision of ultrasonography in context of SARS‐CoV‐2\n",
      "[ Score = 0.24723282 ] Community Pharmacists in Taiwan at the Frontline Against the Novel Coronavirus Pandemic: Gatekeepers for the Rationing of Personal Protective Equipment\n",
      "[ Score = 0.21536364 ] What face mask for what use in the context of COVID-19 pandemic? The French guidelines\n",
      "##############\n",
      "QUERY NUMBER  12\n",
      "tfidf TfidfModel(num_docs=1209, num_nnz=90067)\n",
      "QUERY: coronavirus quarantine\n",
      "[ Score = 0.6305082 ] Coronavirus and quarantine: will we sacrifice our elderly to protect them?\n",
      "[ Score = 0.5755726 ] Novel coronavirus, poor quarantine, and the risk of pandemic\n",
      "[ Score = 0.5624691 ] Coronavirus, humanpathogenes\n",
      "[ Score = 0.50288814 ] The psychological impact of quarantine and how to reduce it: rapid review of the evidence\n",
      "[ Score = 0.50271994 ] Quarantine for SARS, Taiwan\n",
      "[ Score = 0.49502802 ] Quarantine and its legal dimension\n",
      "[ Score = 0.48090383 ] Home quarantine or centralized quarantine, which is more conducive to fighting COVID-19 pandemic?\n",
      "[ Score = 0.47694573 ] The psychological effects of quarantine during COVID-19 outbreak: Sentiment analysis of social media data\n",
      "[ Score = 0.47184286 ] Dynamics of a model with quarantine-adjusted incidence and quarantine of susceptible individuals\n",
      "[ Score = 0.46995306 ] The Outbreak Cases with the Novel Coronavirus Suggest Upgraded Quarantine and Isolation in Korea\n",
      "##############\n",
      "QUERY NUMBER  13\n",
      "tfidf TfidfModel(num_docs=1401, num_nnz=107408)\n",
      "QUERY: how does coronavirus spread\n",
      "[ Score = 0.8018713 ] Coronavirus: the spread of misinformation\n",
      "[ Score = 0.56183034 ] Potential for global spread of a novel coronavirus from China\n",
      "[ Score = 0.524948 ] The spread of novel coronavirus has created an alarming situation worldwide\n",
      "[ Score = 0.4629606 ] Spread of Middle East Respiratory Coronavirus: Genetic versus Epidemiological Data\n",
      "[ Score = 0.44186217 ] Coronavirus Disease 19 (COVID-19): Implications for Clinical Dental Care\n",
      "[ Score = 0.43077296 ] Review on Machine and Deep Learning Models for the Detection and Prediction of Coronavirus\n",
      "[ Score = 0.41997004 ] The contribution of dry indoor built environment on the spread of Coronavirus: Data from various Indian states\n",
      "[ Score = 0.41876364 ] The effect of uncontrolled travelers and social distancing on the spread of novel coronavirus disease (COVID-19) in Colombia\n",
      "[ Score = 0.4138385 ] Does COVID-19 Spread Through Droplets Alone?\n",
      "[ Score = 0.40093565 ] Food Delivery Service During Social Distancing: Proactively Preventing or Potentially Spreading Coronavirus Disease–2019?\n",
      "##############\n",
      "QUERY NUMBER  14\n",
      "tfidf TfidfModel(num_docs=996, num_nnz=80545)\n",
      "QUERY: coronavirus super spreaders\n",
      "[ Score = 0.56168115 ] Clinical and Epidemiologic Characteristics of Spreaders of Middle East Respiratory Syndrome Coronavirus during the 2015 Outbreak in Korea\n",
      "[ Score = 0.50814426 ] MERS, SARS, and Ebola: The Role of Super-Spreaders in Infectious Disease\n",
      "[ Score = 0.4927085 ] Spatial super-spreaders and super-susceptibles in human movement networks\n",
      "[ Score = 0.48921746 ] A super-spreader of COVID-19 in Ningbo city in China\n",
      "[ Score = 0.48541227 ] Chest tube with air leaks is a potential “super spreader” of COVID-19\n",
      "[ Score = 0.43416592 ] Pausing super spreader events for COVID-19 mitigation: International Hajj pilgrimage cancellation\n",
      "[ Score = 0.36792454 ] Part of the Cure or Spreader of the Disease?\n",
      "[ Score = 0.3452558 ] Super-spreaders in infectious diseases\n",
      "[ Score = 0.32360816 ] COVID-19 Super-spreaders: Definitional Quandaries and Implications\n",
      "[ Score = 0.3078095 ] Characterizing super-spreading in microblog: An epidemic-based information propagation model\n",
      "##############\n",
      "QUERY NUMBER  15\n",
      "tfidf TfidfModel(num_docs=1528, num_nnz=122988)\n",
      "QUERY: coronavirus outside body\n",
      "[ Score = 0.36044386 ] Body Cavity Fluids\n",
      "[ Score = 0.30861253 ] COVID-19 Epidemic Outside China: 34 Founders and Exponential Growth\n",
      "[ Score = 0.2931895 ] Computed Tomography Manifestations of 5 Cases of the Novel Coronavirus Disease 2019 (COVID-19) Pneumonia From Patients Outside Wuhan\n",
      "[ Score = 0.28393114 ] Psoriatic disease and body composition: a systematic review of the literature\n",
      "[ Score = 0.27313435 ] Unraveling the Structure of Viral Replication Complexes at Super-Resolution\n",
      "[ Score = 0.27225575 ] Estimating number of global importations of COVID-19 from Wuhan, risk of transmission outside mainland China and COVID-19 introduction index between countries outside mainland China\n",
      "[ Score = 0.25972745 ] Conceptual Design of a Body Bag for Preventing Infections and Safe Disposal of Deceased from COVID-19 Virus\n",
      "[ Score = 0.24781373 ] Effectiveness of Social Networks for Studying Biological Agents and Identifying Cancer Biomarkers\n",
      "[ Score = 0.23674491 ] Shell disorder analysis predicts greater resilience of the SARS-CoV-2 (COVID-19) outside the body and in body fluids\n",
      "[ Score = 0.22820382 ] Novel coronavirus transmission to water bodies; risk of COVID-19 pneumonia to aquatic mammals\n",
      "##############\n",
      "QUERY NUMBER  16\n",
      "tfidf TfidfModel(num_docs=1247, num_nnz=96743)\n",
      "QUERY: how long does coronavirus survive on surfaces\n",
      "[ Score = 0.4791813 ] Long-Term Outcomes After Open Repair for Ruptured Abdominal Aortic Aneurysm\n",
      "[ Score = 0.4125984 ] Virus survival in evaporated saliva microdroplets deposited on inanimate surfaces\n",
      "[ Score = 0.4090926 ] Declining winter survival and fitness implications associated with latitudinal distribution in Norwegian Greylag Geese Anser anser\n",
      "[ Score = 0.39608777 ] Likelihood of survival of coronavirus disease 2019\n",
      "[ Score = 0.3456021 ] Survival of Hendra Virus in the Environment: Modelling the Effect of Temperature\n",
      "[ Score = 0.34376305 ] Survival of Microorganisms on Inanimate Surfaces\n",
      "[ Score = 0.34263474 ] Effect of thermal control of dry fomites on regulating the survival of human pathogenic bacteria responsible for nosocomial infections\n",
      "[ Score = 0.31749696 ] How long do nosocomial pathogens persist on inanimate surfaces? A systematic review\n",
      "[ Score = 0.3036387 ] Survival and Transport of Enteric Viruses in the Environment\n",
      "[ Score = 0.3036387 ] Environmental survival and microbicide inactivation of coronaviruses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "QUERY NUMBER  17\n",
      "tfidf TfidfModel(num_docs=976, num_nnz=84939)\n",
      "QUERY: coronavirus clinical trials\n",
      "[ Score = 0.64664865 ] Systematic review of the registered clinical trials for coronavirus disease 2019 (COVID-19)\n",
      "[ Score = 0.6327981 ] Systematic Review of the Registered Clinical Trials of Coronavirus Disease 2019 (COVID-19)\n",
      "[ Score = 0.613948 ] HUMAN CORONAVIRUS DATA FROM FOUR CLINICAL TRIALS OF MASKS AND RESPIRATORS\n",
      "[ Score = 0.5923928 ] A Decade On: Systematic Review of ClinicalTrials.gov Infectious Disease Trials, 2007–2017\n",
      "[ Score = 0.5908346 ] The race to find a SARS-CoV-2 drug can only be won by a few chosen drugs: a systematic review of registers of clinical trials of drugs aimed at preventing or treating COVID-19\n",
      "[ Score = 0.57358706 ] Characteristics of COVID-19 Clinical Trials in China Based on the Registration Data on ChiCTR and ClinicalTrials.gov\n",
      "[ Score = 0.57212317 ] A brief review of antiviral drugs evaluated in registered clinical trials for COVID-19\n",
      "[ Score = 0.5672686 ] Characteristics of registered clinical trials assessing treatments for COVID-19: a cross-sectional analysis\n",
      "[ Score = 0.54639 ] A large number of COVID-19 interventional clinical trials were registered soon after the pandemic onset: a descriptive analysis\n",
      "[ Score = 0.5411321 ] Hydroxychloroquine as Prophylaxis for Coronavirus SARS-CoV-2 Infection: Review ff the Ongoing Clinical Trials\n",
      "##############\n",
      "QUERY NUMBER  18\n",
      "tfidf TfidfModel(num_docs=945, num_nnz=72222)\n",
      "QUERY: masks prevent coronavirus\n",
      "[ Score = 0.50676966 ] Coronavirus outbreaks: prevention and management recommendations\n",
      "[ Score = 0.48825055 ] Decontamination of face masks with steam for mask reuse in fighting the pandemic COVID‐19: experimental supports\n",
      "[ Score = 0.4804324 ] Can Masks Be Reused After Hot Water Decontamination During the COVID-19 Pandemic?\n",
      "[ Score = 0.47527155 ] Testing the Efficacy of Homemade Masks: Would They Protect in an Influenza Pandemic?\n",
      "[ Score = 0.47463843 ] Brief research report: Bidirectional impact of imperfect mask use on reproduction number of COVID-19: A next generation matrix approach()\n",
      "[ Score = 0.45542884 ] COVID‐19: Face masks and human‐to‐human transmission\n",
      "[ Score = 0.45192495 ] Wearing masks and the fight against the novel coronavirus (COVID-19)\n",
      "[ Score = 0.4436454 ] Modified N95 Mask Delivers High Inspired Oxygen Concentrations While Effectively Filtering Aerosolized Microparticles\n",
      "[ Score = 0.44133848 ] Protecting healthcare staff from severe acute respiratory syndrome: filtration capacity of multiple surgical masks\n",
      "[ Score = 0.43828636 ] Mask is the possible key for self‐isolation in COVID‐19 pandemic\n",
      "##############\n",
      "QUERY NUMBER  19\n",
      "tfidf TfidfModel(num_docs=1128, num_nnz=82823)\n",
      "QUERY: what alcohol sanitizer kills coronavirus\n",
      "[ Score = 0.5094698 ] How and who does SARS kill?\n",
      "[ Score = 0.35837138 ] The pandemic of COVID-19 and its implications for the purity and authenticity of alcohol-based hand sanitizers: The health risks associated with falsified sanitizers and recommendations for regulatory and public health bodies\n",
      "[ Score = 0.34549317 ] Alcohol and COVID-19\n",
      "[ Score = 0.32243377 ] Translational Studies of Alcoholism: Bridging the Gap\n",
      "[ Score = 0.30525503 ] Hidden threat lurking behind the alcohol sanitizers in COVID‐19 outbreak\n",
      "[ Score = 0.28990006 ] Hand Sanitizers: A Review on Formulation Aspects, Adverse Effects, and Regulations\n",
      "[ Score = 0.28575557 ] COVID-19 and forced alcohol abstinence in India: The dilemmas around ethics and rights\n",
      "[ Score = 0.280971 ] COVID-19 and frequent use of hand sanitizers; human health and environmental hazards by exposure pathways\n",
      "[ Score = 0.26322702 ] Is another public health crisis brewing beneath the COVID-19 pandemic?\n",
      "[ Score = 0.2601713 ] Hand Sanitizers: A Review of Ingredients, Mechanisms of Action, Modes of Delivery, and Efficacy Against Coronaviruses\n",
      "##############\n",
      "QUERY NUMBER  20\n",
      "tfidf TfidfModel(num_docs=869, num_nnz=62552)\n",
      "QUERY: coronavirus and ACE inhibitors\n",
      "[ Score = 0.6919658 ] Rethinking the Renin-Angiotensin System and Its Role in Cardiovascular Regulation\n",
      "[ Score = 0.58897984 ] ACE inhibitors and COVID‐19: We don't know yet\n",
      "[ Score = 0.5758041 ] Ace revisited: A new target for structure-based drug design\n",
      "[ Score = 0.53947055 ] Structure and Function of Angiotensin Converting Enzyme and Its Inhibitors\n",
      "[ Score = 0.5376626 ] Comment on “ACE inhibitors and COVID‐19: We don't know yet”\n",
      "[ Score = 0.5263963 ] Continue ACE inhibitors/ ARB’S till further evidence in Coronavirus disease 2019 (COVID-19)\n",
      "[ Score = 0.512608 ] ACE for all – a molecular perspective\n",
      "[ Score = 0.4543465 ] Association of Angiotensin-Converting Enzyme Inhibitors and Angiotensin Receptor Blockers with the Risk of Hospitalization and Death in Hypertensive Patients with Coronavirus Disease-19\n",
      "[ Score = 0.45013162 ] Characterization of the first angiotensin-converting like enzyme in bacteria: Ancestor ACE is already active\n",
      "[ Score = 0.44866255 ] Conformational Fingerprinting Using Monoclonal Antibodies (on the Example of Angiotensin I-Converting Enzyme-ACE)\n",
      "##############\n",
      "QUERY NUMBER  21\n",
      "tfidf TfidfModel(num_docs=1258, num_nnz=105916)\n",
      "QUERY: coronavirus mortality\n",
      "[ Score = 0.62785643 ] Hypertension and coronavirus disease 2019 mortality\n",
      "[ Score = 0.57315195 ] Racial Inequalities in Mortality from Coronavirus: The Tip of the Iceberg\n",
      "[ Score = 0.5069239 ] Coronavirus Pandemic as Black Swan Event\n",
      "[ Score = 0.4963641 ] Spatiotemporal evolution of coronavirus disease 2019 mortality in Brazil in 2020\n",
      "[ Score = 0.4963641 ] Coronavirus Disease (Covid-19): What Are We Learning in a Country With High Mortality Rate?\n",
      "[ Score = 0.45913258 ] Genetic evolution analysis of 2019 novel coronavirus and coronavirus from other species\n",
      "[ Score = 0.45442343 ] Un nuevo coronavirus emerge\n",
      "[ Score = 0.45442343 ] Novel coronavirus (2019-nCoV): Update on 3rd Coronavirus Outbreak of 21st Century\n",
      "[ Score = 0.45442343 ] Are we ready for the new coronavirus?\n",
      "[ Score = 0.4304319 ] Advances in the Relationship Between Coronavirus Infection and Cardiovascular Diseases\n",
      "##############\n",
      "QUERY NUMBER  22\n",
      "tfidf TfidfModel(num_docs=981, num_nnz=76176)\n",
      "QUERY: coronavirus heart impacts\n",
      "[ Score = 0.6660433 ] Toward understanding the 2019 Coronavirus and its impact on the heart\n",
      "[ Score = 0.50698906 ] The Heart in the Time of the ‘Coronavirus’\n",
      "[ Score = 0.41878176 ] At the heart of COVID-19\n",
      "[ Score = 0.39483124 ] ACE Inhibition in Heart Failure and Ischaemic Heart Disease\n",
      "[ Score = 0.39278796 ] Coronavirus disease 2019 and the cardiovascular system: Impacts and implications\n",
      "[ Score = 0.39013672 ] Regression Approach for Modeling COVID-19 Spread and its Impact On Stock Market\n",
      "[ Score = 0.38843608 ] The impact of the COVID-19 pandemic and Italian lockdown measures on clinical presentation and management of acute heart failure\n",
      "[ Score = 0.37666762 ] COVID-19: the gendered impacts of the outbreak\n",
      "[ Score = 0.36741948 ] The impact of Coronavirus (COVID-19) on head and neck cancer patients’ care\n",
      "[ Score = 0.35098785 ] Region‐resolved proteomics profiling of monkey heart\n",
      "##############\n",
      "QUERY NUMBER  23\n",
      "tfidf TfidfModel(num_docs=963, num_nnz=72675)\n",
      "QUERY: coronavirus hypertension\n",
      "[ Score = 0.61914194 ] Hypertension and coronavirus disease 2019 mortality\n",
      "[ Score = 0.59152627 ] Hypertension and COVID-19\n",
      "[ Score = 0.59152627 ] COVID-19 and hypertension\n",
      "[ Score = 0.57345665 ] A new normal for hypertension medicine with coronavirus disease-2019 (COVID-19): proposal from the president of the Japanese Society of Hypertension\n",
      "[ Score = 0.5651967 ] Hypertension and coronavirus disease 2019: what do we really know?\n",
      "[ Score = 0.48975614 ] COVID-19 patients with hypertension have more severe disease: a multicenter retrospective observational study\n",
      "[ Score = 0.4551022 ] Is Hypertension a Real Risk Factor for Poor Prognosis in the COVID-19 Pandemic?\n",
      "[ Score = 0.44812915 ] Understanding the current status of patients with pulmonary hypertension during COVID-19 outbreak: a small-scale national survey from China\n",
      "[ Score = 0.4409052 ] Clinical characteristics of coronavirus disease 2019 (COVID-19) patients with hypertension on renin–angiotensin system inhibitors\n",
      "[ Score = 0.4368711 ] Association of hypertension with the severity and fatality of SARS-CoV-2 infection: A meta-analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "QUERY NUMBER  24\n",
      "tfidf TfidfModel(num_docs=918, num_nnz=69491)\n",
      "QUERY: coronavirus diabetes\n",
      "[ Score = 0.81461936 ] Coronavirus and diabetes: an update\n",
      "[ Score = 0.5771639 ] Practical recommendations for the management of diabetes in patients with COVID-19\n",
      "[ Score = 0.5562721 ] Diabetes management and specific considerations for patients with diabetes during coronavirus diseases pandemic: A scoping review\n",
      "[ Score = 0.542721 ] Diabetes and COVID‐19: psychosocial consequences of the COVID‐19 pandemic in people with diabetes in Denmark—what characterizes people with high levels of COVID‐19‐related worries?\n",
      "[ Score = 0.5393326 ] Coronavirus Disease 2019 and Diabetes: The Epidemic and the Korean Diabetes Association Perspective\n",
      "[ Score = 0.53273576 ] COVID‐19 and diabetes\n",
      "[ Score = 0.52262604 ] The burden of type 2 diabetes: are we doing enough?\n",
      "[ Score = 0.4981807 ] Diabetes and Novel Coronavirus Infection: Implications for Treatment\n",
      "[ Score = 0.49595004 ] The double burden of diabetes and global infection in low and middle-income countries\n",
      "[ Score = 0.49556434 ] The Challenges of COVID-19 for People Living With Diabetes: Considerations for Digital Health\n",
      "##############\n",
      "QUERY NUMBER  25\n",
      "tfidf TfidfModel(num_docs=1201, num_nnz=106572)\n",
      "QUERY: coronavirus biomarkers\n",
      "[ Score = 0.5899308 ] Biomarkers of sepsis: time for a reappraisal\n",
      "[ Score = 0.5455061 ] IDBD: Infectious Disease Biomarker Database\n",
      "[ Score = 0.5104973 ] The Acute Respiratory Distress Syndrome Biomarker Pipeline: Crippling Gaps between Discovery and Clinical Utility\n",
      "[ Score = 0.49897087 ] Biomarkers of Pulmonary Diseases\n",
      "[ Score = 0.46690175 ] The discovery and identification of a candidate proteomic biomarker of active tuberculosis\n",
      "[ Score = 0.4625584 ] Systems Biology and Biomarker Discovery\n",
      "[ Score = 0.43960714 ] Proteomic analysis of serum, plasma, and lymph for the identification of biomarkers\n",
      "[ Score = 0.4295261 ] Chapter 1 Introduction\n",
      "[ Score = 0.42571932 ] Research Advances in Biomarker for Sepsis\n",
      "[ Score = 0.4137248 ] Interleukin-6 as a potential biomarker of COVID-19 progression\n",
      "##############\n",
      "QUERY NUMBER  26\n",
      "tfidf TfidfModel(num_docs=1302, num_nnz=109155)\n",
      "QUERY: coronavirus early symptoms\n",
      "[ Score = 0.5250544 ] The importance of olfactory and gustatory disorders as early symptoms of coronavirus disease (COVID-19)\n",
      "[ Score = 0.47728676 ] GI symptoms as early signs of COVID-19 in hospitalised Italian patients\n",
      "[ Score = 0.414725 ] Pulmonary Pathology of Early Phase 2019 Novel Coronavirus Pneumonia\n",
      "[ Score = 0.414725 ] Chest computed tomography images of early coronavirus disease (COVID-19)\n",
      "[ Score = 0.39100653 ] Uncertainty in using chest computed tomography in early coronavirus disease (COVID-19)\n",
      "[ Score = 0.38616303 ] The Impact of Host-Based Early Warning on Disease Outbreaks\n",
      "[ Score = 0.38175094 ] EARLY DETECTION OF COVID-19\n",
      "[ Score = 0.37391353 ] An early test and treat strategy for SARS-CoV-2\n",
      "[ Score = 0.37094134 ] In reply: Uncertainty in using chest computed tomography in early coronavirus disease (COVID-19)\n",
      "[ Score = 0.36471152 ] Case Report on Early Diagnosis of COVID-19\n",
      "##############\n",
      "QUERY NUMBER  27\n",
      "tfidf TfidfModel(num_docs=1136, num_nnz=91668)\n",
      "QUERY: coronavirus asymptomatic\n",
      "[ Score = 0.56392044 ] Asymptomatic Patients with Novel Coronavirus Disease (COVID-19)\n",
      "[ Score = 0.56392044 ] A confirmed asymptomatic carrier of 2019 novel coronavirus\n",
      "[ Score = 0.5220889 ] Estimation of the asymptomatic ratio of novel coronavirus infections (COVID-19)\n",
      "[ Score = 0.495297 ] Comparison of transmissibility of coronavirus between symptomatic and asymptomatic patients: Reanalysis of the Ningbo Covid-19 data\n",
      "[ Score = 0.4883694 ] Asymptomatic Severe Acute Respiratory Syndrome–associated Coronavirus Infection\n",
      "[ Score = 0.48629355 ] An Asymptomatic Patient with COVID-19\n",
      "[ Score = 0.47076467 ] The time scale of asymptomatic transmission affects estimates of epidemic potential in the COVID-19 outbreak\n",
      "[ Score = 0.47076467 ] The time scale of asymptomatic transmission affects estimates of epidemic potential in the COVID-19 outbreak\n",
      "[ Score = 0.46043912 ] Identification and management of asymptomatic carriers of coronavirus disease 2019 (COVID‐19) in China\n",
      "[ Score = 0.45694232 ] Asymptomatic cases with SARS‐CoV‐2 infection\n",
      "##############\n",
      "QUERY NUMBER  28\n",
      "tfidf TfidfModel(num_docs=754, num_nnz=56175)\n",
      "QUERY: coronavirus hydroxychloroquine\n",
      "[ Score = 0.59140515 ] A Case of Breakthrough COVID-19 during Hydroxychloroquine Maintenance\n",
      "[ Score = 0.5721328 ] Hydroxychloroquine for Coronavirus: The Urgent Need for a Moratorium on Prescriptions\n",
      "[ Score = 0.56254876 ] Hydroxychloroquine shortage\n",
      "[ Score = 0.52969205 ] Aminoquinolines against coronavirus disease 2019 (COVID-19): chloroquine or hydroxychloroquine\n",
      "[ Score = 0.5246983 ] Coronavirus, humanpathogenes\n",
      "[ Score = 0.47729427 ] Similar incidence of Coronavirus Disease 2019 (COVID-19) in patients with rheumatic diseases with and without hydroxychloroquine therapy\n",
      "[ Score = 0.46714455 ] Could Chloroquine /Hydroxychloroquine Be Harmful in Coronavirus Disease 2019 (COVID-19) Treatment?\n",
      "[ Score = 0.46714455 ] The Role of Hydroxychloroquine in Coronavirus Disease 2019. A Versatile Tool at the Service of Humanity\n",
      "[ Score = 0.4593191 ] Chloroquine/Hydroxychloroquine Overdose\n",
      "[ Score = 0.42491168 ] Hydroxychloroquine in patients with mainly mild to moderate coronavirus disease 2019: open label, randomised controlled trial\n",
      "##############\n",
      "QUERY NUMBER  29\n",
      "tfidf TfidfModel(num_docs=946, num_nnz=79690)\n",
      "QUERY: coronavirus drug repurposing\n",
      "[ Score = 0.49509913 ] Boosting the arsenal against COVID-19 through computational drug repurposing\n",
      "[ Score = 0.49509913 ] Repurposing an HIV Drug for Zika Virus Therapy\n",
      "[ Score = 0.46455812 ] Drug repurposing prediction for COVID-19 using probabilistic networks and crowdsourced curation\n",
      "[ Score = 0.46073478 ] Drug Repurposing to Fight Colistin and Carbapenem-Resistant Bacteria\n",
      "[ Score = 0.45837268 ] Shotgun drug repurposing biotechnology to tackle epidemics and pandemics\n",
      "[ Score = 0.44596377 ] Pharmacodynamics and Systems Pharmacology Approaches to Repurposing Drugs in the Wake of Global Health Burden\n",
      "[ Score = 0.43635693 ] Drug repurposing for breast cancer therapy: Old weapon for new battle\n",
      "[ Score = 0.4349326 ] Drug Repurposing for Viral Infectious Diseases: How Far Are We?\n",
      "[ Score = 0.42876837 ] News Feature: To counter the pandemic, clinicians bank on repurposed drugs\n",
      "[ Score = 0.42876837 ] Drug repurposing in the era of COVID‐19: a call for leadership and government investment\n",
      "##############\n",
      "QUERY NUMBER  30\n",
      "tfidf TfidfModel(num_docs=757, num_nnz=63584)\n",
      "QUERY: coronavirus remdesivir\n",
      "[ Score = 0.49868464 ] Remdesivir for Treatment of COVID-19: Combination of Pulmonary and IV Administration May Offer Aditional Benefit\n",
      "[ Score = 0.4968394 ] The use of convalescent plasma therapy and remdesivir in the successful management of a critically ill obstetric patient with novel coronavirus 2019 infection: A case report\n",
      "[ Score = 0.4792786 ] Remdesivir (GS-5734) Impedes Enterovirus Replication Through Viral RNA Synthesis Inhibition\n",
      "[ Score = 0.47173044 ] Remdesivir inhibits renal fibrosis in obstructed kidneys\n",
      "[ Score = 0.47173044 ] Uncertainty about the Efficacy of Remdesivir on COVID-19\n",
      "[ Score = 0.47173044 ] Remdesivir — An Important First Step\n",
      "[ Score = 0.4571083 ] Development and validation of a UHPLC-MS/MS method for quantification of the prodrug remdesivir and its metabolite GS-441524: a tool for clinical pharmacokinetics of SARS-CoV-2/COVID-19 and Ebola virus disease\n",
      "[ Score = 0.42823264 ] Remdesivir: A Review of Its Discovery and Development Leading to Emergency Use Authorization for Treatment of COVID-19\n",
      "[ Score = 0.42459056 ] That Escalated Quickly: Remdesivir's Place in Therapy for COVID-19\n",
      "[ Score = 0.42192852 ] Remdesivir for COVID-19: challenges of underpowered studies\n",
      "##############\n",
      "QUERY NUMBER  31\n",
      "tfidf TfidfModel(num_docs=1306, num_nnz=112283)\n",
      "QUERY: difference between coronavirus and flu\n",
      "[ Score = 0.45056772 ] The complex associations of climate variability with seasonal influenza A and B virus transmission in subtropical Shanghai, China\n",
      "[ Score = 0.44345534 ] Influenza species and subtypes circulation among hospitalized patients in Laleh hospital during two influenza seasonal (2016-2017 and 2017-2018) using a multiplex Real Time-Polymerase Chain Reaction\n",
      "[ Score = 0.41835737 ] Difference in immune response in vaccinated and unvaccinated Swedish individuals after the 2009 influenza pandemic\n",
      "[ Score = 0.38594687 ] Initial psychological responses to Influenza A, H1N1 (\"Swine flu\")\n",
      "[ Score = 0.37538373 ] Pollen Explains Flu-Like and COVID-19 Seasonality\n",
      "[ Score = 0.37312737 ] The invention of the swine-flu pandemic\n",
      "[ Score = 0.36573797 ] Waiting for the Flu: Cognitive Inertia and the Spanish Influenza Pandemic of 1918–19\n",
      "[ Score = 0.35178787 ] UK newspapers' representations of the 2009–10 outbreak of swine flu: one health scare not over-hyped by the media?\n",
      "[ Score = 0.35011095 ] Preventive behaviors, beliefs, and anxieties in relation to the swine flu outbreak among college students aged 18–24 years\n",
      "[ Score = 0.3345468 ] An econometric analysis of SARS and Avian Flu on international tourist arrivals to Asia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "QUERY NUMBER  32\n",
      "tfidf TfidfModel(num_docs=1220, num_nnz=100958)\n",
      "QUERY: coronavirus subtypes\n",
      "[ Score = 0.5011336 ] SFJ: An Implementation of Semantic Featherweight Java\n",
      "[ Score = 0.4670424 ] In-Vitro Subtype-Specific Modulation of HIV-1 Trans-Activator of Transcription (Tat) on RNAi Silencing Suppressor Activity and Cell Death\n",
      "[ Score = 0.43890134 ] Genotypes and subtypes of Cryptosporidium spp. in neonatal calves in Northern Ireland\n",
      "[ Score = 0.42300546 ] Pathogenicity of three genetically diverse strains of PRRSV Type 1 in specific pathogen free pigs\n",
      "[ Score = 0.38286084 ] Dating the time of viral subtype divergence\n",
      "[ Score = 0.3757472 ] An unusual cluster of HIV-1 B/F recombinants in an Asian population\n",
      "[ Score = 0.37399027 ] A Human Monoclonal Antibody with Neutralizing Activity against Highly Divergent Influenza Subtypes\n",
      "[ Score = 0.35409623 ] Molecular characterization of Cryptosporidium isolates from diarrheal dairy calves in France\n",
      "[ Score = 0.3523856 ] Infections with Influenza A/H3N2/ Subtype in Poland in the 2016/2017 Epidemic Season\n",
      "[ Score = 0.34198546 ] Characterization of HIV-1 subtypes and drug resistance mutations in Henan Province, China (2017–2019)\n",
      "##############\n",
      "QUERY NUMBER  33\n",
      "tfidf TfidfModel(num_docs=991, num_nnz=85769)\n",
      "QUERY: coronavirus vaccine candidates\n",
      "[ Score = 0.39074874 ] Universal coronavirus vaccines: the time to start is now\n",
      "[ Score = 0.37850595 ] MERS Vaccine Candidate Offers Promise, but Questions Remain\n",
      "[ Score = 0.37391847 ] Towards the design of multiepitope-based peptide vaccine candidate against SARS-CoV-2\n",
      "[ Score = 0.36417878 ] Chapter 3 SARS coronavirus infections of the lower respiratory tract and their prevention\n",
      "[ Score = 0.3540451 ] Vaccines for SARS-CoV-2: Lessons from Other Coronavirus Strains\n",
      "[ Score = 0.35166636 ] A candidate multi-epitope vaccine against SARS-CoV-2\n",
      "[ Score = 0.34743336 ] A SARS-CoV-2 vaccine candidate: In silico cloning and validation\n",
      "[ Score = 0.34299126 ] Animal models and antibody assays for evaluating candidate SARS vaccines: Summary of a technical meeting 25–26 August 2005, London, UK\n",
      "[ Score = 0.3327402 ] A Scalable Topical Vectored Vaccine Candidate Against SARS-CoV-2\n",
      "[ Score = 0.33185905 ] Middle East Respiratory Syndrome Vaccine Candidates: Cautious Optimism\n",
      "##############\n",
      "QUERY NUMBER  34\n",
      "tfidf TfidfModel(num_docs=1374, num_nnz=114575)\n",
      "QUERY: coronavirus recovery\n",
      "[ Score = 0.49095827 ] ANALYSIS OF CLINICAL RECOVERY-PERIOD AND RECOVERY RATE ESTIMATION OF THE FIRST 1000 COVID-19 PATIENTS IN SINGAPORE\n",
      "[ Score = 0.4828099 ] Factors associated with recovery delay in a sample of patients diagnosed by MERS‐CoV rRT‐PCR: A Saudi Arabian multicenter retrospective study\n",
      "[ Score = 0.45099595 ] COVID-19 and SARS Coronavirus 2: Antibodies for the Immediate Rescue and Recovery Phase\n",
      "[ Score = 0.39618126 ] Chapter 9 Recovery from Infection\n",
      "[ Score = 0.385019 ] Anxiety persists after recovery from acquired COVID-19 in anaesthesiologists\n",
      "[ Score = 0.385019 ] Recurrence of COVID-19 after recovery: a case report from Italy\n",
      "[ Score = 0.3564583 ] Flattening the Disability Curve: Rehabilitation and Recovery after COVID-19 Infection\n",
      "[ Score = 0.35137305 ] Acute respiratory failure and the kinetics of neutrophil recovery in pediatric hematopoietic cell transplantation: a multicenter study\n",
      "[ Score = 0.3304984 ] COVID-19 pandemic and its recovery time of patients in India: A pilot study\n",
      "[ Score = 0.3296732 ] Evaluation of eluents for the recovery of an enveloped virus from hands by whole-hand sampling\n",
      "##############\n",
      "QUERY NUMBER  35\n",
      "tfidf TfidfModel(num_docs=1054, num_nnz=93344)\n",
      "QUERY: coronavirus public datasets\n",
      "[ Score = 0.4508176 ] Google Dataset Search by the Numbers\n",
      "[ Score = 0.35789102 ] The Time for Universal Masking of the Public for Coronavirus Disease 2019 Is Now\n",
      "[ Score = 0.35444948 ] Lest We Forget: A Dataset of Coronavirus-Related News Headlines in Swiss Media\n",
      "[ Score = 0.34915662 ] Dataset for country profile and mobility analysis in the assessment of COVID-19 pandemic\n",
      "[ Score = 0.34740645 ] Leveraging Schema Labels to Enhance Dataset Search\n",
      "[ Score = 0.32252288 ] Dutch General Public Reaction on Governmental COVID-19 Measures and Announcements in Twitter Data\n",
      "[ Score = 0.32213184 ] Rapidly Bootstrapping a Question Answering Dataset for COVID-19\n",
      "[ Score = 0.31923187 ] Indian Publications on SARS-CoV-2: A Bibliometric Study of WHO COVID-19 Database\n",
      "[ Score = 0.31871054 ] Dataset on dynamics of Coronavirus on Twitter\n",
      "[ Score = 0.31762627 ] MosMedData: Chest CT Scans With COVID-19 Related Findings Dataset\n",
      "##############\n",
      "QUERY NUMBER  36\n",
      "tfidf TfidfModel(num_docs=898, num_nnz=74268)\n",
      "QUERY: SARS-CoV-2 spike structure\n",
      "[ Score = 0.5853478 ] CoV3D: A database of high resolution coronavirus protein structures\n",
      "[ Score = 0.5838694 ] Structure-based Design of Prefusion-stabilized SARS-CoV-2 Spikes\n",
      "[ Score = 0.58326465 ] Crystal structure of Nsp15 endoribonuclease NendoU from SARS‐CoV‐2\n",
      "[ Score = 0.57702667 ] Structural Basis of SARS-CoV-2 Spike Protein Priming by TMPRSS2\n",
      "[ Score = 0.5690726 ] Crystal structure of Nsp15 endoribonuclease NendoU from SARS-CoV-2\n",
      "[ Score = 0.56457853 ] Comprehensive in-vivo secondary structure of the SARS-CoV-2 genome reveals novel regulatory motifs and mechanisms\n",
      "[ Score = 0.5542449 ] A spike with which to beat COVID-19?\n",
      "[ Score = 0.54834545 ] Cryo-EM structure of infectious bronchitis coronavirus spike protein reveals structural and functional evolution of coronavirus spike proteins\n",
      "[ Score = 0.52458006 ] Structures of human antibodies bound to SARS-CoV-2 spike reveal common epitopes and recurrent features of antibodies\n",
      "[ Score = 0.5239977 ] Structural basis of SARS-CoV-2 spike protein induced by ACE2\n",
      "##############\n",
      "QUERY NUMBER  37\n",
      "tfidf TfidfModel(num_docs=892, num_nnz=76101)\n",
      "QUERY: SARS-CoV-2 phylogenetic analysis\n",
      "[ Score = 0.57141256 ] Median-joining network analysis of SARS-CoV-2 genomes is neither phylogenetic nor evolutionary\n",
      "[ Score = 0.521626 ] Reply to Sánchez-Pacheco et al., Chookajorn, and Mavian et al.: Explaining phylogenetic network analysis of SARS-CoV-2 genomes\n",
      "[ Score = 0.49239445 ] Phylogenetic analysis of the full‐length SARS‐CoV sequences: Evidence for phylogenetic discordance in three genomic regions\n",
      "[ Score = 0.45867068 ] The Role of Phylogenetic Analysis in Clarifying the Infection Source of a COVID-19 Patient\n",
      "[ Score = 0.4437982 ] The Novel Coronavirus SARS‐CoV‐2: From a Zoonotic Infection to Coronavirus Disease‐19 (COVID19)\n",
      "[ Score = 0.43957505 ] Genomic characterization and phylogenetic analysis of SARS‐COV‐2 in Italy\n",
      "[ Score = 0.42021617 ] Phylogenetic analysis of SARS-CoV-2 genomes in Turkey\n",
      "[ Score = 0.41714215 ] Phylogenetic Analysis of SARS-CoV-2 Genomes in Turkey\n",
      "[ Score = 0.4136725 ] Insights into molecular evolution recombination of pandemic SARS-CoV-2 using Saudi Arabian sequences\n",
      "[ Score = 0.4106352 ] Phylogenetics Algorithms and Applications\n",
      "##############\n",
      "QUERY NUMBER  38\n",
      "tfidf TfidfModel(num_docs=1313, num_nnz=106917)\n",
      "QUERY: COVID inflammatory response\n",
      "[ Score = 0.6048243 ] Models of cytokine dynamics in the inflammatory response of viral zoonotic infectious diseases\n",
      "[ Score = 0.5461073 ] Predictive factors of poor outcomes in the COVID-19 epidemic: Consider the inflammatory response\n",
      "[ Score = 0.48091006 ] Pathogenesis of virus-induced immune-mediated demyelination\n",
      "[ Score = 0.47711432 ] A Dynamic Immune Response Shapes COVID-19 Progression\n",
      "[ Score = 0.4729428 ] Inflammatory Response Cells During Acute Respiratory Distress Syndrome in Patients With Coronavirus Disease 2019 (COVID-19)\n",
      "[ Score = 0.4671817 ] Distinct features of SARS-CoV-2-specific IgA response in COVID-19 patients\n",
      "[ Score = 0.44932097 ] COVID-19 infection: the perspectives on immune responses\n",
      "[ Score = 0.44831064 ] Pathogenic T cells and inflammatory monocytes incite inflammatory storm in severe COVID-19 patients\n",
      "[ Score = 0.44524163 ] Imbalanced Host Response to SARS-CoV-2 Drives Development of COVID-19\n",
      "[ Score = 0.4434511 ] Glycyrrhizin inhibits the manifestations of anti-inflammatory responses that appear in association with systemic inflammatory response syndrome (SIRS)-like reactions\n",
      "##############\n",
      "QUERY NUMBER  39\n",
      "tfidf TfidfModel(num_docs=760, num_nnz=57198)\n",
      "QUERY: COVID-19 cytokine storm\n",
      "[ Score = 0.78907824 ] THE “PERFECT CYTOKINE STORM” OF COVID-19\n",
      "[ Score = 0.705773 ] Can tocilizumab calm the cytokine storm of COVID-19?\n",
      "[ Score = 0.705773 ] Cytokine storm syndrome in severe COVID-19\n",
      "[ Score = 0.705773 ] In Reply–The “Perfect Cytokine Storm” of COVID-19\n",
      "[ Score = 0.705773 ] A glimpse into the eye of the COVID-19 cytokine storm\n",
      "[ Score = 0.705773 ] Targeting inflammation and cytokine storm in COVID-19\n",
      "[ Score = 0.705773 ] COVID-19 Cytokine Storm and Novel Truth\n",
      "[ Score = 0.6861925 ] Cytokine storms in infectious diseases\n",
      "[ Score = 0.67486656 ] The pathogenesis and treatment of the `Cytokine Storm' in COVID-19\n",
      "[ Score = 0.6540823 ] A Mathematical Model of Cytokine Dynamics During a Cytokine Storm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "QUERY NUMBER  40\n",
      "tfidf TfidfModel(num_docs=916, num_nnz=76933)\n",
      "QUERY: coronavirus mutations\n",
      "[ Score = 0.5320487 ] Functional analysis of the stem loop S3 and S4 structures in the coronavirus 3′UTR\n",
      "[ Score = 0.47644278 ] RdRp mutations are associated with SARS-CoV-2 genome evolution\n",
      "[ Score = 0.4744976 ] Time Series Prediction of COVID-19 by Mutation Rate Analysis using Recurrent Neural Network-based LSTM Model\n",
      "[ Score = 0.4637258 ] Recurrent mutations associated with isolation and passage of SARS coronavirus in cells from non‐human primates\n",
      "[ Score = 0.45029032 ] Genome Sequencing of a Severe Acute Respiratory Syndrome Coronavirus 2 Isolate Obtained from a South African Patient with Coronavirus Disease 2019\n",
      "[ Score = 0.44091716 ] Prediction of mutations engineered by randomness in H5N1 hemagglutinins of influenza A virus\n",
      "[ Score = 0.43526208 ] Prediction of mutations engineered by randomness in H5N1 neuraminidases from influenza A virus\n",
      "[ Score = 0.43191063 ] Prediction of amino acid pairs sensitive to mutations in the spike protein from SARS related coronavirus\n",
      "[ Score = 0.42496598 ] The Mutational Robustness of Influenza A Virus\n",
      "[ Score = 0.413365 ] Interpatient mutational spectrum of human coronavirus‐OC43 revealed by illumina sequencing\n",
      "##############\n",
      "QUERY NUMBER  41\n",
      "tfidf TfidfModel(num_docs=671, num_nnz=56850)\n",
      "QUERY: COVID-19 in African-Americans\n",
      "[ Score = 0.62103575 ] Racial Disparity of Coronavirus Disease 2019 (COVID-19) in African American Communities\n",
      "[ Score = 0.6060339 ] COVID-19 is Out of Proportion in African Americans. This Will Come as No Surprise…\n",
      "[ Score = 0.6060339 ] African American COVID-19 Mortality: A Sentinel Event\n",
      "[ Score = 0.6060339 ] Being African American and Rural: A Double Jeopardy from Covid‐19\n",
      "[ Score = 0.58148277 ] Disproportionate COVID-19 Related Mortality Amongst African Americans in Four Southern States in the United States\n",
      "[ Score = 0.49482465 ] Genetic Susceptibility for COVID-19-Associated Sudden Cardiac Death in African Americans\n",
      "[ Score = 0.4710968 ] Are Clinicians Contributing to Excess African American COVID-19 Deaths? Unbeknownst to Them, They May Be\n",
      "[ Score = 0.46297157 ] Richard Allen Williams, M.D.: a Career Fighting Disparities and Fostering Equity\n",
      "[ Score = 0.4552909 ] Social Equity and COVID‐19: The Case of African Americans\n",
      "[ Score = 0.44551882 ] Differential expression of COVID-19-related genes in European Americans and African Americans\n",
      "##############\n",
      "QUERY NUMBER  42\n",
      "tfidf TfidfModel(num_docs=463, num_nnz=35201)\n",
      "QUERY: Vitamin D and COVID-19\n",
      "[ Score = 0.89789337 ] Vitamin D and COVID-19\n",
      "[ Score = 0.6690532 ] Vitamin C and Coronavirus\n",
      "[ Score = 0.6690532 ] Vitamin D and Coronavirus\n",
      "[ Score = 0.6624614 ] Vitamin D: Photobiological and Ecological Aspects\n",
      "[ Score = 0.65838796 ] Vitamin D deficiency and COVID-19 pandemic\n",
      "[ Score = 0.6441855 ] Perspective: Vitamin D deficiency and COVID‐19 severity – plausibly linked by latitude, ethnicity, impacts on cytokines, ACE2, and thrombosis (R1)\n",
      "[ Score = 0.63490653 ] Vitamin C as a Possible Therapy for COVID-19\n",
      "[ Score = 0.63490653 ] Vitamin D supplementation in the COVID-19 pandemic\n",
      "[ Score = 0.6214815 ] Supplementation of Vitamin C to Weaner Diets Increases IgM Concentration and Improves the Biological Activity of Vitamin E in Alveolar Macrophages\n",
      "[ Score = 0.5965958 ] Vitamin D deficiency 2.0: an update on the current status worldwide\n",
      "##############\n",
      "QUERY NUMBER  43\n",
      "tfidf TfidfModel(num_docs=503, num_nnz=36463)\n",
      "QUERY: violence during pandemic\n",
      "[ Score = 0.5421012 ] Trauma Does not Quarantine: Violence During the COVID-19 Pandemic\n",
      "[ Score = 0.4948685 ] Alarming trends in US domestic violence during the COVID-19 pandemic\n",
      "[ Score = 0.4851549 ] COVID-19 and Domestic Violence against Women\n",
      "[ Score = 0.45416918 ] Domestic violence against women and the COVID-19 pandemic: What is the role of psychiatry?\n",
      "[ Score = 0.4421325 ] Pylons ablaze: Examining the role of 5G COVID‐19 conspiracy beliefs and support for violence\n",
      "[ Score = 0.4341789 ] The pandemic paradox: The consequences of COVID‐19 on domestic violence\n",
      "[ Score = 0.4303642 ] DANGER IN DANGER: INTERPERSONAL VIOLENCE DURING COVID-19 QUARANTINE\n",
      "[ Score = 0.42856866 ] Structural violence in the era of a new pandemic: the case of the Gaza Strip\n",
      "[ Score = 0.42856866 ] The Urgent Need to Address Violence Against Health Workers During the COVID-19 Pandemic\n",
      "[ Score = 0.41177815 ] An increasing risk of family violence during the Covid-19 pandemic: Strengthening community collaborations to save lives\n",
      "##############\n",
      "QUERY NUMBER  44\n",
      "tfidf TfidfModel(num_docs=796, num_nnz=65438)\n",
      "QUERY: impact of masks on coronavirus transmission\n",
      "[ Score = 0.4374778 ] Do Humidity and Temperature Impact the Spread of the Novel Coronavirus?\n",
      "[ Score = 0.4359643 ] Mask use during COVID-19: A risk adjusted strategy()\n",
      "[ Score = 0.41979128 ] COVID‐19: Face masks and human‐to‐human transmission\n",
      "[ Score = 0.41842824 ] Impact of contact tracing on SARS-CoV-2 transmission\n",
      "[ Score = 0.41574037 ] HUMAN CORONAVIRUS DATA FROM FOUR CLINICAL TRIALS OF MASKS AND RESPIRATORS\n",
      "[ Score = 0.40958926 ] Brief research report: Bidirectional impact of imperfect mask use on reproduction number of COVID-19: A next generation matrix approach()\n",
      "[ Score = 0.40502572 ] Coronavirus disease 2019 and the cardiovascular system: Impacts and implications\n",
      "[ Score = 0.40475726 ] The Time for Universal Masking of the Public for Coronavirus Disease 2019 Is Now\n",
      "[ Score = 0.39539427 ] Wearing masks and the fight against the novel coronavirus (COVID-19)\n",
      "[ Score = 0.3844516 ] Decontamination of face masks with steam for mask reuse in fighting the pandemic COVID‐19: experimental supports\n",
      "##############\n",
      "QUERY NUMBER  45\n",
      "tfidf TfidfModel(num_docs=658, num_nnz=48961)\n",
      "QUERY: coronavirus mental health impact\n",
      "[ Score = 0.68021584 ] Impact of coronavirus outbreak on psychological health\n",
      "[ Score = 0.63203865 ] Impact of SARS-CoV-2 (Coronavirus) Pandemic on Public Mental Health\n",
      "[ Score = 0.546931 ] Novel Coronavirus and Related Public Health Interventions Are Negatively Impacting Mental Health Services\n",
      "[ Score = 0.48817083 ] The COVID‐19 pandemic and mental health impacts\n",
      "[ Score = 0.46828997 ] Mental health at the age of coronavirus: time for change\n",
      "[ Score = 0.44563696 ] The Impact of Covid‐19 Pandemic on Elderly Mental Health\n",
      "[ Score = 0.4335522 ] Mental health and a novel coronavirus (2019-nCoV) in China\n",
      "[ Score = 0.4335522 ] Mental health characteristics associated with dysfunctional coronavirus anxiety\n",
      "[ Score = 0.41257963 ] The impact of COVID-19 on mental health in the Hispanic Caribbean region\n",
      "[ Score = 0.41257963 ] The mental health impact of providing spine care during COVID-19\n",
      "##############\n",
      "QUERY NUMBER  46\n",
      "tfidf TfidfModel(num_docs=440, num_nnz=35851)\n",
      "QUERY: dexamethasone coronavirus\n",
      "[ Score = 0.6469625 ] Dexamethasone for pulpitis\n",
      "[ Score = 0.45747158 ] Dexamethasone in community-acquired pneumonia\n",
      "[ Score = 0.45747158 ] The use of dexamethasone in the treatment of COVID-19\n",
      "[ Score = 0.38739142 ] Reducing dexamethasone antiemetic prophylaxis during the COVID-19 pandemic: recommendations from Ontario, Canada\n",
      "[ Score = 0.26593664 ] Multiple Myeloma in the Time of COVID-19\n",
      "[ Score = 0.26147032 ] Effect of Dexamethasone in Hospitalized Patients with COVID-19: Preliminary Report\n",
      "[ Score = 0.26141235 ] Receptor-mediated induction of human dermal fibroblast ectoaminopeptidase N by glucocorticoids\n",
      "[ Score = 0.24622531 ] COVID-19-associated ARDS treated with DEXamethasone (CoDEX): Study design and rationale for a randomized trial.\n",
      "[ Score = 0.20061456 ] CryptoDex: A randomised, double-blind, placebo-controlled phase III trial of adjunctive dexamethasone in HIV-infected adults with cryptococcal meningitis: study protocol for a randomised control trial\n",
      "[ Score = 0.18975037 ] Zika Virus Infection in Dexamethasone-immunosuppressed Mice Demonstrating Disseminated Infection with Multi-organ Involvement Including Orchitis Effectively Treated by Recombinant Type I Interferons\n",
      "##############\n",
      "QUERY NUMBER  47\n",
      "tfidf TfidfModel(num_docs=602, num_nnz=52297)\n",
      "QUERY: COVID-19 outcomes in children\n",
      "[ Score = 0.5741097 ] Children with COVID-19 at a specialist centre: initial experience and outcome\n",
      "[ Score = 0.439395 ] Slums Are Not Places for Children to Live Vulnerabilities, Health Outcomes, and Possible Interventions\n",
      "[ Score = 0.43806428 ] Outcomes of COVID-19: disparities in obesity and by ethnicity/race\n",
      "[ Score = 0.41125745 ] Coronavirus Disease 2019 in Children: Characteristics, Antimicrobial Treatment, and Outcomes\n",
      "[ Score = 0.40992254 ] COVID‐19 in pregnancy: risk of adverse neonatal outcomes\n",
      "[ Score = 0.40970352 ] Authors’ reply to correspondence in response to “Will children reveal their secret? The coronavirus dilemma”\n",
      "[ Score = 0.4055687 ] Three further ways that the COVID-19 pandemic will affect health outcomes\n",
      "[ Score = 0.3749427 ] Clinical Characteristics of Children with COVID-19: A Rapid Review and Meta-Analysis\n",
      "[ Score = 0.36340517 ] Spectrum of COVID‐19 in Children\n",
      "[ Score = 0.36151084 ] Risk factors for a poor outcome among children admitted with clinically severe pneumonia to a university hospital in Rabat, Morocco\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "QUERY NUMBER  48\n",
      "tfidf TfidfModel(num_docs=491, num_nnz=41671)\n",
      "QUERY: school reopening coronavirus\n",
      "[ Score = 0.568277 ] Reopening schools after the COVID-19 lockdown\n",
      "[ Score = 0.5622488 ] The impact of school reopening on the spread of COVID-19 in England\n",
      "[ Score = 0.45147365 ] Expected impact of reopening schools after lockdown on COVID-19 epidemic in Ile-de-France\n",
      "[ Score = 0.4018325 ] Should Schools Reopen Early or Late? – Transmission Dynamics of COVID-19 in Children\n",
      "[ Score = 0.36745095 ] Shut and re-open: the role of schools in the spread of COVID-19 in Europe\n",
      "[ Score = 0.36745095 ] Shut and re-open: the role of schools in the spread of COVID-19 in Europe\n",
      "[ Score = 0.36648026 ] Cost Benefit Analysis of Limited Reopening Relative to a Herd Immunity Strategy or Shelter in Place for SARS-CoV-2 in the United States\n",
      "[ Score = 0.3653023 ] Determining the optimal strategy for reopening schools, work and society in the UK: balancing earlier opening and the impact of test and trace strategies with the risk of occurrence of a secondary COVID-19 pandemic wave\n",
      "[ Score = 0.35896626 ] COVID-19: Social Media Sentiment Analysis on Reopening\n",
      "[ Score = 0.35013863 ] Prevent the resurgence of infectious disease with asymptomatic carriers\n",
      "##############\n",
      "QUERY NUMBER  49\n",
      "tfidf TfidfModel(num_docs=770, num_nnz=65707)\n",
      "QUERY: post-infection COVID-19 immunity\n",
      "[ Score = 0.48782644 ] Post‐Acute Care Preparedness in a COVID‐19 World\n",
      "[ Score = 0.39329267 ] Aetiology of paediatric pneumonia after the introduction of pneumococcal conjugate vaccine\n",
      "[ Score = 0.3577591 ] Post-COVID-19 global health strategies: the need for an interdisciplinary approach\n",
      "[ Score = 0.35708198 ] COVID-19 OUTBREAK IN POST-SOVIET STATES: MODELING THE BEST AND WORST POSSIBLE SCENARIOS\n",
      "[ Score = 0.3443024 ] Growth kinetics of SARS-coronavirus in Vero E6 cells\n",
      "[ Score = 0.30406865 ] COVID-19 infection: the perspectives on immune responses\n",
      "[ Score = 0.29383206 ] In-host Modelling of COVID-19 Kinetics in Humans\n",
      "[ Score = 0.28456187 ] Early phase of the COVID-19 outbreak in Hungary and post-lockdown scenarios\n",
      "[ Score = 0.28054985 ] Impaired production of immune mediators in dengue virus type 2-infected mononuclear cells of adults with end stage renal disease\n",
      "[ Score = 0.27534404 ] Topological data analysis to model the shape of immune responses during co-infections\n",
      "##############\n",
      "QUERY NUMBER  50\n",
      "tfidf TfidfModel(num_docs=581, num_nnz=49112)\n",
      "QUERY: mRNA vaccine coronavirus\n",
      "[ Score = 0.69668716 ] Quantification of individual subgenomic mRNA species during replication of the coronavirus transmissible gastroenteritis virus\n",
      "[ Score = 0.5278736 ] Biomedical applications of mRNA nanomedicine\n",
      "[ Score = 0.5244613 ] Relationship between mRNA stability and intron presence\n",
      "[ Score = 0.52169514 ] Efficient hepatic delivery and protein expression enabled by optimized mRNA and ionizable lipid nanoparticle\n",
      "[ Score = 0.52086395 ] Sequencing of coronavirus IBV genomic RNA: a 195-base open reading frame encoded by mRNA B\n",
      "[ Score = 0.48743844 ] An Evidence Based Perspective on mRNA-SARS-CoV-2 Vaccine Development\n",
      "[ Score = 0.4797001 ] A Comparison of Plasmid DNA and mRNA as Vaccine Technologies\n",
      "[ Score = 0.44194043 ] Changed in translation: mRNA recoding by −1 programmed ribosomal frameshifting\n",
      "[ Score = 0.4321785 ] mRNA Vaccines: Possible Tools to Combat SARS-CoV-2\n",
      "[ Score = 0.42539176 ] The promise of mRNA vaccines: a biotech and industrial perspective\n"
     ]
    }
   ],
   "source": [
    "all_rankings_TFIDF=[]\n",
    "f=open(\"../queries/trecRun_tfidf.txt\",\"w\")\n",
    "for topi in range(len(topics)):\n",
    "    # We select all cord_uid in relevance_data for the query number topi\n",
    "    cords_relevance_data = set()\n",
    "    aux = 0\n",
    "    for i in range(topi+1):\n",
    "        aux += relevance_data.loc[relevance_data.topic_id==(i)].shape[0]\n",
    "    for i in range(relevance_data.loc[relevance_data.topic_id==(topi+1)].shape[0]):\n",
    "        cords_relevance_data.add(relevance_data.loc[relevance_data.topic_id==(topi+1)].cord_uid[aux+i])\n",
    "    # We select all cord_uid that are in both (relevance_data and dt) for the query number topi\n",
    "    dt_rel_data = pd.DataFrame()\n",
    "    aux = 0\n",
    "    for i in range(dt.cord_uid.shape[0]):\n",
    "        if dt.cord_uid[i] in cords_relevance_data:\n",
    "            dt_rel_data[aux] = dt.iloc[i]\n",
    "            aux +=1\n",
    "    dt_rel_data = dt_rel_data.T\n",
    "    # Now we rank for the query number topi\n",
    "    print(\"##############\")\n",
    "    print(\"QUERY NUMBER \", (topi+1))\n",
    "    value = topics[(topi+1)]\n",
    "    rank = launch_query(dt_rel_data, value[\"query\"],(topi+1),f)\n",
    "    all_rankings_TFIDF.append(rank)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/joaopalotti/trectools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trectools in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (0.0.44)\n",
      "Requirement already satisfied: scipy>=0.10.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (1.5.2)\n",
      "Requirement already satisfied: sarge>=0.1.1 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (0.1.6)\n",
      "Requirement already satisfied: scikit-learn>=0.15 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (0.23.2)\n",
      "Requirement already satisfied: pandas>=0.15.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (1.1.3)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (4.6.1)\n",
      "Requirement already satisfied: matplotlib>=1.5 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (3.3.2)\n",
      "Requirement already satisfied: bs4>=0.0.0.1 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (0.0.1)\n",
      "Requirement already satisfied: numpy>=1.0.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from scikit-learn>=0.15->trectools) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from scikit-learn>=0.15->trectools) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from pandas>=0.15.0->trectools) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from pandas>=0.15.0->trectools) (2020.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (8.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from bs4>=0.0.0.1->trectools) (4.9.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.15.0->trectools) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4>=0.0.0.1->trectools) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install trectools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.009, NDCG: 0.038\n",
      "P-value for wrt P@10 between r1 and r2: 0.051\n"
     ]
    }
   ],
   "source": [
    "# TREC_EVAL using document score\n",
    "from trectools import TrecQrel, TrecRun, TrecEval\n",
    "# A typical evaluation workflow\n",
    "\n",
    "# We load a TrecRun object\n",
    "r1 = TrecRun(\"../queries/trecRun_tfidf.txt\")\n",
    "#print(r1.topics()[:5]) # Shows the first 5 topics\n",
    "r2 = TrecRun(\"../queries/trecRun2.txt\") # INVENTADO\n",
    "#print(r2.topics()[:5]) # Shows the first 5 topics\n",
    "\n",
    "\n",
    "# We load a TrecQrel object\n",
    "relevance_data_qrels = \"../queries/relevance_judgements.txt\"\n",
    "qrels = TrecQrel(relevance_data_qrels)\n",
    "\n",
    "te = TrecEval(r1, qrels)\n",
    "map_result = te.get_map(trec_eval = True ) # The result is the same as trec_eval\n",
    "ndcg_result = te.get_ndcg(trec_eval=True)     \n",
    "\n",
    "print(\"MAP: %.3f, NDCG: %.3f\" % (map_result, ndcg_result)) # Solo evalúa los 10 primeros resultados de cada ranking!!!\n",
    "\n",
    "\n",
    "### r2 es inventado en este caso. Esta parte solo lo utilizaremos con este y el siguiente vsm \n",
    "\n",
    "result_r1 = r1.evaluate_run(qrels, per_query=True) \n",
    "result_r2 = r2.evaluate_run(qrels, per_query=True)\n",
    "\n",
    "# Inspect for statistically significant differences between the two runs for  P_10 using two-tailed Student t-test\n",
    "pvalue = result_r1.compare_with(result_r2, metric=\"P_10\")\n",
    "\n",
    "print(\"P-value for wrt P@10 between r1 and r2: %.3f\" % (pvalue[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. VSM based on word co-occurrence implementation - WCSVSM\n",
    "We have used the VSM based on word co-occurrence implementation with our dataset.\n",
    "\n",
    "This implementation was proposed in: \n",
    "\n",
    "Chen, S., Chen, Y., Yuan, F., & Chang, X. (2020). Establishment of herbal prescription vector space model based on word co-occurrence. Journal of Supercomputing, 76(5), 3590–3601. https://doi.org/10.1007/s11227-018-2559-3\n",
    "\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from gensim) (4.0.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Sandrus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the required software is now installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now import the functions provided by NLTK to perform tokenizing considering punctuation signs.\n",
    "from nltk.tokenize import wordpunct_tokenize, regexp_tokenize\n",
    "# Next, we import required functions to filter-out stopwords for the English language.\n",
    "from nltk.corpus import stopwords\n",
    "# Now we import the function that implements the Porter's stemming algorithm.\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following functions of the Simple VSM implementation:\n",
    "** Please, run the next two defs before:\n",
    "- preprocess_document(doc) -> returns a list containing all STEMS in the collection (document). Input: One document\n",
    "- preprocess_query(q)  -> returns a list containing all STEMS in the collection (query). Input: One query\n",
    "- create_dictionary(docs) -> returns a dictionary containing the mappings WORD_ID -> WORD. This dictionary is required to create the vector-based word representations. Input: all documents\n",
    "- docs2bows(allData, dictionary) -> returns vectors, bag of words-based representation for each document in the collection. Each bow include in bows contain an id and freq. dict(id) return the name of the word. => TF-weighted vectors (Build Later)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Different words in the collection\n",
    "def create_dictionaryWCS(docs):\n",
    "    print(\"-- create_dictionaryWCS\")\n",
    "    #print(preprocess_document(docs.iloc[79754]))\n",
    "    # List all pre-processing documents\n",
    "    pdocs = [preprocess_document(docs.iloc[i]) for i in range(docs.shape[0])]\n",
    "    #print(\"pdocs: \", pdocs)\n",
    "    # Build the dictionary\n",
    "    dictionary = corpora.Dictionary(pdocs)\n",
    "    # Save in a file\n",
    "    dictionary.save('wcs_vsm.dict')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2bowsWCS(allData, dictionary):\n",
    "    print(\"-- docs2bowsWCS\")\n",
    "    docs = [preprocess_document(allData.iloc[i]) for i in range(allData.shape[0])]\n",
    "    # We obtain the set of frequencies for each term\n",
    "    vectors = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    corpora.MmCorpus.serialize('wcs_vsm_docs.mm', vectors)\n",
    "    return vectors    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic1 = create_dictionaryWCS(dt_rel_data)\n",
    "vect1 = docs2bowsWCS(dt_rel_data,dic1)\n",
    "print(vect1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id, freq = vect1[0][0]\n",
    "print(id)\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in vect1:\n",
    "    tvec1 = [(dic1[id], freq) for (id, freq) in v]\n",
    "    print(tvec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate Mutual Information ( (frequency of word i and j)^2 / ((frequency of word i)*(frequency of word i)) ) , and with that the weight ( weight = TF X IDF X MI(wordi,wordj)).  CAMBIAR ESTO\n",
    "\n",
    "First, we will use the extraction algorithm of the paper for extract the word pair.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIMERO TENEMOS QUE BUSCAR EL SET R DE PARES DE CO-OCURRENCIAS. \n",
    "# PARA ELLO CALCULAREMOS TF DE PARES DE CO-OCURRENCIAS Y EL MI\n",
    "# COMO TF TOMAREMOS EL MINIMO ENTRE DOS PALABRAS Y COMO MI EL MAX/SUMA DE CADA TF DE CADA PALABRA.\n",
    "# VAMOS A CALCULAR EL TF Y EL MI A LA VEZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF = Frequency of word co-occurrence\n",
    "# Support (FREQ(W1,W2)) threshold >= 3\n",
    "\n",
    "# Introducimos un solo vector asociado a un único documento, no el conjunto de vectores para todos los documentos\n",
    "def TF_MI_Pairs_cooccu_WCS(vector): # Co-occurence fequency between 2 words: min(w1,w2)\n",
    "    cooccurrence_frequency_MI = []\n",
    "    for j in range(len(vector)): # Leemos cada par (id, frecuencia)\n",
    "        idWord1, freqWord1 = vector[j]\n",
    "        # If we start iterating, at j+1, there is no possibility of adding (j,i) with i<j (only if (i,j) is added)\n",
    "        for h in range(j+1,len(vector)): \n",
    "            idWord2, freqWord2 = vector[h] # Asi ya tenemos las 2 palabras del par con sus frecuencias\n",
    "            freqMin = min(freqWord1, freqWord2) # Hacemos el min como hemos dicho\n",
    "            if freqMin >= 3 : # Threshold support >= 3\n",
    "                MI = freqMin / max(freqWord1, freqWord2) # Sacamos el MI= min(word1,word2)/ word1+word2\n",
    "                if MI > 0.3 :  # Threshold confidence > 0.3\n",
    "                    cooccurrence_frequency_MI.append(((idWord1,idWord2),freqMin,MI))  \n",
    "    \n",
    "                \n",
    "    return cooccurrence_frequency_MI # OUTPUT: List of tuples (co-ocurrence, coocurrence_frequency, coocurrence_MI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary[964])\n",
    "for i in range(0,50):\n",
    "    pairs = TF_MI_Pairs_cooccu_WCS(vect1[i])\n",
    "    print(\"PAIRS: \", pairs)\n",
    "    for (i,j),freq,MI in pairs:\n",
    "        print(i,j)\n",
    "        print(dictionary[i],dictionary[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para todos los vectores: \n",
    "def R_TF_IDF_MI_weight_cooccu_WCS(vectors): # Vamos a calcular TF, IDF, R (todos los pairs relevantes de todo el corpus)\n",
    "    # R = [] Implicit in the list cooccurrence_document_frequency\n",
    "    pairs_docs = []\n",
    "    cooccurrence_document_frequency = [] # List of tuples (co-ocurrence, documente frequency of the co-ocurrence)\n",
    "    \n",
    "    for v in vectors: \n",
    "        #print(\"DOCUMENTO Nº \",doc_num)\n",
    "        pairs = TF_MI_Pairs_cooccu_WCS(v)\n",
    "\n",
    "        #print(\"Choosen:\",choosed)\n",
    "        #print(\"Pairs:\",pairs)\n",
    "        pairs_docs.append(pairs)\n",
    "        for p,_,_ in pairs:\n",
    "            is_in_list = False\n",
    "            for df in range(len(cooccurrence_document_frequency)):\n",
    "                if (cooccurrence_document_frequency[df][0] == p):\n",
    "                    cooccurrence_document_frequency[df][1] = cooccurrence_document_frequency[df][1] +1\n",
    "                    is_in_list = True\n",
    "                    break\n",
    "            if not is_in_list :\n",
    "                cooccurrence_document_frequency.append([p,1])\n",
    "            \n",
    "    # Now, we have the IDF\n",
    "\n",
    "    return sorted(cooccurrence_document_frequency), pairs_docs\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence_document_frequency, pairs_docs= R_TF_IDF_MI_weight_cooccu_WCS(vect1) # Recuerda: vect1 es el vector de frecuencias para cada palabra de los documentos del relevance_data para la primera query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pairs_docs)*len(pairs_docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cooccurrence_document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pairs_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(cooccurrence,cooccurrence_document_frequency,number_of_documents) :\n",
    "    for i,df in cooccurrence_document_frequency:\n",
    "        assert(df != 0)\n",
    "        if (cooccurrence == i): \n",
    "            # The idf is N/df, where N is the total number of documents and df the number of documents containing the co-occurrence\n",
    "            return number_of_documents/df\n",
    "        assert(df>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YA TENEMOS R, Y ADEMÁS TENEMOS EL TF, DF, Y EL MI DE CADA PAR DE PALABRAS EN R PARA CADA DOC\n",
    "# AHORA CALCULAREMOS EL WEIGHT = TF*IDF*MI PARA CADA PAR DE PALABRAS EN CADA DOCUMENTO\n",
    "def weight(vectors):\n",
    "    print(\"-- weighted document vectors\")\n",
    "    cooccurrence_document_frequency, pairs_docs = R_TF_IDF_MI_weight_cooccu_WCS(vectors)\n",
    "        # R: Todos los pares de palabras finales en todos los documentos\n",
    "        # TF_MI_all: Obtenemos una tupla (idWord1,idWord2,freqWord1,freqWord2,freqMin,MI) para cada \n",
    "            # par de palabra de R en cada documento \n",
    "        # coocurrence_document_frequency: Para cada par en R obtenemos su DF guardado como [par, total] para el corpus de la query\n",
    "        # pairs_docs: Pares de palabras finales contenidas en R en cada documento\n",
    "    # Ahora para documento y para cada par de palabras de cada doc, calcularemos el weight: \n",
    "    model =[]\n",
    "    for document_index,document in enumerate(pairs_docs): # For each doc\n",
    "        weight_i = {}\n",
    "        for p,TF,MI in document:\n",
    "            # We obtain the idf for the coocurrence\n",
    "            IDF = get_idf(p,cooccurrence_document_frequency, len(pairs_docs))\n",
    "            weight_i.update({p:math.log10(TF)*math.log10(IDF)*MI})\n",
    "        model.append(weight_i)\n",
    "    return model, cooccurrence_document_frequency, pairs_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we1, cooccurrence_document_frequency, pairs_docs = weight(vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(we1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_cooccurrences(vq,cooccurrence_document_frequency,number_of_documents):\n",
    "    relevant_cooccurrences_weighted = {}\n",
    "    for id1,freq1 in vq: # Leemos cada par (id, frecuencia)\n",
    "        for id2,freq2 in vq:\n",
    "            if (id1,id2) in [e[0] for e in cooccurrence_document_frequency] :\n",
    "                TF = min(freq1,freq2)\n",
    "                IDF = get_idf((id1,id2),cooccurrence_document_frequency, number_of_documents)\n",
    "                MI = TF/max(freq1,freq2)\n",
    "                relevant_cooccurrences_weighted.update({(id1,id2) : math.log10(TF)*math.log10(IDF)*MI})\n",
    "                \n",
    "    return relevant_cooccurrences_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_relevant_cooccurrences([(63,2),(77,2)],cooccurrence_document_frequency,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AHORA YA TENEMOS LOS WEIGHTS PARA LAS PALABRAS RELEVANTES, AHORA PROCESAREMOS PARA LA QUERY Y SACAREMOS LOS PARES DE PALABRAS\n",
    "\n",
    "def take_second(elem): # Para en el ranking ordenar por score\n",
    "    return elem[1]\n",
    "\n",
    "def launch_queryWCS(allData, q, number, f=\"null\",  filename='wcs_vsm_docs.mm'):\n",
    "    dictionary = create_dictionaryWCS(allData) # Creamos el diccionario\n",
    "    \n",
    "    vectors = docs2bowsWCS(allData, dictionary) # Creamos un array con una fila para documento, y cada elemento de la fila: (id palabra,freq)\n",
    "    \n",
    "    weight_docs, cooccurrence_document_frequency, pairs_docs = weight(vectors)\n",
    "    print(list(weight_docs[0].keys())[0])\n",
    "    print(dictionary[list(weight_docs[0].keys())[0][0]])\n",
    "    print(dictionary[list(weight_docs[0].keys())[0][1]])\n",
    "        # con weight(vectors) Calculamos:\n",
    "            # R: los pares relevantes de todo el corpus\n",
    "            # el weight de esos pares en cada documento\n",
    "            # TF_MI_all: una tupla (idWord1,idWord2,freqWord1,freqWord2,freqMin,MI) para cada par de palabra de R en cada doc \n",
    "            # coocurrence_document_frequency: el IDF para cada par con su frecuencia total para todo el corpus\n",
    "            # pairs_docs: se muestran los pares pertenecientes a R que aparecen en cada documento relevantes \n",
    "    print(\"-- preprocess_query\")\n",
    "    pq = preprocess_query(q) # Preprocesamos la query\n",
    "    print(\"-- doc2bow_query\")\n",
    "    vq = dictionary.doc2bow(pq) #Asociamos un ID a cada palabra\n",
    "    \n",
    "    print(\"-- query_weighted_vector\")\n",
    "    # Obtain the relevant co-occurrences (the ones of R, implicit in cooccurrence_documente_frequenct) that are present in the query: \n",
    "    pairs_q = get_relevant_cooccurrences(vq,cooccurrence_document_frequency, len(pairs_docs))\n",
    "\n",
    "\n",
    "    #print(\"pairs_q: \", pairs_q)\n",
    "    \n",
    "    print(\"-- launch the query\")\n",
    "    # For each document\n",
    "    ranking = []\n",
    "    for i,doc in enumerate(weight_docs) :\n",
    "        # Find the cosine between the document and the query: i.e. the dot product of the vectors / mult. of the length of the vectors\n",
    "        dot_product = 0\n",
    "        document_vector_length = 0\n",
    "        query_vector_length = 0\n",
    "        for cooccurrence in doc.keys() :\n",
    "            if cooccurrence in pairs_q.keys() :\n",
    "                dot_product += doc[cooccurrence]*pairs_q[cooccurrence]\n",
    "            document_vector_length += doc[cooccurrence]**2\n",
    "        for cooccurrence in pairs_q.keys() :\n",
    "            query_vector_length += pairs_q[cooccurrence]**2\n",
    "        value = 0\n",
    "        if query_vector_length == 0 or document_vector_length == 0 :\n",
    "            value = 0.0\n",
    "        else:\n",
    "            value = abs(dot_product/(math.sqrt(document_vector_length)*math.sqrt(query_vector_length)))\n",
    "        ranking.append((i,value))\n",
    "    \n",
    "    ranking.sort(key= lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i in range(0,10) :\n",
    "        print(\"[ Score = \"+str(ranking[i][1])+\" ] \"+allData.iloc[ranking[i][0]].title)\n",
    "        if f!=\"null\":\n",
    "            f.write(str(number)+\" Q0 \"+str(allData.iloc[ranking[i][0]].cord_uid)+\" \"+str(i+1)+\" \"+str(ranking[i][1])+\" mySystem \\n\")\n",
    "    \"\"\"\n",
    "    pos = 1\n",
    "    for doc, score in ranking:\n",
    "        if ( pos <=10 ): # First ten positions\n",
    "            print(\"[ Score = \"+str(ranking[i][1])+\" ] \"+allData.iloc[ranking[i][0]].title)\n",
    "            if f!=\"null\":\n",
    "                f.write(str(number)+\" Q0 \"+str(allData.iloc[doc].cord_uid)+\" \"+str(pos)+\" \"+str(round(score,3))+\" mySystem \\n\")\n",
    "        else: \n",
    "            if f!=\"null\":\n",
    "                f.write(str(number)+\" Q0 \"+str(allData.iloc[doc].cord_uid)+\" \"+str(pos)+\" \"+str(round(score,3))+\" mySystem \\n\")\n",
    "            else:\n",
    "                break\n",
    "        pos += 1    \"\"\"\n",
    "    return ranking\n",
    "            \n",
    "    \n",
    "'''   \n",
    "    print(\"-- totalWeight\")\n",
    "    totalWeight = []\n",
    "    for doc_number in range(len(allData)):  \n",
    "        weight_doc = 0\n",
    "        for pairsq in pairs_q:\n",
    "            for w in weight_docs[doc_number]:\n",
    "                p = w[0]\n",
    "                i,j = p\n",
    "                if pairsq == p or pairsq == (j,i):\n",
    "                    weight_doc += w[1]\n",
    "                    break                \n",
    "        totalWeight.append((doc_number,weight_doc))\n",
    "    #print(\"totalWeight: \", totalWeight)\n",
    "    \n",
    "    print(\"-- ranking\")\n",
    "    ranking = sorted(totalWeight, key =take_second,reverse=True )\n",
    "    print(\"QUERY:\",q)\n",
    "    pos = 0\n",
    "    for doc, score in ranking:\n",
    "        \"\"\"\n",
    "        if(round(score,3)==0):\n",
    "            print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + allData.iloc[doc].title,\"#########################\"); \n",
    "        else:\n",
    "            print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + allData.iloc[doc].title); \n",
    "        \"\"\"\n",
    "        if ( pos <10 ): # First ten positions\n",
    "            print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + allData.iloc[doc].title); \n",
    "        else: \n",
    "            break\n",
    "        pos += 1\n",
    "    return ranking\n",
    "    \"\"\"              \n",
    "    qtWeight = tWeight_Matrix[vq] # A PARTIR DE AQUÍ ESTÁ EL FALLO\n",
    "    sim = index[qtWeight]\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "    print(\"##############\")\n",
    "    print(\"QUERY:\",q)\n",
    "    pos = 0\n",
    "    for doc, score in ranking:\n",
    "        \"\"\"\"\"\"\n",
    "        if(round(score,3)==0):\n",
    "            print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + allData.iloc[doc].title,\"#########################\"); \n",
    "        else:\n",
    "            print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + allData.iloc[doc].title); \n",
    "        \"\"\"\"\"\"\n",
    "        if ( pos <10 ): # First ten positions\n",
    "            print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + allData.iloc[doc].title); \n",
    "        else: \n",
    "            break\n",
    "        pos += 1\n",
    "    return ranking\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can launch any query we see fit to our newly created Information Retrieval engine.\n",
    "\n",
    "We choose the query 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select all cord_uid in relevance_data for the query 1\n",
    "cords_relevance_data = set()\n",
    "for i in range(relevance_data.loc[relevance_data.topic_id==1].shape[0]):\n",
    "    cords_relevance_data.add(relevance_data.loc[relevance_data.topic_id==1].cord_uid[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select all cord_uid that are in both (relevance_data and dt) for the query 1\n",
    "dt_rel_data = pd.DataFrame()\n",
    "aux = 0\n",
    "for i in range(dt.cord_uid.shape[0]):\n",
    "    #print(dt.cord_uid[i])\n",
    "    if dt.cord_uid[i] in cords_relevance_data:\n",
    "        #print(dt.cord_uid[i])\n",
    "        dt_rel_data[aux] = dt.iloc[i]\n",
    "        aux +=1\n",
    "dt_rel_data = dt_rel_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we rank for the query 1\n",
    "value = topics[1]\n",
    "# rankWCS= launch_queryWCS(dt_rel_data, value[\"query\"])\n",
    "rank_WCS= launch_queryWCS(dt_rel_data, value[\"query\"], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance evaluation -- All Queries\n",
    "First, we are going to ranking for all the queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "QUERY NUMBER  1\n",
      "-- create_dictionaryWCS\n",
      "-- docs2bowsWCS\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-79cc0cabdd5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"QUERY NUMBER \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtopi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mrank_WCS\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlaunch_queryWCS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_rel_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"query\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mall_rankings_WCS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrank_WCS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-144-62ac41a46e71>\u001b[0m in \u001b[0;36mlaunch_queryWCS\u001b[1;34m(allData, q, number, f, filename)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dictionaryWCS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Creamos el diccionario\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocs2bowsWCS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Creamos un array con una fila para documento, y cada elemento de la fila: (id palabra,freq)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mweight_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcooccurrence_document_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpairs_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-4fcf02c10fac>\u001b[0m in \u001b[0;36mdocs2bowsWCS\u001b[1;34m(allData, dictionary)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdocs2bowsWCS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-- docs2bowsWCS\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# We obtain the set of frequencies for each term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-4fcf02c10fac>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdocs2bowsWCS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-- docs2bowsWCS\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# We obtain the set of frequencies for each term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-4287d362c0b2>\u001b[0m in \u001b[0;36mpreprocess_document\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Each doc is each df row. We will only use title and abstract: dt.iloc[i].title and dt.iloc[i].abstract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mstopset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstract\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# For empty documents without title and abstract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     21\u001b[0m         return [\n\u001b[0;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         ]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_rankings_WCS=[]\n",
    "f=open(\"../queries/trecRun_WCS.txt\",\"w\")\n",
    "for topi in range(len(topics)):\n",
    "    # We select all cord_uid in relevance_data for the query number topi\n",
    "    cords_relevance_data = set()\n",
    "    aux = 0\n",
    "    for i in range(topi+1):\n",
    "        aux += relevance_data.loc[relevance_data.topic_id==(i)].shape[0]\n",
    "    for i in range(relevance_data.loc[relevance_data.topic_id==(topi+1)].shape[0]):\n",
    "        cords_relevance_data.add(relevance_data.loc[relevance_data.topic_id==(topi+1)].cord_uid[aux+i])\n",
    "    # We select all cord_uid that are in both (relevance_data and dt) for the query number topi\n",
    "    dt_rel_data = pd.DataFrame()\n",
    "    aux = 0\n",
    "    for i in range(dt.cord_uid.shape[0]):\n",
    "        if dt.cord_uid[i] in cords_relevance_data:\n",
    "            dt_rel_data[aux] = dt.iloc[i]\n",
    "            aux +=1\n",
    "    dt_rel_data = dt_rel_data.T\n",
    "    # Now we rank for the query number topi\n",
    "    print(\"##############\")\n",
    "    print(\"QUERY NUMBER \", (topi+1))\n",
    "    value = topics[(topi+1)]\n",
    "    rank_WCS= launch_queryWCS(dt_rel_data, value[\"query\"],(topi+1),f)\n",
    "    all_rankings_WCS.append(rank_WCS)\n",
    "    \n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trectools in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (0.0.44)\n",
      "Requirement already satisfied: bs4>=0.0.0.1 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (0.0.1)\n",
      "Requirement already satisfied: sarge>=0.1.1 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (0.1.6)\n",
      "Requirement already satisfied: numpy>=1.0.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (1.19.2)\n",
      "Requirement already satisfied: scikit-learn>=0.15 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (0.23.2)\n",
      "Requirement already satisfied: pandas>=0.15.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (1.1.3)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (4.6.1)\n",
      "Requirement already satisfied: scipy>=0.10.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=1.5 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from trectools) (3.3.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from bs4>=0.0.0.1->trectools) (4.9.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from scikit-learn>=0.15->trectools) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from scikit-learn>=0.15->trectools) (2.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from pandas>=0.15.0->trectools) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from pandas>=0.15.0->trectools) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4>=0.0.0.1->trectools) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sandrus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.15.0->trectools) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install trectools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREC_EVAL using document score\n",
    "from trectools import TrecQrel, TrecRun, TrecEval\n",
    "# A typical evaluation workflow\n",
    "\n",
    "# We load a TrecRun object\n",
    "r1 = TrecRun(\"../queries/trecRun_tfidf.txt\")\n",
    "#print(r1.topics()[:5]) # Shows the first 5 topics\n",
    "r2 = TrecRun(\"../queries/trecRun_WCS\")\n",
    "#print(r2.topics()[:5]) # Shows the first 5 topics\n",
    "\n",
    "\n",
    "# We load a TrecQrel object\n",
    "relevance_data_qrels = \"../queries/relevance_judgements.txt\"\n",
    "qrels = TrecQrel(relevance_data_qrels)\n",
    "\n",
    "te = TrecEval(r2, qrels)\n",
    "map_result = te.get_map(trec_eval = True ) # The result is the same as trec_eval\n",
    "ndcg_result = te.get_ndcg(trec_eval=True)     \n",
    "\n",
    "print(\"MAP: %.3f, NDCG: %.3f\" % (map_result, ndcg_result))\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "result_r1 = r1.evaluate_run(qrels, per_query=True) \n",
    "result_r2 = r2.evaluate_run(qrels, per_query=True)\n",
    "\n",
    "# Inspect for statistically significant differences between the two runs for  P_10 using two-tailed Student t-test\n",
    "pvalue = result_r1.compare_with(result_r2, metric=\"P_10\")\n",
    "\n",
    "print(\"P-value for wrt P@10 between r1 and r2: %.3f\" % (pvalue[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VSM based on combine the word co-occurrence implementation and TF-IDF implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, relying only in co-occurrenes might by a pitfall to avoid. The queries tend to be small thus co-occurrence will not be frequent. As such, we decide lo linearly combine the ranking provided by the TF-IDF model and by the k-best co-occurrences model. \n",
    "\n",
    "$final\\_ranking = \\alpha * TFIDF\\_ranking + (1-\\alpha) * WCS\\_ranking, \\alpha \\in [0,1]$\n",
    "\n",
    "In other words, for each document in the ranking we will do a ponderate sum (with $\\alpha$) of their cosine similarities of both ranking models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the TF-IDF model for query 1\n",
    "value = topics[1]\n",
    "rank_tfidf= launch_query(dt_rel_data, value[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rank_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearly combination function\n",
    "def combine_ranks(rank1, rank2, alpha) :\n",
    "    dict_rank = dict(rank1)\n",
    "    rank_final = []\n",
    "    for i,v in rank2 :\n",
    "        rank_final.append((i, alpha * dict_rank[i] + (1-alpha)*v))\n",
    "    rank_final.sort(key= lambda x: x[1], reverse=True)\n",
    "    for i in range(0,10) :\n",
    "        print(\"[ Score = \"+str(rank_final[i][1])+\" ] \"+dt_rel_data.iloc[rank_final[i][0]].title)\n",
    "    return rank_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rank_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-6e7d3002c605>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcombine_ranks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrank_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank_WCS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rank_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "combine_ranks(rank_tfidf, rank_WCS, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
